{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, test_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "    #test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'comparativeWS01'\n",
    "epochNum         = 30\n",
    "device           = torch.device('cuda:2')\n",
    "ensembleSize     = 6\n",
    "\n",
    "trainPercentageParam = 0.6\n",
    "validPercentageParam = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41712\n"
     ]
    }
   ],
   "source": [
    "mamadroid_meta_df = pd.read_parquet('dataset/mamadroid_meta.parquet')\n",
    "mamadroid_meta_df = mamadroid_meta_df[['sha256', 'year', 'tag']]\n",
    "\n",
    "drebin_df = pd.read_msgpack('/ws/mnt/habouch/datasets/android_dataset/meta/drebin_meta.msg')\n",
    "drebin_df = drebin_df[['sha256']]\n",
    "drebin_df['tag'] = 'malware'\n",
    "drebin_df['year'] = 'drebin'\n",
    "drebin_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mamadroid_df = pd.concat([mamadroid_meta_df, drebin_df], sort=False)\n",
    "mamadroid_df.drop_duplicates(subset='sha256', inplace=True)\n",
    "\n",
    "doneList = [item.split('/')[-1] for item in glob.glob('/ws/mnt/local/data/output/mamadroid/*')]\n",
    "mamadroid_df = mamadroid_df.loc[mamadroid_df.sha256.isin(doneList)]\n",
    "\n",
    "mamadroid_df['label'] = (mamadroid_df.tag == 'malware').apply(int)\n",
    "mamadroid_df['filePath'] = '/ws/mnt/local/data/output/mamadroid/' + mamadroid_df.sha256\n",
    "print(len(mamadroid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40706\n",
      "37758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>tag</th>\n",
       "      <th>filePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <th>1</th>\n",
       "      <td>6928</td>\n",
       "      <td>6928</td>\n",
       "      <td>6928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <th>1</th>\n",
       "      <td>13654</td>\n",
       "      <td>13654</td>\n",
       "      <td>13654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <th>1</th>\n",
       "      <td>3732</td>\n",
       "      <td>3732</td>\n",
       "      <td>3732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <th>1</th>\n",
       "      <td>2184</td>\n",
       "      <td>2184</td>\n",
       "      <td>2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drebin</th>\n",
       "      <th>1</th>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <th>0</th>\n",
       "      <td>1638</td>\n",
       "      <td>1638</td>\n",
       "      <td>1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <th>0</th>\n",
       "      <td>4986</td>\n",
       "      <td>4986</td>\n",
       "      <td>4986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sha256    tag  filePath\n",
       "year   label                         \n",
       "2013   1        6928   6928      6928\n",
       "2014   1       13654  13654     13654\n",
       "2015   1        3732   3732      3732\n",
       "2016   1        2184   2184      2184\n",
       "drebin 1        4636   4636      4636\n",
       "new    0        1638   1638      1638\n",
       "old    0        4986   4986      4986"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "androzoo_df = pd.read_parquet('dataset/androzoo_meta.parquet')\n",
    "df = androzoo_df.loc[androzoo_df.sha256.isin(mamadroid_df.loc[mamadroid_df.label == 0, 'sha256'])]\n",
    "df.dropna(subset=['vt_detection'], inplace=True)\n",
    "df = df.loc[df.vt_detection > 0]\n",
    "mamadroid_df = mamadroid_df.loc[~mamadroid_df.sha256.isin(df.sha256)]\n",
    "print(len(mamadroid_df))\n",
    "\n",
    "df = androzoo_df.loc[androzoo_df.sha256.isin(mamadroid_df.loc[mamadroid_df.label == 1, 'sha256'])]\n",
    "df.dropna(subset=['vt_detection'], inplace=True)\n",
    "df = df.loc[df.vt_detection < 4]\n",
    "mamadroid_df = mamadroid_df.loc[~mamadroid_df.sha256.isin(df.sha256)]\n",
    "print(len(mamadroid_df))\n",
    "\n",
    "mamadroid_df.groupby(['year', 'label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "        ['drebin', 'old'],\n",
    "        ['2013',   'old'],\n",
    "        ['2014',   'old'],\n",
    "        ['2014',   'new'],\n",
    "        ['2015',   'new'],\n",
    "        ['2016',   'new'],\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## drebin_old\n",
      "5773\n",
      "0    3004\n",
      "1    2769\n",
      "Name: label, dtype: int64\n",
      "3849\n",
      "0    1982\n",
      "1    1867\n",
      "Name: label, dtype: int64\n",
      "37758\n",
      "1    31134\n",
      "0     6624\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_drebin_old [0000] TF1: 94.7557, Tloss: 0.00434005, VF1: 97.2830, VLoss: 0.00157100,\n",
      "Train: train_drebin_old [0001] TF1: 98.3719, Tloss: 0.00171387, VF1: 98.3142, VLoss: 0.00100549,\n",
      "Train: train_drebin_old [0002] TF1: 98.5188, Tloss: 0.00131676, VF1: 98.3449, VLoss: 0.00088253,\n",
      "Train: train_drebin_old [0003] TF1: 98.6999, Tloss: 0.00118682, VF1: 98.2119, VLoss: 0.00078494,\n",
      "Train: train_drebin_old [0004] TF1: 99.3672, Tloss: 0.00068038, VF1: 80.3708, VLoss: 0.00695164,\n",
      "Train: train_drebin_old [0005] TF1: 99.1147, Tloss: 0.00083075, VF1: 96.9444, VLoss: 0.00168006,\n",
      "Train: train_drebin_old [0006] TF1: 99.4399, Tloss: 0.00061484, VF1: 98.2941, VLoss: 0.00091501,\n",
      "Train: train_drebin_old [0007] TF1: 99.1694, Tloss: 0.00069082, VF1: 96.9945, VLoss: 0.00144518,\n",
      "Train: train_drebin_old [0008] TF1: 99.4218, Tloss: 0.00059888, VF1: 97.8836, VLoss: 0.00113844,\n",
      "Train: train_drebin_old [0009] TF1: 98.8991, Tloss: 0.00094813, VF1: 97.8261, VLoss: 0.00104977,\n",
      "Train: train_drebin_old [0010] TF1: 99.6026, Tloss: 0.00053609, VF1: 98.0434, VLoss: 0.00109098,\n",
      "Train: train_drebin_old [0011] TF1: 99.3318, Tloss: 0.00056888, VF1: 98.3914, VLoss: 0.00079084,\n",
      "Train: train_drebin_old [0012] TF1: 99.1867, Tloss: 0.00080918, VF1: 98.0111, VLoss: 0.00106194,\n",
      "Train: train_drebin_old [0013] TF1: 99.5488, Tloss: 0.00048578, VF1: 97.5970, VLoss: 0.00120304,\n",
      "Train: train_drebin_old [0014] TF1: 99.5846, Tloss: 0.00038291, VF1: 98.4591, VLoss: 0.00087734,\n",
      "Train: train_drebin_old [0015] TF1: 99.5478, Tloss: 0.00043729, VF1: 87.9697, VLoss: 0.00595854,\n",
      "Train: train_drebin_old [0016] TF1: 99.4040, Tloss: 0.00048445, VF1: 94.1476, VLoss: 0.00303303,\n",
      "Train: train_drebin_old [0017] TF1: 99.3495, Tloss: 0.00051437, VF1: 98.4459, VLoss: 0.00077092,\n",
      "Train: train_drebin_old [0018] TF1: 99.6929, Tloss: 0.00032339, VF1: 96.6970, VLoss: 0.00214492,\n",
      "Train: train_drebin_old [0019] TF1: 99.5300, Tloss: 0.00046926, VF1: 95.4218, VLoss: 0.00265142,\n",
      "Train: train_drebin_old [0020] TF1: 99.1509, Tloss: 0.00070570, VF1: 97.7067, VLoss: 0.00104581,\n",
      "Train: train_drebin_old [0021] TF1: 99.6749, Tloss: 0.00036844, VF1: 98.1730, VLoss: 0.00086666,\n",
      "Train: train_drebin_old [0022] TF1: 99.7652, Tloss: 0.00024568, VF1: 84.9357, VLoss: 0.00539542,\n",
      "Train: train_drebin_old [0023] TF1: 99.8014, Tloss: 0.00021265, VF1: 93.6480, VLoss: 0.00429265,\n",
      "Train: train_drebin_old [0024] TF1: 99.4397, Tloss: 0.00043022, VF1: 98.0795, VLoss: 0.00110589,\n",
      "Train: train_drebin_old [0025] TF1: 99.4401, Tloss: 0.00051482, VF1: 98.2437, VLoss: 0.00094456,\n",
      "Train: train_drebin_old [0026] TF1: 99.4940, Tloss: 0.00039278, VF1: 97.8870, VLoss: 0.00118960,\n",
      "Train: train_drebin_old [0027] TF1: 99.6027, Tloss: 0.00034772, VF1: 98.2736, VLoss: 0.00109500,\n",
      "Train: train_drebin_old [0028] TF1: 99.5846, Tloss: 0.00047550, VF1: 97.4547, VLoss: 0.00142537,\n",
      "Train: train_drebin_old [0029] TF1: 99.5483, Tloss: 0.00038430, VF1: 93.6896, VLoss: 0.00286792,\n",
      "Epoch    29: reducing learning rate of group 0 to 2.4000e-04.\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_drebin_old_017.pth Fragment=00 score=0.8412058621394208\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_drebin_old_003.pth Fragment=00 score=0.7853596807164411\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_drebin_old_011.pth Fragment=00 score=0.7679449943690356\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_drebin_old_021.pth Fragment=00 score=0.7277958009426455\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_drebin_old_014.pth Fragment=00 score=0.8452033001590884\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_drebin_old_002.pth Fragment=00 score=0.8414133724492637\n",
      "Extend: f1score=80.2136, vcoverage=80.5339, vf1score=83.9144, vexentdSize=30408, ecoverage=19.4661, ef1score=66.2259, erestSize=7350\n",
      "########\n",
      "######## 2013_old\n",
      "7148\n",
      "1    4189\n",
      "0    2959\n",
      "Name: label, dtype: int64\n",
      "4766\n",
      "1    2739\n",
      "0    2027\n",
      "Name: label, dtype: int64\n",
      "37758\n",
      "1    31134\n",
      "0     6624\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2013_old [0000] TF1: 96.5137, Tloss: 0.00359827, VF1: 97.2658, VLoss: 0.00155977,\n",
      "Train: train_2013_old [0001] TF1: 98.6491, Tloss: 0.00162695, VF1: 96.6006, VLoss: 0.00175733,\n",
      "Train: train_2013_old [0002] TF1: 99.0312, Tloss: 0.00119525, VF1: 95.7379, VLoss: 0.00228202,\n",
      "Train: train_2013_old [0003] TF1: 99.1518, Tloss: 0.00103474, VF1: 98.5053, VLoss: 0.00088439,\n",
      "Train: train_2013_old [0004] TF1: 99.2715, Tloss: 0.00080136, VF1: 94.5766, VLoss: 0.00271039,\n",
      "Train: train_2013_old [0005] TF1: 99.4392, Tloss: 0.00062702, VF1: 98.8686, VLoss: 0.00072402,\n",
      "Train: train_2013_old [0006] TF1: 99.2840, Tloss: 0.00085993, VF1: 97.0004, VLoss: 0.00182277,\n",
      "Train: train_2013_old [0007] TF1: 99.5942, Tloss: 0.00048008, VF1: 98.5432, VLoss: 0.00111115,\n",
      "Train: train_2013_old [0008] TF1: 99.3551, Tloss: 0.00078702, VF1: 98.6975, VLoss: 0.00086507,\n",
      "Train: train_2013_old [0009] TF1: 99.5465, Tloss: 0.00049789, VF1: 98.6126, VLoss: 0.00080967,\n",
      "Train: train_2013_old [0010] TF1: 99.6537, Tloss: 0.00036639, VF1: 98.6618, VLoss: 0.00099811,\n",
      "Train: train_2013_old [0011] TF1: 99.4747, Tloss: 0.00055237, VF1: 98.5107, VLoss: 0.00091090,\n",
      "Train: train_2013_old [0012] TF1: 99.7732, Tloss: 0.00030529, VF1: 98.5428, VLoss: 0.00100854,\n",
      "Train: train_2013_old [0013] TF1: 99.8450, Tloss: 0.00020657, VF1: 98.5297, VLoss: 0.00097370,\n",
      "Train: train_2013_old [0014] TF1: 99.3909, Tloss: 0.00059111, VF1: 96.4892, VLoss: 0.00253260,\n",
      "Train: train_2013_old [0015] TF1: 99.5583, Tloss: 0.00039395, VF1: 98.5813, VLoss: 0.00089007,\n",
      "Train: train_2013_old [0016] TF1: 99.6181, Tloss: 0.00046579, VF1: 98.5639, VLoss: 0.00080140,\n",
      "Train: train_2013_old [0017] TF1: 99.6061, Tloss: 0.00039582, VF1: 98.3026, VLoss: 0.00118472,\n",
      "Train: train_2013_old [0018] TF1: 99.6419, Tloss: 0.00043851, VF1: 89.3185, VLoss: 0.00696808,\n",
      "Train: train_2013_old [0019] TF1: 99.6536, Tloss: 0.00033744, VF1: 98.3013, VLoss: 0.00131333,\n",
      "Epoch    19: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2013_old [0020] TF1: 99.6896, Tloss: 0.00036580, VF1: 98.7505, VLoss: 0.00098656,\n",
      "Train: train_2013_old [0021] TF1: 99.6536, Tloss: 0.00037454, VF1: 98.7393, VLoss: 0.00078974,\n",
      "Train: train_2013_old [0022] TF1: 99.7971, Tloss: 0.00024463, VF1: 97.2559, VLoss: 0.00196298,\n",
      "Train: train_2013_old [0023] TF1: 99.7493, Tloss: 0.00028749, VF1: 98.7881, VLoss: 0.00097580,\n",
      "Train: train_2013_old [0024] TF1: 99.8926, Tloss: 0.00013313, VF1: 98.6013, VLoss: 0.00090736,\n",
      "Train: train_2013_old [0025] TF1: 99.9164, Tloss: 0.00013645, VF1: 98.6111, VLoss: 0.00096273,\n",
      "Train: train_2013_old [0026] TF1: 99.7850, Tloss: 0.00022775, VF1: 98.7727, VLoss: 0.00087034,\n",
      "Train: train_2013_old [0027] TF1: 99.7492, Tloss: 0.00029218, VF1: 98.1084, VLoss: 0.00119519,\n",
      "Train: train_2013_old [0028] TF1: 99.8329, Tloss: 0.00021359, VF1: 98.6241, VLoss: 0.00098204,\n",
      "Train: train_2013_old [0029] TF1: 99.9164, Tloss: 0.00011726, VF1: 98.1348, VLoss: 0.00148473,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2013_old_005.pth Fragment=00 score=0.8765238686906883\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2013_old_021.pth Fragment=00 score=0.8636810844295448\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2013_old_016.pth Fragment=00 score=0.912461953213323\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2013_old_009.pth Fragment=00 score=0.8966005164668011\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2013_old_008.pth Fragment=00 score=0.8641813344994717\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2013_old_026.pth Fragment=00 score=0.8353451175305505\n",
      "Extend: f1score=87.0076, vcoverage=84.0325, vf1score=91.9053, vexentdSize=31729, ecoverage=15.9675, ef1score=60.1762, erestSize=6029\n",
      "########\n",
      "######## 2014_old\n",
      "11184\n",
      "1    8170\n",
      "0    3014\n",
      "Name: label, dtype: int64\n",
      "7456\n",
      "1    5484\n",
      "0    1972\n",
      "Name: label, dtype: int64\n",
      "37758\n",
      "1    31134\n",
      "0     6624\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2014_old [0000] TF1: 95.5773, Tloss: 0.00537887, VF1: 95.5856, VLoss: 0.00249588,\n",
      "Train: train_2014_old [0001] TF1: 97.7647, Tloss: 0.00288730, VF1: 96.8633, VLoss: 0.00198742,\n",
      "Train: train_2014_old [0002] TF1: 98.0149, Tloss: 0.00260355, VF1: 93.1642, VLoss: 0.00455635,\n",
      "Train: train_2014_old [0003] TF1: 98.2435, Tloss: 0.00224009, VF1: 97.0286, VLoss: 0.00194839,\n",
      "Train: train_2014_old [0004] TF1: 98.4632, Tloss: 0.00197486, VF1: 96.3667, VLoss: 0.00250620,\n",
      "Train: train_2014_old [0005] TF1: 98.7456, Tloss: 0.00163269, VF1: 97.7434, VLoss: 0.00149910,\n",
      "Train: train_2014_old [0006] TF1: 98.6709, Tloss: 0.00170664, VF1: 97.9573, VLoss: 0.00142358,\n",
      "Train: train_2014_old [0007] TF1: 98.7336, Tloss: 0.00169032, VF1: 98.1616, VLoss: 0.00133850,\n",
      "Train: train_2014_old [0008] TF1: 98.8627, Tloss: 0.00152417, VF1: 96.4973, VLoss: 0.00240132,\n",
      "Train: train_2014_old [0009] TF1: 98.7102, Tloss: 0.00159306, VF1: 95.4736, VLoss: 0.00289674,\n",
      "Train: train_2014_old [0010] TF1: 99.1127, Tloss: 0.00118212, VF1: 97.9584, VLoss: 0.00152076,\n",
      "Train: train_2014_old [0011] TF1: 98.8919, Tloss: 0.00146170, VF1: 83.6144, VLoss: 0.00874623,\n",
      "Train: train_2014_old [0012] TF1: 99.1308, Tloss: 0.00122669, VF1: 98.3906, VLoss: 0.00114455,\n",
      "Train: train_2014_old [0013] TF1: 99.0823, Tloss: 0.00126374, VF1: 98.2704, VLoss: 0.00132666,\n",
      "Train: train_2014_old [0014] TF1: 98.9787, Tloss: 0.00132473, VF1: 97.7959, VLoss: 0.00160040,\n",
      "Train: train_2014_old [0015] TF1: 99.2471, Tloss: 0.00110873, VF1: 98.4469, VLoss: 0.00114348,\n",
      "Train: train_2014_old [0016] TF1: 99.2353, Tloss: 0.00096587, VF1: 89.9179, VLoss: 0.01137705,\n",
      "Train: train_2014_old [0017] TF1: 99.2716, Tloss: 0.00103385, VF1: 97.4149, VLoss: 0.00203236,\n",
      "Train: train_2014_old [0018] TF1: 99.2347, Tloss: 0.00106762, VF1: 90.1037, VLoss: 0.01283208,\n",
      "Train: train_2014_old [0019] TF1: 99.2224, Tloss: 0.00109053, VF1: 95.2795, VLoss: 0.00318543,\n",
      "Train: train_2014_old [0020] TF1: 99.2833, Tloss: 0.00102418, VF1: 98.4372, VLoss: 0.00131073,\n",
      "Train: train_2014_old [0021] TF1: 99.2228, Tloss: 0.00099826, VF1: 98.4271, VLoss: 0.00127224,\n",
      "Train: train_2014_old [0022] TF1: 99.2045, Tloss: 0.00103169, VF1: 97.8300, VLoss: 0.00164653,\n",
      "Epoch    22: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2014_old [0023] TF1: 99.3205, Tloss: 0.00091779, VF1: 98.5205, VLoss: 0.00116281,\n",
      "Train: train_2014_old [0024] TF1: 99.4920, Tloss: 0.00072587, VF1: 98.1947, VLoss: 0.00146224,\n",
      "Train: train_2014_old [0025] TF1: 99.4795, Tloss: 0.00073231, VF1: 97.7618, VLoss: 0.00237474,\n",
      "Train: train_2014_old [0026] TF1: 99.3998, Tloss: 0.00085229, VF1: 96.3833, VLoss: 0.00281821,\n",
      "Train: train_2014_old [0027] TF1: 99.3815, Tloss: 0.00080288, VF1: 98.4971, VLoss: 0.00140357,\n",
      "Train: train_2014_old [0028] TF1: 99.4921, Tloss: 0.00072509, VF1: 96.5279, VLoss: 0.00354344,\n",
      "Train: train_2014_old [0029] TF1: 99.4734, Tloss: 0.00071522, VF1: 98.2177, VLoss: 0.00185317,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_old_015.pth Fragment=00 score=0.9713806926463824\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_old_012.pth Fragment=00 score=0.9706655278891004\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_old_023.pth Fragment=00 score=0.9683811407847015\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_old_021.pth Fragment=00 score=0.9730587927196543\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_old_020.pth Fragment=00 score=0.9755317238790134\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_old_013.pth Fragment=00 score=0.9739354250154758\n",
      "Extend: f1score=97.5766, vcoverage=95.1348, vf1score=98.4997, vexentdSize=35921, ecoverage=4.8652, ef1score=74.4781, erestSize=1837\n",
      "########\n",
      "######## 2014_new\n",
      "9175\n",
      "1    8189\n",
      "0     986\n",
      "Name: label, dtype: int64\n",
      "6117\n",
      "1    5465\n",
      "0     652\n",
      "Name: label, dtype: int64\n",
      "37758\n",
      "1    31134\n",
      "0     6624\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2014_new [0000] TF1: 98.2964, Tloss: 0.00381855, VF1: 99.1327, VLoss: 0.00113085,\n",
      "Train: train_2014_new [0001] TF1: 99.1050, Tloss: 0.00162746, VF1: 99.3258, VLoss: 0.00079322,\n",
      "Train: train_2014_new [0002] TF1: 99.3600, Tloss: 0.00118813, VF1: 98.4771, VLoss: 0.00155378,\n",
      "Train: train_2014_new [0003] TF1: 99.3048, Tloss: 0.00105966, VF1: 99.4619, VLoss: 0.00052867,\n",
      "Train: train_2014_new [0004] TF1: 99.5546, Tloss: 0.00076495, VF1: 98.9313, VLoss: 0.00101074,\n",
      "Train: train_2014_new [0005] TF1: 99.5423, Tloss: 0.00077672, VF1: 98.5390, VLoss: 0.00166982,\n",
      "Train: train_2014_new [0006] TF1: 99.5605, Tloss: 0.00072979, VF1: 99.2368, VLoss: 0.00065493,\n",
      "Train: train_2014_new [0007] TF1: 99.5913, Tloss: 0.00067689, VF1: 98.5656, VLoss: 0.00137462,\n",
      "Train: train_2014_new [0008] TF1: 99.6766, Tloss: 0.00063038, VF1: 99.2727, VLoss: 0.00064813,\n",
      "Train: train_2014_new [0009] TF1: 99.6888, Tloss: 0.00058094, VF1: 99.1222, VLoss: 0.00109861,\n",
      "Train: train_2014_new [0010] TF1: 99.7435, Tloss: 0.00043718, VF1: 98.7786, VLoss: 0.00133292,\n",
      "Train: train_2014_new [0011] TF1: 99.6888, Tloss: 0.00054576, VF1: 99.4063, VLoss: 0.00068362,\n",
      "Train: train_2014_new [0012] TF1: 99.7986, Tloss: 0.00037331, VF1: 99.4522, VLoss: 0.00049731,\n",
      "Train: train_2014_new [0013] TF1: 99.6582, Tloss: 0.00053589, VF1: 99.3363, VLoss: 0.00068879,\n",
      "Train: train_2014_new [0014] TF1: 99.7254, Tloss: 0.00049122, VF1: 99.4439, VLoss: 0.00048489,\n",
      "Train: train_2014_new [0015] TF1: 99.6887, Tloss: 0.00048393, VF1: 99.3264, VLoss: 0.00068542,\n",
      "Train: train_2014_new [0016] TF1: 99.7254, Tloss: 0.00046176, VF1: 99.5619, VLoss: 0.00049662,\n",
      "Train: train_2014_new [0017] TF1: 99.7864, Tloss: 0.00042409, VF1: 95.6091, VLoss: 0.00290804,\n",
      "Train: train_2014_new [0018] TF1: 99.7010, Tloss: 0.00040975, VF1: 97.2680, VLoss: 0.00341446,\n",
      "Epoch    18: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2014_new [0019] TF1: 99.8779, Tloss: 0.00023363, VF1: 99.3765, VLoss: 0.00062153,\n",
      "Train: train_2014_new [0020] TF1: 99.7559, Tloss: 0.00033944, VF1: 99.1107, VLoss: 0.00115197,\n",
      "Train: train_2014_new [0021] TF1: 99.7985, Tloss: 0.00029261, VF1: 99.4070, VLoss: 0.00056311,\n",
      "Train: train_2014_new [0022] TF1: 99.8841, Tloss: 0.00023622, VF1: 99.4044, VLoss: 0.00057443,\n",
      "Train: train_2014_new [0023] TF1: 99.7253, Tloss: 0.00042756, VF1: 98.8872, VLoss: 0.00110892,\n",
      "Train: train_2014_new [0024] TF1: 99.8657, Tloss: 0.00023773, VF1: 99.4798, VLoss: 0.00056627,\n",
      "Train: train_2014_new [0025] TF1: 99.7680, Tloss: 0.00032433, VF1: 99.4430, VLoss: 0.00054143,\n",
      "Epoch    25: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2014_new [0026] TF1: 99.8413, Tloss: 0.00024774, VF1: 99.4319, VLoss: 0.00052359,\n",
      "Train: train_2014_new [0027] TF1: 99.9145, Tloss: 0.00017528, VF1: 99.2112, VLoss: 0.00071964,\n",
      "Train: train_2014_new [0028] TF1: 99.8535, Tloss: 0.00021455, VF1: 99.4254, VLoss: 0.00064412,\n",
      "Train: train_2014_new [0029] TF1: 99.8901, Tloss: 0.00018726, VF1: 99.4257, VLoss: 0.00059044,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_new_014.pth Fragment=00 score=0.9484326640742798\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_new_016.pth Fragment=00 score=0.9477192648319208\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_new_012.pth Fragment=00 score=0.9460002213753737\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_new_026.pth Fragment=00 score=0.9500016138923856\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_new_003.pth Fragment=00 score=0.9437843140357262\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2014_new_025.pth Fragment=00 score=0.949134199134199\n",
      "Extend: f1score=94.9191, vcoverage=92.8254, vf1score=96.6792, vexentdSize=35049, ecoverage=7.1746, ef1score=52.9597, erestSize=2709\n",
      "########\n",
      "######## 2015_new\n",
      "3222\n",
      "1    2211\n",
      "0    1011\n",
      "Name: label, dtype: int64\n",
      "2148\n",
      "1    1521\n",
      "0     627\n",
      "Name: label, dtype: int64\n",
      "37758\n",
      "1    31134\n",
      "0     6624\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2015_new [0000] TF1: 91.5325, Tloss: 0.00860606, VF1: 83.3196, VLoss: 0.01072118,\n",
      "Train: train_2015_new [0001] TF1: 96.0616, Tloss: 0.00486454, VF1: 94.4305, VLoss: 0.00295701,\n",
      "Train: train_2015_new [0002] TF1: 96.7566, Tloss: 0.00381255, VF1: 89.3655, VLoss: 0.00678687,\n",
      "Train: train_2015_new [0003] TF1: 96.8630, Tloss: 0.00349697, VF1: 96.7197, VLoss: 0.00183909,\n",
      "Train: train_2015_new [0004] TF1: 97.4243, Tloss: 0.00291242, VF1: 96.0254, VLoss: 0.00249769,\n",
      "Train: train_2015_new [0005] TF1: 97.5499, Tloss: 0.00286440, VF1: 95.7129, VLoss: 0.00255513,\n",
      "Train: train_2015_new [0006] TF1: 97.6934, Tloss: 0.00273644, VF1: 95.9515, VLoss: 0.00224991,\n",
      "Train: train_2015_new [0007] TF1: 97.9157, Tloss: 0.00253589, VF1: 96.7678, VLoss: 0.00184321,\n",
      "Train: train_2015_new [0008] TF1: 98.3398, Tloss: 0.00206092, VF1: 92.9336, VLoss: 0.00530214,\n",
      "Train: train_2015_new [0009] TF1: 98.2607, Tloss: 0.00206872, VF1: 92.1069, VLoss: 0.00496416,\n",
      "Train: train_2015_new [0010] TF1: 98.0499, Tloss: 0.00231980, VF1: 92.1071, VLoss: 0.00451935,\n",
      "Train: train_2015_new [0011] TF1: 97.9167, Tloss: 0.00231117, VF1: 65.2790, VLoss: 0.01422766,\n",
      "Train: train_2015_new [0012] TF1: 97.9823, Tloss: 0.00213569, VF1: 86.4205, VLoss: 0.00938419,\n",
      "Train: train_2015_new [0013] TF1: 98.4818, Tloss: 0.00181411, VF1: 78.2574, VLoss: 0.01034932,\n",
      "Train: train_2015_new [0014] TF1: 98.2345, Tloss: 0.00207944, VF1: 93.6725, VLoss: 0.00530076,\n",
      "Train: train_2015_new [0015] TF1: 98.4845, Tloss: 0.00179233, VF1: 93.6956, VLoss: 0.00389172,\n",
      "Train: train_2015_new [0016] TF1: 98.3666, Tloss: 0.00190944, VF1: 95.1481, VLoss: 0.00332185,\n",
      "Train: train_2015_new [0017] TF1: 98.3443, Tloss: 0.00182132, VF1: 96.9818, VLoss: 0.00174432,\n",
      "Train: train_2015_new [0018] TF1: 97.9887, Tloss: 0.00252818, VF1: 91.1544, VLoss: 0.00575162,\n",
      "Train: train_2015_new [0019] TF1: 98.4364, Tloss: 0.00182664, VF1: 65.8150, VLoss: 0.01428153,\n",
      "Train: train_2015_new [0020] TF1: 98.4824, Tloss: 0.00175171, VF1: 72.8941, VLoss: 0.01272904,\n",
      "Train: train_2015_new [0021] TF1: 98.7963, Tloss: 0.00151384, VF1: 95.3552, VLoss: 0.00292738,\n",
      "Train: train_2015_new [0022] TF1: 98.3228, Tloss: 0.00188084, VF1: 95.5747, VLoss: 0.00270681,\n",
      "Train: train_2015_new [0023] TF1: 98.6183, Tloss: 0.00158347, VF1: 97.4158, VLoss: 0.00174369,\n",
      "Train: train_2015_new [0024] TF1: 99.0705, Tloss: 0.00118349, VF1: 96.9536, VLoss: 0.00191043,\n",
      "Train: train_2015_new [0025] TF1: 98.8464, Tloss: 0.00145088, VF1: 97.5016, VLoss: 0.00168166,\n",
      "Train: train_2015_new [0026] TF1: 98.5928, Tloss: 0.00153500, VF1: 97.2231, VLoss: 0.00166362,\n",
      "Train: train_2015_new [0027] TF1: 98.8006, Tloss: 0.00131420, VF1: 97.3475, VLoss: 0.00180143,\n",
      "Train: train_2015_new [0028] TF1: 98.9126, Tloss: 0.00137401, VF1: 97.1429, VLoss: 0.00178601,\n",
      "Train: train_2015_new [0029] TF1: 99.0274, Tloss: 0.00109720, VF1: 94.1323, VLoss: 0.00446185,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2015_new_026.pth Fragment=00 score=0.9463143964156449\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2015_new_025.pth Fragment=00 score=0.9504347826086956\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2015_new_023.pth Fragment=00 score=0.9470422895106682\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2015_new_017.pth Fragment=00 score=0.9439846817901535\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2015_new_028.pth Fragment=00 score=0.9300712896953985\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2015_new_027.pth Fragment=00 score=0.9153246227083967\n",
      "Extend: f1score=95.0319, vcoverage=86.8743, vf1score=97.1629, vexentdSize=32802, ecoverage=13.1257, ef1score=73.5772, erestSize=4956\n",
      "########\n",
      "######## 2016_new\n",
      "2293\n",
      "1    1332\n",
      "0     961\n",
      "Name: label, dtype: int64\n",
      "1529\n",
      "1    852\n",
      "0    677\n",
      "Name: label, dtype: int64\n",
      "37758\n",
      "1    31134\n",
      "0     6624\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2016_new [0000] TF1: 88.9824, Tloss: 0.00932866, VF1: 71.7841, VLoss: 0.01629144,\n",
      "Train: train_2016_new [0001] TF1: 93.9336, Tloss: 0.00569158, VF1: 89.9106, VLoss: 0.00405498,\n",
      "Train: train_2016_new [0002] TF1: 95.3285, Tloss: 0.00446401, VF1: 95.2210, VLoss: 0.00229528,\n",
      "Train: train_2016_new [0003] TF1: 95.9879, Tloss: 0.00380106, VF1: 80.6113, VLoss: 0.00858614,\n",
      "Train: train_2016_new [0004] TF1: 96.5414, Tloss: 0.00315197, VF1: 94.7242, VLoss: 0.00236814,\n",
      "Train: train_2016_new [0005] TF1: 96.7498, Tloss: 0.00340395, VF1: 95.4654, VLoss: 0.00205576,\n",
      "Train: train_2016_new [0006] TF1: 97.0222, Tloss: 0.00305017, VF1: 92.8571, VLoss: 0.00367933,\n",
      "Train: train_2016_new [0007] TF1: 97.3664, Tloss: 0.00273774, VF1: 78.1116, VLoss: 0.00837269,\n",
      "Train: train_2016_new [0008] TF1: 97.9638, Tloss: 0.00240888, VF1: 92.9455, VLoss: 0.00319075,\n",
      "Train: train_2016_new [0009] TF1: 97.3238, Tloss: 0.00246492, VF1: 95.6366, VLoss: 0.00222545,\n",
      "Train: train_2016_new [0010] TF1: 97.2505, Tloss: 0.00262141, VF1: 88.2198, VLoss: 0.00505441,\n",
      "Train: train_2016_new [0011] TF1: 97.3338, Tloss: 0.00250052, VF1: 86.0201, VLoss: 0.00674856,\n",
      "Train: train_2016_new [0012] TF1: 98.0776, Tloss: 0.00210538, VF1: 77.0007, VLoss: 0.00948102,\n",
      "Train: train_2016_new [0013] TF1: 97.8547, Tloss: 0.00219764, VF1: 96.4097, VLoss: 0.00192924,\n",
      "Train: train_2016_new [0014] TF1: 98.0008, Tloss: 0.00205137, VF1: 94.4581, VLoss: 0.00272291,\n",
      "Train: train_2016_new [0015] TF1: 97.7828, Tloss: 0.00224894, VF1: 94.2581, VLoss: 0.00263572,\n",
      "Train: train_2016_new [0016] TF1: 98.0407, Tloss: 0.00196454, VF1: 94.1514, VLoss: 0.00271541,\n",
      "Train: train_2016_new [0017] TF1: 98.2298, Tloss: 0.00163844, VF1: 95.7612, VLoss: 0.00230137,\n",
      "Train: train_2016_new [0018] TF1: 98.6064, Tloss: 0.00159534, VF1: 95.1220, VLoss: 0.00251306,\n",
      "Train: train_2016_new [0019] TF1: 97.5188, Tloss: 0.00258095, VF1: 96.4012, VLoss: 0.00186547,\n",
      "Train: train_2016_new [0020] TF1: 98.3421, Tloss: 0.00175933, VF1: 94.5632, VLoss: 0.00265940,\n",
      "Train: train_2016_new [0021] TF1: 98.2298, Tloss: 0.00184885, VF1: 94.7189, VLoss: 0.00245033,\n",
      "Train: train_2016_new [0022] TF1: 98.3421, Tloss: 0.00175924, VF1: 91.9822, VLoss: 0.00430509,\n",
      "Train: train_2016_new [0023] TF1: 98.1614, Tloss: 0.00166444, VF1: 96.0867, VLoss: 0.00221252,\n",
      "Train: train_2016_new [0024] TF1: 98.0776, Tloss: 0.00191208, VF1: 95.6679, VLoss: 0.00246668,\n",
      "Epoch    24: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2016_new [0025] TF1: 98.4522, Tloss: 0.00142159, VF1: 95.2106, VLoss: 0.00243078,\n",
      "Train: train_2016_new [0026] TF1: 98.0820, Tloss: 0.00182138, VF1: 96.3927, VLoss: 0.00202182,\n",
      "Train: train_2016_new [0027] TF1: 98.3127, Tloss: 0.00149801, VF1: 96.3227, VLoss: 0.00225820,\n",
      "Train: train_2016_new [0028] TF1: 98.8696, Tloss: 0.00106844, VF1: 93.2802, VLoss: 0.00339267,\n",
      "Train: train_2016_new [0029] TF1: 98.7170, Tloss: 0.00138322, VF1: 89.9408, VLoss: 0.00479099,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2016_new_019.pth Fragment=00 score=0.9518274318274319\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2016_new_013.pth Fragment=00 score=0.9378914923938622\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2016_new_026.pth Fragment=00 score=0.9073281030525198\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2016_new_005.pth Fragment=00 score=0.9465952412251873\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2016_new_023.pth Fragment=00 score=0.9377948657678682\n",
      "Evaluate: ModelPath=./traces/comparativeWS01/model_train_2016_new_009.pth Fragment=00 score=0.9404781479167353\n",
      "Extend: f1score=94.4827, vcoverage=81.9429, vf1score=97.2058, vexentdSize=30940, ecoverage=18.0571, ef1score=80.3527, erestSize=6818\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for tag in tags:\n",
    "    currentTag = tag[0] + '_' + tag[1]\n",
    "\n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    #\n",
    "    dataset_df = mamadroid_df.loc[(mamadroid_df.year == tag[0]) | (mamadroid_df.year == tag[1])]\n",
    "    test_df    = mamadroid_df\n",
    "\n",
    "    #\n",
    "    trainLoader, validLoader, testLoader = getDataloaders(dataset_df, test_df, trainPercentage=trainPercentageParam, \n",
    "                                                                               validPercentage=validPercentageParam)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "    #\n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "    #\n",
    "    evalDataset(ws, evalresult_df, probaUpperBorn=0.8,  probaLowerBorn=0.2)\n",
    "\n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    #\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
