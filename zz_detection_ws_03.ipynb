{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):int(validPercentage * len(dataset_df))]]\n",
    "    test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'detectionWS03'\n",
    "epochNum         = 100\n",
    "device           = torch.device('cuda:2')\n",
    "ensembleSize     = 20\n",
    "trainPercentageParam = 0.8\n",
    "validPercentageParam = 0.9\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## drebin\n",
      "12873\n",
      "0    8575\n",
      "1    4298\n",
      "Name: label, dtype: int64\n",
      "1609\n",
      "0    1081\n",
      "1     528\n",
      "Name: label, dtype: int64\n",
      "1610\n",
      "0    1072\n",
      "1     538\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_drebin [0000] TF1: 95.0006, Tloss: 0.00322638, VF1: 97.5096, VLoss: 0.00080828,\n",
      "Train: train_drebin [0001] TF1: 97.4950, Tloss: 0.00154297, VF1: 88.8889, VLoss: 0.00396057,\n",
      "Train: train_drebin [0002] TF1: 98.2423, Tloss: 0.00113237, VF1: 97.2973, VLoss: 0.00073543,\n",
      "Train: train_drebin [0003] TF1: 98.1238, Tloss: 0.00110434, VF1: 95.4545, VLoss: 0.00140995,\n",
      "Train: train_drebin [0004] TF1: 98.8712, Tloss: 0.00078222, VF1: 98.3825, VLoss: 0.00050750,\n",
      "Train: train_drebin [0005] TF1: 98.8235, Tloss: 0.00076821, VF1: 98.8679, VLoss: 0.00035665,\n",
      "Train: train_drebin [0006] TF1: 98.6968, Tloss: 0.00076605, VF1: 98.1922, VLoss: 0.00049243,\n",
      "Train: train_drebin [0007] TF1: 98.8591, Tloss: 0.00069551, VF1: 89.1938, VLoss: 0.00327615,\n",
      "Train: train_drebin [0008] TF1: 98.8937, Tloss: 0.00069123, VF1: 98.6767, VLoss: 0.00041805,\n",
      "Train: train_drebin [0009] TF1: 99.1860, Tloss: 0.00054374, VF1: 88.0000, VLoss: 0.00570622,\n",
      "Train: train_drebin [0010] TF1: 98.7064, Tloss: 0.00064050, VF1: 98.6717, VLoss: 0.00042743,\n",
      "Train: train_drebin [0011] TF1: 99.2089, Tloss: 0.00053673, VF1: 95.2569, VLoss: 0.00162729,\n",
      "Train: train_drebin [0012] TF1: 99.2787, Tloss: 0.00042873, VF1: 98.1991, VLoss: 0.00050557,\n",
      "Train: train_drebin [0013] TF1: 99.4062, Tloss: 0.00038823, VF1: 98.4791, VLoss: 0.00042671,\n",
      "Train: train_drebin [0014] TF1: 99.0352, Tloss: 0.00052842, VF1: 98.1203, VLoss: 0.00060127,\n",
      "Train: train_drebin [0015] TF1: 99.2318, Tloss: 0.00043231, VF1: 89.7436, VLoss: 0.00340163,\n",
      "Train: train_drebin [0016] TF1: 99.3250, Tloss: 0.00041810, VF1: 97.0205, VLoss: 0.00086642,\n",
      "Train: train_drebin [0017] TF1: 99.4649, Tloss: 0.00036836, VF1: 98.1991, VLoss: 0.00049291,\n",
      "Train: train_drebin [0018] TF1: 99.3951, Tloss: 0.00039155, VF1: 98.2824, VLoss: 0.00045867,\n",
      "Train: train_drebin [0019] TF1: 99.2674, Tloss: 0.00040082, VF1: 98.2857, VLoss: 0.00074333,\n",
      "Train: train_drebin [0020] TF1: 99.3254, Tloss: 0.00042927, VF1: 65.2393, VLoss: 0.01556546,\n",
      "Train: train_drebin [0021] TF1: 99.4412, Tloss: 0.00036478, VF1: 97.7401, VLoss: 0.00062752,\n",
      "Train: train_drebin [0022] TF1: 99.4065, Tloss: 0.00039206, VF1: 97.8846, VLoss: 0.00075146,\n",
      "Train: train_drebin [0023] TF1: 99.3365, Tloss: 0.00042988, VF1: 95.7312, VLoss: 0.00169118,\n",
      "Train: train_drebin [0024] TF1: 99.5810, Tloss: 0.00025321, VF1: 98.2890, VLoss: 0.00055925,\n",
      "Train: train_drebin [0025] TF1: 99.5108, Tloss: 0.00031508, VF1: 97.5701, VLoss: 0.00065966,\n",
      "Train: train_drebin [0026] TF1: 99.4065, Tloss: 0.00035402, VF1: 98.6616, VLoss: 0.00062050,\n",
      "Train: train_drebin [0027] TF1: 99.6742, Tloss: 0.00027267, VF1: 97.6744, VLoss: 0.00070924,\n",
      "Train: train_drebin [0028] TF1: 99.2324, Tloss: 0.00057183, VF1: 97.4884, VLoss: 0.00083789,\n",
      "Train: train_drebin [0029] TF1: 99.5114, Tloss: 0.00033616, VF1: 98.4067, VLoss: 0.00064437,\n",
      "Train: train_drebin [0030] TF1: 99.5929, Tloss: 0.00027840, VF1: 98.2955, VLoss: 0.00072999,\n",
      "Epoch    30: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_drebin [0031] TF1: 99.7556, Tloss: 0.00024325, VF1: 98.0207, VLoss: 0.00069178,\n",
      "Train: train_drebin [0032] TF1: 99.4881, Tloss: 0.00032321, VF1: 98.7678, VLoss: 0.00053864,\n",
      "Train: train_drebin [0033] TF1: 99.5929, Tloss: 0.00025076, VF1: 98.0769, VLoss: 0.00077199,\n",
      "Train: train_drebin [0034] TF1: 99.6512, Tloss: 0.00020855, VF1: 98.5809, VLoss: 0.00065741,\n",
      "Train: train_drebin [0035] TF1: 99.7673, Tloss: 0.00018053, VF1: 98.2092, VLoss: 0.00064360,\n",
      "Train: train_drebin [0036] TF1: 99.7207, Tloss: 0.00018056, VF1: 98.1168, VLoss: 0.00065453,\n",
      "Train: train_drebin [0037] TF1: 99.6626, Tloss: 0.00022937, VF1: 98.2824, VLoss: 0.00065678,\n",
      "Train: train_drebin [0038] TF1: 99.6397, Tloss: 0.00017420, VF1: 97.5746, VLoss: 0.00082640,\n",
      "Train: train_drebin [0039] TF1: 99.6162, Tloss: 0.00023367, VF1: 98.0843, VLoss: 0.00072845,\n",
      "Train: train_drebin [0040] TF1: 99.6743, Tloss: 0.00022474, VF1: 98.5782, VLoss: 0.00056143,\n",
      "Train: train_drebin [0041] TF1: 99.7091, Tloss: 0.00018658, VF1: 98.1991, VLoss: 0.00081119,\n",
      "Train: train_drebin [0042] TF1: 99.8139, Tloss: 0.00017854, VF1: 98.3947, VLoss: 0.00068783,\n",
      "Train: train_drebin [0043] TF1: 99.6628, Tloss: 0.00022063, VF1: 98.5755, VLoss: 0.00063583,\n",
      "Train: train_drebin [0044] TF1: 99.6161, Tloss: 0.00023297, VF1: 98.5809, VLoss: 0.00072042,\n",
      "Epoch    44: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_drebin [0045] TF1: 99.8255, Tloss: 0.00013354, VF1: 98.5728, VLoss: 0.00072733,\n",
      "Train: train_drebin [0046] TF1: 99.8139, Tloss: 0.00012951, VF1: 98.0245, VLoss: 0.00084016,\n",
      "Train: train_drebin [0047] TF1: 99.7556, Tloss: 0.00017370, VF1: 98.7654, VLoss: 0.00057028,\n",
      "Train: train_drebin [0048] TF1: 99.8022, Tloss: 0.00014991, VF1: 98.1238, VLoss: 0.00095382,\n",
      "Train: train_drebin [0049] TF1: 99.8837, Tloss: 0.00008921, VF1: 98.6742, VLoss: 0.00068613,\n",
      "Train: train_drebin [0050] TF1: 99.8371, Tloss: 0.00010561, VF1: 98.4877, VLoss: 0.00075008,\n",
      "Train: train_drebin [0051] TF1: 99.6042, Tloss: 0.00018922, VF1: 96.7682, VLoss: 0.00103928,\n",
      "Train: train_drebin [0052] TF1: 99.7558, Tloss: 0.00014487, VF1: 98.5755, VLoss: 0.00070803,\n",
      "Train: train_drebin [0053] TF1: 99.8371, Tloss: 0.00012179, VF1: 98.1273, VLoss: 0.00070063,\n",
      "Train: train_drebin [0054] TF1: 99.9302, Tloss: 0.00007910, VF1: 60.8696, VLoss: 0.01112743,\n",
      "Train: train_drebin [0055] TF1: 99.7791, Tloss: 0.00013237, VF1: 98.3114, VLoss: 0.00073701,\n",
      "Train: train_drebin [0056] TF1: 99.7209, Tloss: 0.00019840, VF1: 98.3947, VLoss: 0.00058012,\n",
      "Train: train_drebin [0057] TF1: 99.6858, Tloss: 0.00018499, VF1: 98.6692, VLoss: 0.00072225,\n",
      "Train: train_drebin [0058] TF1: 99.7092, Tloss: 0.00026932, VF1: 97.9323, VLoss: 0.00070723,\n",
      "Train: train_drebin [0059] TF1: 99.6740, Tloss: 0.00020346, VF1: 98.7631, VLoss: 0.00059279,\n",
      "Train: train_drebin [0060] TF1: 99.8837, Tloss: 0.00014972, VF1: 98.9573, VLoss: 0.00057398,\n",
      "Epoch    60: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_drebin [0061] TF1: 99.8488, Tloss: 0.00010366, VF1: 98.5755, VLoss: 0.00072659,\n",
      "Train: train_drebin [0062] TF1: 99.8836, Tloss: 0.00011020, VF1: 97.5701, VLoss: 0.00086571,\n",
      "Train: train_drebin [0063] TF1: 99.8372, Tloss: 0.00010462, VF1: 98.5809, VLoss: 0.00074499,\n",
      "Train: train_drebin [0064] TF1: 99.7790, Tloss: 0.00016731, VF1: 98.4820, VLoss: 0.00083438,\n",
      "Train: train_drebin [0065] TF1: 99.8372, Tloss: 0.00014201, VF1: 97.9323, VLoss: 0.00078717,\n",
      "Train: train_drebin [0066] TF1: 99.8720, Tloss: 0.00011125, VF1: 98.7678, VLoss: 0.00080262,\n",
      "Epoch    66: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_drebin [0067] TF1: 99.9070, Tloss: 0.00006683, VF1: 98.5809, VLoss: 0.00081049,\n",
      "Train: train_drebin [0068] TF1: 99.9535, Tloss: 0.00005454, VF1: 98.3947, VLoss: 0.00089400,\n",
      "Train: train_drebin [0069] TF1: 99.8836, Tloss: 0.00008209, VF1: 98.0282, VLoss: 0.00090371,\n",
      "Train: train_drebin [0070] TF1: 99.9069, Tloss: 0.00009590, VF1: 98.3947, VLoss: 0.00077703,\n",
      "Train: train_drebin [0071] TF1: 99.8022, Tloss: 0.00014158, VF1: 97.6570, VLoss: 0.00126329,\n",
      "Train: train_drebin [0072] TF1: 99.8489, Tloss: 0.00011769, VF1: 97.9284, VLoss: 0.00072481,\n",
      "Train: train_drebin [0073] TF1: 99.8604, Tloss: 0.00007410, VF1: 98.6717, VLoss: 0.00068785,\n",
      "Train: train_drebin [0074] TF1: 99.8836, Tloss: 0.00008229, VF1: 98.4877, VLoss: 0.00075876,\n",
      "Epoch    74: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_drebin [0075] TF1: 99.8256, Tloss: 0.00008922, VF1: 98.4848, VLoss: 0.00076757,\n",
      "Train: train_drebin [0076] TF1: 99.9302, Tloss: 0.00007750, VF1: 98.3019, VLoss: 0.00089634,\n",
      "Train: train_drebin [0077] TF1: 99.8604, Tloss: 0.00008685, VF1: 98.1168, VLoss: 0.00074963,\n",
      "Train: train_drebin [0078] TF1: 99.9418, Tloss: 0.00005836, VF1: 98.3947, VLoss: 0.00069597,\n",
      "Train: train_drebin [0079] TF1: 99.9185, Tloss: 0.00005247, VF1: 98.2126, VLoss: 0.00091747,\n",
      "Train: train_drebin [0080] TF1: 99.9419, Tloss: 0.00005324, VF1: 98.3977, VLoss: 0.00069955,\n",
      "Train: train_drebin [0081] TF1: 99.8836, Tloss: 0.00009757, VF1: 98.3947, VLoss: 0.00075546,\n",
      "Train: train_drebin [0082] TF1: 99.8953, Tloss: 0.00009465, VF1: 98.2092, VLoss: 0.00077702,\n",
      "Train: train_drebin [0083] TF1: 99.8953, Tloss: 0.00007443, VF1: 98.4877, VLoss: 0.00073979,\n",
      "Train: train_drebin [0084] TF1: 99.9535, Tloss: 0.00003690, VF1: 98.5836, VLoss: 0.00064099,\n",
      "Train: train_drebin [0085] TF1: 99.9069, Tloss: 0.00007092, VF1: 98.3051, VLoss: 0.00064014,\n",
      "Train: train_drebin [0086] TF1: 99.9302, Tloss: 0.00005625, VF1: 98.5809, VLoss: 0.00086539,\n",
      "Train: train_drebin [0087] TF1: 99.7557, Tloss: 0.00012346, VF1: 98.4791, VLoss: 0.00074998,\n",
      "Train: train_drebin [0088] TF1: 99.9186, Tloss: 0.00006638, VF1: 98.5782, VLoss: 0.00065758,\n",
      "Train: train_drebin [0089] TF1: 99.9069, Tloss: 0.00005862, VF1: 98.6742, VLoss: 0.00072193,\n",
      "Train: train_drebin [0090] TF1: 99.8836, Tloss: 0.00007807, VF1: 96.6790, VLoss: 0.00107091,\n",
      "Epoch    90: reducing learning rate of group 0 to 7.8643e-05.\n",
      "Train: train_drebin [0091] TF1: 99.9186, Tloss: 0.00006413, VF1: 98.2126, VLoss: 0.00072265,\n",
      "Train: train_drebin [0092] TF1: 99.8487, Tloss: 0.00008178, VF1: 98.5809, VLoss: 0.00069732,\n",
      "Train: train_drebin [0093] TF1: 99.9186, Tloss: 0.00004522, VF1: 98.3977, VLoss: 0.00084449,\n",
      "Train: train_drebin [0094] TF1: 99.8488, Tloss: 0.00012060, VF1: 98.3825, VLoss: 0.00075118,\n",
      "Train: train_drebin [0095] TF1: 99.9186, Tloss: 0.00005096, VF1: 98.3886, VLoss: 0.00069335,\n",
      "Train: train_drebin [0096] TF1: 99.9535, Tloss: 0.00005547, VF1: 98.8571, VLoss: 0.00065317,\n",
      "Epoch    96: reducing learning rate of group 0 to 6.2915e-05.\n",
      "Train: train_drebin [0097] TF1: 99.9186, Tloss: 0.00004158, VF1: 98.0132, VLoss: 0.00076028,\n",
      "Train: train_drebin [0098] TF1: 99.9302, Tloss: 0.00003703, VF1: 98.2059, VLoss: 0.00073320,\n",
      "Train: train_drebin [0099] TF1: 99.9302, Tloss: 0.00004575, VF1: 98.1061, VLoss: 0.00077015,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_005.pth Fragment=00 score=0.9842738205365403\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_008.pth Fragment=00 score=0.9879963065558635\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_013.pth Fragment=00 score=0.9860465116279069\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_010.pth Fragment=00 score=0.9841565703634669\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_018.pth Fragment=00 score=0.9822263797942001\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_006.pth Fragment=00 score=0.9878844361602982\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_017.pth Fragment=00 score=0.9888268156424581\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_012.pth Fragment=00 score=0.9879518072289157\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_004.pth Fragment=00 score=0.9794007490636704\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_032.pth Fragment=00 score=0.9898242368177613\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_024.pth Fragment=00 score=0.9925650557620818\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_040.pth Fragment=00 score=0.989786443825441\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_047.pth Fragment=00 score=0.9888268156424581\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_060.pth Fragment=00 score=0.9870129870129869\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_056.pth Fragment=00 score=0.9833024118738405\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_059.pth Fragment=00 score=0.9888059701492536\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_014.pth Fragment=00 score=0.9834254143646409\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_026.pth Fragment=00 score=0.9906367041198502\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_021.pth Fragment=00 score=0.9843893480257118\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_drebin_043.pth Fragment=00 score=0.9907063197026023\n",
      "Extend: f1score=99.1643, vcoverage=98.4472, vf1score=99.8102, vexentdSize=1585, ecoverage=1.5528, ef1score=69.5652, erestSize=25\n",
      "########\n",
      "######## genome\n",
      "6048\n",
      "0    5035\n",
      "1    1013\n",
      "Name: label, dtype: int64\n",
      "756\n",
      "0    639\n",
      "1    117\n",
      "Name: label, dtype: int64\n",
      "756\n",
      "0    626\n",
      "1    130\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_genome [0000] TF1: 92.1521, Tloss: 0.00378318, VF1: 89.6226, VLoss: 0.00112883,\n",
      "Train: train_genome [0001] TF1: 98.1281, Tloss: 0.00101396, VF1: 54.6584, VLoss: 0.00425764,\n",
      "Train: train_genome [0002] TF1: 99.5069, Tloss: 0.00042780, VF1: 98.2759, VLoss: 0.00030626,\n",
      "Train: train_genome [0003] TF1: 98.9120, Tloss: 0.00041190, VF1: 93.6364, VLoss: 0.00110766,\n",
      "Train: train_genome [0004] TF1: 98.8642, Tloss: 0.00042092, VF1: 95.5357, VLoss: 0.00067443,\n",
      "Train: train_genome [0005] TF1: 99.4584, Tloss: 0.00022848, VF1: 98.2609, VLoss: 0.00021455,\n",
      "Train: train_genome [0006] TF1: 99.5569, Tloss: 0.00017010, VF1: 96.4602, VLoss: 0.00057647,\n",
      "Train: train_genome [0007] TF1: 99.8024, Tloss: 0.00016535, VF1: 94.1176, VLoss: 0.00111601,\n",
      "Train: train_genome [0008] TF1: 99.7038, Tloss: 0.00012555, VF1: 96.9957, VLoss: 0.00037594,\n",
      "Train: train_genome [0009] TF1: 99.9013, Tloss: 0.00010007, VF1: 98.2609, VLoss: 0.00032688,\n",
      "Train: train_genome [0010] TF1: 99.9507, Tloss: 0.00005751, VF1: 98.7013, VLoss: 0.00028781,\n",
      "Train: train_genome [0011] TF1: 99.9013, Tloss: 0.00008218, VF1: 98.2759, VLoss: 0.00051331,\n",
      "Train: train_genome [0012] TF1: 99.1613, Tloss: 0.00026353, VF1: 94.6939, VLoss: 0.00075101,\n",
      "Train: train_genome [0013] TF1: 98.7654, Tloss: 0.00039049, VF1: 97.3684, VLoss: 0.00060268,\n",
      "Train: train_genome [0014] TF1: 99.3599, Tloss: 0.00020891, VF1: 97.4790, VLoss: 0.00038741,\n",
      "Train: train_genome [0015] TF1: 99.8024, Tloss: 0.00009205, VF1: 98.7124, VLoss: 0.00030470,\n",
      "Train: train_genome [0016] TF1: 99.9507, Tloss: 0.00005898, VF1: 98.2609, VLoss: 0.00038037,\n",
      "Epoch    16: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_genome [0017] TF1: 100.0000, Tloss: 0.00003293, VF1: 96.9163, VLoss: 0.00067526,\n",
      "Train: train_genome [0018] TF1: 100.0000, Tloss: 0.00002115, VF1: 98.2609, VLoss: 0.00046059,\n",
      "Train: train_genome [0019] TF1: 100.0000, Tloss: 0.00002227, VF1: 99.1379, VLoss: 0.00025301,\n",
      "Train: train_genome [0020] TF1: 100.0000, Tloss: 0.00001539, VF1: 96.4602, VLoss: 0.00068657,\n",
      "Train: train_genome [0021] TF1: 99.5556, Tloss: 0.00017817, VF1: 96.9957, VLoss: 0.00049052,\n",
      "Train: train_genome [0022] TF1: 99.5564, Tloss: 0.00019749, VF1: 96.9163, VLoss: 0.00089188,\n",
      "Train: train_genome [0023] TF1: 99.6547, Tloss: 0.00012284, VF1: 98.2609, VLoss: 0.00052113,\n",
      "Train: train_genome [0024] TF1: 99.4579, Tloss: 0.00015575, VF1: 21.3740, VLoss: 0.01468096,\n",
      "Train: train_genome [0025] TF1: 99.0119, Tloss: 0.00034895, VF1: 96.9163, VLoss: 0.00076789,\n",
      "Train: train_genome [0026] TF1: 99.7533, Tloss: 0.00007183, VF1: 98.7013, VLoss: 0.00040674,\n",
      "Epoch    26: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_genome [0027] TF1: 99.8520, Tloss: 0.00006157, VF1: 97.8166, VLoss: 0.00067381,\n",
      "Train: train_genome [0028] TF1: 99.9012, Tloss: 0.00004441, VF1: 97.4359, VLoss: 0.00036000,\n",
      "Train: train_genome [0029] TF1: 99.7033, Tloss: 0.00006372, VF1: 97.8166, VLoss: 0.00046262,\n",
      "Train: train_genome [0030] TF1: 100.0000, Tloss: 0.00001829, VF1: 98.7013, VLoss: 0.00033141,\n",
      "Train: train_genome [0031] TF1: 99.9507, Tloss: 0.00003093, VF1: 98.7013, VLoss: 0.00027393,\n",
      "Train: train_genome [0032] TF1: 100.0000, Tloss: 0.00001707, VF1: 97.8166, VLoss: 0.00074193,\n",
      "Epoch    32: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_genome [0033] TF1: 100.0000, Tloss: 0.00001006, VF1: 98.2609, VLoss: 0.00053261,\n",
      "Train: train_genome [0034] TF1: 100.0000, Tloss: 0.00001208, VF1: 98.2609, VLoss: 0.00043000,\n",
      "Train: train_genome [0035] TF1: 99.9506, Tloss: 0.00003506, VF1: 96.9163, VLoss: 0.00072661,\n",
      "Train: train_genome [0036] TF1: 99.8026, Tloss: 0.00007341, VF1: 97.3684, VLoss: 0.00080871,\n",
      "Train: train_genome [0037] TF1: 100.0000, Tloss: 0.00002640, VF1: 97.3684, VLoss: 0.00066411,\n",
      "Train: train_genome [0038] TF1: 99.9013, Tloss: 0.00004860, VF1: 97.3913, VLoss: 0.00035702,\n",
      "Train: train_genome [0039] TF1: 99.9507, Tloss: 0.00004036, VF1: 98.7013, VLoss: 0.00023879,\n",
      "Epoch    39: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_genome [0040] TF1: 99.8519, Tloss: 0.00004613, VF1: 97.8166, VLoss: 0.00042330,\n",
      "Train: train_genome [0041] TF1: 99.7041, Tloss: 0.00008845, VF1: 94.5946, VLoss: 0.00105350,\n",
      "Train: train_genome [0042] TF1: 99.7038, Tloss: 0.00009873, VF1: 96.5217, VLoss: 0.00059826,\n",
      "Train: train_genome [0043] TF1: 99.9506, Tloss: 0.00002674, VF1: 97.3913, VLoss: 0.00068771,\n",
      "Train: train_genome [0044] TF1: 100.0000, Tloss: 0.00001823, VF1: 97.8541, VLoss: 0.00044465,\n",
      "Train: train_genome [0045] TF1: 100.0000, Tloss: 0.00000878, VF1: 97.8355, VLoss: 0.00047858,\n",
      "Train: train_genome [0046] TF1: 99.9013, Tloss: 0.00003697, VF1: 98.2609, VLoss: 0.00039127,\n",
      "Train: train_genome [0047] TF1: 99.9507, Tloss: 0.00002153, VF1: 97.8166, VLoss: 0.00048508,\n",
      "Train: train_genome [0048] TF1: 100.0000, Tloss: 0.00001109, VF1: 98.2609, VLoss: 0.00053415,\n",
      "Train: train_genome [0049] TF1: 99.7533, Tloss: 0.00007685, VF1: 98.7013, VLoss: 0.00030849,\n",
      "Train: train_genome [0050] TF1: 99.9013, Tloss: 0.00003636, VF1: 97.8541, VLoss: 0.00041085,\n",
      "Train: train_genome [0051] TF1: 99.8519, Tloss: 0.00004486, VF1: 98.2609, VLoss: 0.00037871,\n",
      "Epoch    51: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_genome [0052] TF1: 99.9013, Tloss: 0.00003938, VF1: 98.2609, VLoss: 0.00037946,\n",
      "Train: train_genome [0053] TF1: 99.9506, Tloss: 0.00002070, VF1: 98.2609, VLoss: 0.00032158,\n",
      "Train: train_genome [0054] TF1: 99.9507, Tloss: 0.00001699, VF1: 96.9163, VLoss: 0.00101656,\n",
      "Train: train_genome [0055] TF1: 99.8519, Tloss: 0.00008921, VF1: 96.9432, VLoss: 0.00067855,\n",
      "Train: train_genome [0056] TF1: 100.0000, Tloss: 0.00000928, VF1: 97.3913, VLoss: 0.00046189,\n",
      "Train: train_genome [0057] TF1: 100.0000, Tloss: 0.00001197, VF1: 97.4138, VLoss: 0.00047938,\n",
      "Epoch    57: reducing learning rate of group 0 to 7.8643e-05.\n",
      "Train: train_genome [0058] TF1: 100.0000, Tloss: 0.00000902, VF1: 97.3913, VLoss: 0.00054349,\n",
      "Train: train_genome [0059] TF1: 99.9507, Tloss: 0.00001192, VF1: 97.3684, VLoss: 0.00069526,\n",
      "Train: train_genome [0060] TF1: 100.0000, Tloss: 0.00000845, VF1: 98.7013, VLoss: 0.00038924,\n",
      "Train: train_genome [0061] TF1: 100.0000, Tloss: 0.00000954, VF1: 97.8166, VLoss: 0.00055253,\n",
      "Train: train_genome [0062] TF1: 100.0000, Tloss: 0.00001855, VF1: 97.8166, VLoss: 0.00059948,\n",
      "Train: train_genome [0063] TF1: 99.9506, Tloss: 0.00001276, VF1: 97.4359, VLoss: 0.00032521,\n",
      "Train: train_genome [0064] TF1: 100.0000, Tloss: 0.00000903, VF1: 98.2609, VLoss: 0.00056317,\n",
      "Train: train_genome [0065] TF1: 100.0000, Tloss: 0.00000777, VF1: 98.2759, VLoss: 0.00033205,\n",
      "Train: train_genome [0066] TF1: 99.9507, Tloss: 0.00002306, VF1: 97.3913, VLoss: 0.00045623,\n",
      "Train: train_genome [0067] TF1: 99.9507, Tloss: 0.00001852, VF1: 97.8355, VLoss: 0.00037948,\n",
      "Train: train_genome [0068] TF1: 100.0000, Tloss: 0.00001134, VF1: 98.2609, VLoss: 0.00038238,\n",
      "Train: train_genome [0069] TF1: 100.0000, Tloss: 0.00000849, VF1: 97.3684, VLoss: 0.00050736,\n",
      "Train: train_genome [0070] TF1: 99.9013, Tloss: 0.00002741, VF1: 96.2656, VLoss: 0.00065815,\n",
      "Train: train_genome [0071] TF1: 100.0000, Tloss: 0.00001792, VF1: 97.3913, VLoss: 0.00052856,\n",
      "Epoch    71: reducing learning rate of group 0 to 6.2915e-05.\n",
      "Train: train_genome [0072] TF1: 99.7038, Tloss: 0.00007431, VF1: 98.2609, VLoss: 0.00062033,\n",
      "Train: train_genome [0073] TF1: 99.9013, Tloss: 0.00004312, VF1: 97.8166, VLoss: 0.00052460,\n",
      "Train: train_genome [0074] TF1: 100.0000, Tloss: 0.00001140, VF1: 98.2609, VLoss: 0.00046221,\n",
      "Train: train_genome [0075] TF1: 99.9506, Tloss: 0.00003146, VF1: 98.7013, VLoss: 0.00031241,\n",
      "Train: train_genome [0076] TF1: 99.9506, Tloss: 0.00002265, VF1: 98.7124, VLoss: 0.00022538,\n",
      "Train: train_genome [0077] TF1: 99.9507, Tloss: 0.00003315, VF1: 97.8166, VLoss: 0.00058023,\n",
      "Epoch    77: reducing learning rate of group 0 to 5.0332e-05.\n",
      "Train: train_genome [0078] TF1: 100.0000, Tloss: 0.00001430, VF1: 97.8166, VLoss: 0.00033635,\n",
      "Train: train_genome [0079] TF1: 99.9507, Tloss: 0.00001939, VF1: 97.8166, VLoss: 0.00035482,\n",
      "Train: train_genome [0080] TF1: 100.0000, Tloss: 0.00000991, VF1: 97.8166, VLoss: 0.00050423,\n",
      "Train: train_genome [0081] TF1: 100.0000, Tloss: 0.00000841, VF1: 98.7013, VLoss: 0.00032776,\n",
      "Train: train_genome [0082] TF1: 100.0000, Tloss: 0.00000585, VF1: 98.2609, VLoss: 0.00036726,\n",
      "Train: train_genome [0083] TF1: 100.0000, Tloss: 0.00000713, VF1: 97.8166, VLoss: 0.00042249,\n",
      "Train: train_genome [0084] TF1: 99.9506, Tloss: 0.00001340, VF1: 98.7013, VLoss: 0.00034787,\n",
      "Train: train_genome [0085] TF1: 100.0000, Tloss: 0.00001069, VF1: 98.7013, VLoss: 0.00021097,\n",
      "Train: train_genome [0086] TF1: 100.0000, Tloss: 0.00001380, VF1: 97.8166, VLoss: 0.00030785,\n",
      "Train: train_genome [0087] TF1: 100.0000, Tloss: 0.00000988, VF1: 98.2609, VLoss: 0.00041237,\n",
      "Train: train_genome [0088] TF1: 100.0000, Tloss: 0.00000535, VF1: 97.8166, VLoss: 0.00056186,\n",
      "Train: train_genome [0089] TF1: 100.0000, Tloss: 0.00000873, VF1: 97.3684, VLoss: 0.00037690,\n",
      "Train: train_genome [0090] TF1: 100.0000, Tloss: 0.00000652, VF1: 98.7013, VLoss: 0.00035566,\n",
      "Train: train_genome [0091] TF1: 100.0000, Tloss: 0.00000628, VF1: 97.3684, VLoss: 0.00067270,\n",
      "Train: train_genome [0092] TF1: 100.0000, Tloss: 0.00000650, VF1: 98.7013, VLoss: 0.00029100,\n",
      "Train: train_genome [0093] TF1: 100.0000, Tloss: 0.00001066, VF1: 97.3684, VLoss: 0.00070407,\n",
      "Train: train_genome [0094] TF1: 100.0000, Tloss: 0.00000632, VF1: 98.2609, VLoss: 0.00043662,\n",
      "Epoch    94: reducing learning rate of group 0 to 4.0265e-05.\n",
      "Train: train_genome [0095] TF1: 100.0000, Tloss: 0.00000611, VF1: 98.7013, VLoss: 0.00037711,\n",
      "Train: train_genome [0096] TF1: 100.0000, Tloss: 0.00000900, VF1: 97.8166, VLoss: 0.00052235,\n",
      "Train: train_genome [0097] TF1: 99.9506, Tloss: 0.00002140, VF1: 98.7013, VLoss: 0.00022279,\n",
      "Train: train_genome [0098] TF1: 100.0000, Tloss: 0.00000725, VF1: 99.1379, VLoss: 0.00017509,\n",
      "Train: train_genome [0099] TF1: 100.0000, Tloss: 0.00000791, VF1: 98.7013, VLoss: 0.00036793,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_098.pth Fragment=00 score=0.980544747081712\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_085.pth Fragment=00 score=0.9846153846153847\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_005.pth Fragment=00 score=0.9844961240310077\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_097.pth Fragment=00 score=0.9884169884169884\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_076.pth Fragment=00 score=0.9884169884169884\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_039.pth Fragment=00 score=0.9767441860465116\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_019.pth Fragment=00 score=0.9844961240310077\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_031.pth Fragment=00 score=0.980544747081712\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_010.pth Fragment=00 score=0.9883268482490272\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_092.pth Fragment=00 score=0.980544747081712\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_015.pth Fragment=00 score=0.9884169884169884\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_002.pth Fragment=00 score=0.9767441860465116\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_086.pth Fragment=00 score=0.9884169884169884\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_049.pth Fragment=00 score=0.980544747081712\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_075.pth Fragment=00 score=0.9844961240310077\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_053.pth Fragment=00 score=0.9844961240310077\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_063.pth Fragment=00 score=0.9808429118773947\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_009.pth Fragment=00 score=0.9922480620155039\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_081.pth Fragment=00 score=0.980544747081712\n",
      "Evaluate: ModelPath=./traces/detectionWS03/model_train_genome_030.pth Fragment=00 score=0.9884169884169884\n",
      "Extend: f1score=98.8417, vcoverage=99.7354, vf1score=98.8235, vexentdSize=754, ecoverage=0.2646, ef1score=100.0000, erestSize=2\n",
      "########\n",
      "######## amd\n",
      "55826\n",
      "0    37222\n",
      "1    18604\n",
      "Name: label, dtype: int64\n",
      "6978\n",
      "0    4649\n",
      "1    2329\n",
      "Name: label, dtype: int64\n",
      "6979\n",
      "0    4651\n",
      "1    2328\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_amd [0000] TF1: 95.6773, Tloss: 0.00255217, VF1: 90.2072, VLoss: 0.00229016,\n",
      "Train: train_amd [0001] TF1: 98.0002, Tloss: 0.00133227, VF1: 86.4523, VLoss: 0.00411747,\n",
      "Train: train_amd [0002] TF1: 98.2209, Tloss: 0.00116391, VF1: 95.3191, VLoss: 0.00124119,\n",
      "Train: train_amd [0003] TF1: 98.4284, Tloss: 0.00102966, VF1: 98.6839, VLoss: 0.00042570,\n",
      "Train: train_amd [0004] TF1: 98.6783, Tloss: 0.00092175, VF1: 96.0281, VLoss: 0.00114913,\n",
      "Train: train_amd [0005] TF1: 98.7132, Tloss: 0.00087044, VF1: 93.9486, VLoss: 0.00193386,\n",
      "Train: train_amd [0006] TF1: 98.6540, Tloss: 0.00086411, VF1: 98.0533, VLoss: 0.00069197,\n",
      "Train: train_amd [0007] TF1: 98.8796, Tloss: 0.00077948, VF1: 98.5696, VLoss: 0.00051685,\n",
      "Train: train_amd [0008] TF1: 98.7459, Tloss: 0.00080597, VF1: 98.3614, VLoss: 0.00050506,\n",
      "Train: train_amd [0009] TF1: 98.9397, Tloss: 0.00070532, VF1: 98.9686, VLoss: 0.00034676,\n",
      "Train: train_amd [0010] TF1: 98.9634, Tloss: 0.00068798, VF1: 96.8227, VLoss: 0.00104361,\n",
      "Train: train_amd [0011] TF1: 99.0714, Tloss: 0.00064643, VF1: 96.7326, VLoss: 0.00099284,\n",
      "Train: train_amd [0012] TF1: 99.1280, Tloss: 0.00063202, VF1: 97.6636, VLoss: 0.00075889,\n",
      "Train: train_amd [0013] TF1: 99.0528, Tloss: 0.00063756, VF1: 98.9298, VLoss: 0.00038369,\n",
      "Train: train_amd [0014] TF1: 99.1412, Tloss: 0.00062335, VF1: 73.5280, VLoss: 0.01335909,\n",
      "Train: train_amd [0015] TF1: 99.2760, Tloss: 0.00053423, VF1: 98.9633, VLoss: 0.00035810,\n",
      "Train: train_amd [0016] TF1: 99.1334, Tloss: 0.00058822, VF1: 98.4675, VLoss: 0.00060717,\n",
      "Train: train_amd [0017] TF1: 99.2062, Tloss: 0.00055530, VF1: 97.5579, VLoss: 0.00069280,\n",
      "Train: train_amd [0018] TF1: 99.2250, Tloss: 0.00055723, VF1: 98.5873, VLoss: 0.00054220,\n",
      "Train: train_amd [0019] TF1: 99.2626, Tloss: 0.00053615, VF1: 99.0497, VLoss: 0.00030014,\n",
      "Train: train_amd [0020] TF1: 99.2062, Tloss: 0.00056624, VF1: 97.8779, VLoss: 0.00071259,\n",
      "Train: train_amd [0021] TF1: 99.2704, Tloss: 0.00054024, VF1: 99.1435, VLoss: 0.00032784,\n",
      "Epoch    21: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_amd [0022] TF1: 99.3217, Tloss: 0.00049573, VF1: 99.0719, VLoss: 0.00030058,\n",
      "Train: train_amd [0023] TF1: 99.3649, Tloss: 0.00045117, VF1: 98.3878, VLoss: 0.00059534,\n",
      "Train: train_amd [0024] TF1: 99.4109, Tloss: 0.00043670, VF1: 99.0979, VLoss: 0.00029021,\n",
      "Train: train_amd [0025] TF1: 99.3567, Tloss: 0.00046360, VF1: 99.2694, VLoss: 0.00026650,\n",
      "Train: train_amd [0026] TF1: 99.3999, Tloss: 0.00044577, VF1: 98.6985, VLoss: 0.00044967,\n",
      "Train: train_amd [0027] TF1: 99.3678, Tloss: 0.00046738, VF1: 98.9847, VLoss: 0.00035117,\n",
      "Train: train_amd [0028] TF1: 99.4270, Tloss: 0.00042106, VF1: 98.8088, VLoss: 0.00038136,\n",
      "Train: train_amd [0029] TF1: 99.5103, Tloss: 0.00039031, VF1: 99.0578, VLoss: 0.00036793,\n",
      "Train: train_amd [0030] TF1: 99.4107, Tloss: 0.00043361, VF1: 99.0594, VLoss: 0.00033616,\n",
      "Train: train_amd [0031] TF1: 99.4216, Tloss: 0.00040608, VF1: 99.0179, VLoss: 0.00030310,\n",
      "Train: train_amd [0032] TF1: 99.4298, Tloss: 0.00043603, VF1: 98.9537, VLoss: 0.00033311,\n",
      "Train: train_amd [0033] TF1: 99.4378, Tloss: 0.00040406, VF1: 99.1859, VLoss: 0.00030502,\n",
      "Train: train_amd [0034] TF1: 99.4755, Tloss: 0.00042661, VF1: 99.2265, VLoss: 0.00027534,\n",
      "Train: train_amd [0035] TF1: 99.4648, Tloss: 0.00038738, VF1: 99.2923, VLoss: 0.00028689,\n",
      "Train: train_amd [0036] TF1: 99.4541, Tloss: 0.00040850, VF1: 99.2475, VLoss: 0.00028944,\n",
      "Train: train_amd [0037] TF1: 99.4781, Tloss: 0.00039614, VF1: 99.3320, VLoss: 0.00024958,\n",
      "Train: train_amd [0038] TF1: 99.4889, Tloss: 0.00038894, VF1: 99.2028, VLoss: 0.00027460,\n",
      "Train: train_amd [0039] TF1: 99.4650, Tloss: 0.00039976, VF1: 99.3112, VLoss: 0.00024082,\n",
      "Train: train_amd [0040] TF1: 99.5588, Tloss: 0.00035186, VF1: 98.8521, VLoss: 0.00040018,\n",
      "Train: train_amd [0041] TF1: 99.4431, Tloss: 0.00040002, VF1: 99.2271, VLoss: 0.00025868,\n",
      "Train: train_amd [0042] TF1: 99.4916, Tloss: 0.00039122, VF1: 99.2285, VLoss: 0.00027916,\n",
      "Train: train_amd [0043] TF1: 99.5023, Tloss: 0.00035309, VF1: 99.3775, VLoss: 0.00026810,\n",
      "Train: train_amd [0044] TF1: 99.4137, Tloss: 0.00040043, VF1: 99.2275, VLoss: 0.00025321,\n",
      "Train: train_amd [0045] TF1: 99.4971, Tloss: 0.00034715, VF1: 99.0968, VLoss: 0.00029735,\n",
      "Train: train_amd [0046] TF1: 99.4996, Tloss: 0.00037164, VF1: 98.8093, VLoss: 0.00039606,\n",
      "Train: train_amd [0047] TF1: 99.5212, Tloss: 0.00036681, VF1: 98.1212, VLoss: 0.00056262,\n",
      "Train: train_amd [0048] TF1: 99.4888, Tloss: 0.00037599, VF1: 99.1604, VLoss: 0.00029474,\n",
      "Train: train_amd [0049] TF1: 99.4406, Tloss: 0.00039093, VF1: 99.1817, VLoss: 0.00026347,\n",
      "Train: train_amd [0050] TF1: 99.5562, Tloss: 0.00036117, VF1: 99.2491, VLoss: 0.00024834,\n",
      "Train: train_amd [0051] TF1: 99.4997, Tloss: 0.00035043, VF1: 99.3767, VLoss: 0.00025156,\n",
      "Epoch    51: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_amd [0052] TF1: 99.5751, Tloss: 0.00032727, VF1: 99.1045, VLoss: 0.00033091,\n",
      "Train: train_amd [0053] TF1: 99.6020, Tloss: 0.00030538, VF1: 99.0578, VLoss: 0.00037743,\n",
      "Train: train_amd [0054] TF1: 99.6047, Tloss: 0.00031053, VF1: 99.2251, VLoss: 0.00027492,\n",
      "Train: train_amd [0055] TF1: 99.5400, Tloss: 0.00032223, VF1: 99.2258, VLoss: 0.00028229,\n",
      "Train: train_amd [0056] TF1: 99.6236, Tloss: 0.00029430, VF1: 98.4776, VLoss: 0.00054490,\n",
      "Train: train_amd [0057] TF1: 99.5670, Tloss: 0.00033612, VF1: 99.2092, VLoss: 0.00026181,\n",
      "Train: train_amd [0058] TF1: 99.5696, Tloss: 0.00030572, VF1: 99.4195, VLoss: 0.00022427,\n",
      "Train: train_amd [0059] TF1: 99.6154, Tloss: 0.00031429, VF1: 99.1143, VLoss: 0.00033959,\n",
      "Train: train_amd [0060] TF1: 99.6612, Tloss: 0.00026658, VF1: 99.3338, VLoss: 0.00027811,\n",
      "Train: train_amd [0061] TF1: 99.5670, Tloss: 0.00033658, VF1: 99.2065, VLoss: 0.00027263,\n",
      "Train: train_amd [0062] TF1: 99.6128, Tloss: 0.00030458, VF1: 99.0940, VLoss: 0.00030369,\n",
      "Train: train_amd [0063] TF1: 99.5912, Tloss: 0.00031051, VF1: 99.3540, VLoss: 0.00022578,\n",
      "Train: train_amd [0064] TF1: 99.5939, Tloss: 0.00030475, VF1: 99.0606, VLoss: 0.00032299,\n",
      "Train: train_amd [0065] TF1: 99.5670, Tloss: 0.00030579, VF1: 99.4200, VLoss: 0.00023083,\n",
      "Train: train_amd [0066] TF1: 99.6073, Tloss: 0.00028883, VF1: 98.9847, VLoss: 0.00035829,\n",
      "Epoch    66: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_amd [0067] TF1: 99.6288, Tloss: 0.00027673, VF1: 99.3562, VLoss: 0.00020382,\n",
      "Train: train_amd [0068] TF1: 99.6665, Tloss: 0.00027328, VF1: 99.4007, VLoss: 0.00023419,\n",
      "Train: train_amd [0069] TF1: 99.6504, Tloss: 0.00027749, VF1: 99.3352, VLoss: 0.00025305,\n",
      "Train: train_amd [0070] TF1: 99.7204, Tloss: 0.00020565, VF1: 99.3557, VLoss: 0.00022094,\n",
      "Train: train_amd [0071] TF1: 99.6692, Tloss: 0.00027278, VF1: 99.3118, VLoss: 0.00023367,\n",
      "Train: train_amd [0072] TF1: 99.6720, Tloss: 0.00025845, VF1: 99.2034, VLoss: 0.00031964,\n",
      "Train: train_amd [0073] TF1: 99.6693, Tloss: 0.00026553, VF1: 99.3786, VLoss: 0.00026013,\n",
      "Train: train_amd [0074] TF1: 99.6718, Tloss: 0.00027189, VF1: 99.3754, VLoss: 0.00021532,\n",
      "Train: train_amd [0075] TF1: 99.6800, Tloss: 0.00025131, VF1: 99.1597, VLoss: 0.00030050,\n",
      "Train: train_amd [0076] TF1: 99.6289, Tloss: 0.00027829, VF1: 99.2449, VLoss: 0.00025747,\n",
      "Epoch    76: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_amd [0077] TF1: 99.7312, Tloss: 0.00021695, VF1: 99.2899, VLoss: 0.00025613,\n",
      "Train: train_amd [0078] TF1: 99.7418, Tloss: 0.00021380, VF1: 99.2238, VLoss: 0.00028827,\n",
      "Train: train_amd [0079] TF1: 99.6962, Tloss: 0.00023310, VF1: 99.2923, VLoss: 0.00024605,\n",
      "Train: train_amd [0080] TF1: 99.6988, Tloss: 0.00025442, VF1: 99.3759, VLoss: 0.00022932,\n",
      "Train: train_amd [0081] TF1: 99.7204, Tloss: 0.00020449, VF1: 99.4187, VLoss: 0.00023307,\n",
      "Train: train_amd [0082] TF1: 99.6907, Tloss: 0.00023033, VF1: 99.1629, VLoss: 0.00028473,\n",
      "Train: train_amd [0083] TF1: 99.7096, Tloss: 0.00022137, VF1: 99.4622, VLoss: 0.00020131,\n",
      "Train: train_amd [0084] TF1: 99.7472, Tloss: 0.00021526, VF1: 99.3981, VLoss: 0.00025007,\n",
      "Train: train_amd [0085] TF1: 99.7258, Tloss: 0.00020704, VF1: 99.1135, VLoss: 0.00032400,\n",
      "Train: train_amd [0086] TF1: 99.7069, Tloss: 0.00025457, VF1: 99.3557, VLoss: 0.00027226,\n",
      "Train: train_amd [0087] TF1: 99.6935, Tloss: 0.00022207, VF1: 99.3997, VLoss: 0.00025473,\n",
      "Epoch    87: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_amd [0088] TF1: 99.7042, Tloss: 0.00021329, VF1: 99.2501, VLoss: 0.00026163,\n",
      "Train: train_amd [0089] TF1: 99.7150, Tloss: 0.00021720, VF1: 99.2462, VLoss: 0.00028020,\n",
      "Train: train_amd [0090] TF1: 99.7446, Tloss: 0.00021045, VF1: 99.6353, VLoss: 0.00016435,\n",
      "Train: train_amd [0091] TF1: 99.7499, Tloss: 0.00019177, VF1: 99.4634, VLoss: 0.00019469,\n",
      "Train: train_amd [0092] TF1: 99.7445, Tloss: 0.00020902, VF1: 99.5068, VLoss: 0.00020331,\n"
     ]
    }
   ],
   "source": [
    "dataset_metaList = glob.glob('/ws/mnt/local/data/output/meta/*')\n",
    "for datasetMetaPath in dataset_metaList:\n",
    "\n",
    "    currentTag = datasetMetaPath.split('/')[-1].split('.')[0]\n",
    "\n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    #\n",
    "    dataset_df = pd.read_parquet(datasetMetaPath)\n",
    "\n",
    "    #\n",
    "    trainLoader, validLoader, testLoader = getDataloaders(dataset_df, trainPercentage=trainPercentageParam, \n",
    "                                                                      validPercentage=validPercentageParam)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "    #\n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "    #\n",
    "    evalDataset(ws, evalresult_df, probaUpperBorn = 0.8,  probaLowerBorn = 0.2)\n",
    "\n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    #\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
