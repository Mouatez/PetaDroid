{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def extendDataset(ws, result_df, probaUpperBorn = 0.8,  probaLowerBorn = 0.2):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    extend_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })\n",
    "    \n",
    "    rest_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })    \n",
    "    \n",
    "    return extend_df, rest_df\n",
    "\n",
    "def getDataloaders(dataset_df, otest_df, ntest_df, batchSize=32, numWorkers=16, trainPercentage = 0.8):\n",
    "    rand_idx = np.random.RandomState(seed=54).permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(otest_df))\n",
    "    print(otest_df.label.value_counts())\n",
    "    print(len(ntest_df))\n",
    "    print(ntest_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    otestDataset = SampleDataset(otest_df.filePath.values, otest_df.label.values)\n",
    "    otestLoader  = DataLoader(otestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    ntestDataset = SampleDataset(ntest_df.filePath.values, ntest_df.label.values)\n",
    "    ntestLoader  = DataLoader(ntestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, otestLoader, ntestLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962411\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>sha1</th>\n",
       "      <th>md5</th>\n",
       "      <th>dex_date</th>\n",
       "      <th>apk_size</th>\n",
       "      <th>pkg_name</th>\n",
       "      <th>vercode</th>\n",
       "      <th>vt_detection</th>\n",
       "      <th>vt_scan_date</th>\n",
       "      <th>dex_size</th>\n",
       "      <th>markets</th>\n",
       "      <th>label</th>\n",
       "      <th>filePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vt_scan_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>75956</td>\n",
       "      <td>75954</td>\n",
       "      <td>75954</td>\n",
       "      <td>75950</td>\n",
       "      <td>75956</td>\n",
       "      <td>75956</td>\n",
       "      <td>75783</td>\n",
       "      <td>75956</td>\n",
       "      <td>75956</td>\n",
       "      <td>75956</td>\n",
       "      <td>75956</td>\n",
       "      <td>75956</td>\n",
       "      <td>75956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31</th>\n",
       "      <td>185474</td>\n",
       "      <td>185442</td>\n",
       "      <td>185442</td>\n",
       "      <td>185470</td>\n",
       "      <td>185474</td>\n",
       "      <td>185474</td>\n",
       "      <td>185298</td>\n",
       "      <td>185474</td>\n",
       "      <td>185474</td>\n",
       "      <td>185474</td>\n",
       "      <td>185474</td>\n",
       "      <td>185474</td>\n",
       "      <td>185474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "      <td>65808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "      <td>266897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "      <td>60442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "      <td>213751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "      <td>94083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sha256    sha1     md5  dex_date  apk_size  pkg_name  vercode  \\\n",
       "vt_scan_date                                                                  \n",
       "2013-12-31     75956   75954   75954     75950     75956     75956    75783   \n",
       "2014-12-31    185474  185442  185442    185470    185474    185474   185298   \n",
       "2015-12-31     65808   65808   65808     65808     65808     65808    65808   \n",
       "2016-12-31    266897  266897  266897    266897    266897    266897   266897   \n",
       "2017-12-31     60442   60442   60442     60442     60442     60442    60442   \n",
       "2018-12-31    213751  213751  213751    213751    213751    213751   213751   \n",
       "2019-12-31     94083   94083   94083     94083     94083     94083    94083   \n",
       "\n",
       "              vt_detection  vt_scan_date  dex_size  markets   label  filePath  \n",
       "vt_scan_date                                                                   \n",
       "2013-12-31           75956         75956     75956    75956   75956     75956  \n",
       "2014-12-31          185474        185474    185474   185474  185474    185474  \n",
       "2015-12-31           65808         65808     65808    65808   65808     65808  \n",
       "2016-12-31          266897        266897    266897   266897  266897    266897  \n",
       "2017-12-31           60442         60442     60442    60442   60442     60442  \n",
       "2018-12-31          213751        213751    213751   213751  213751    213751  \n",
       "2019-12-31           94083         94083     94083    94083   94083     94083  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdataset_df = pd.read_parquet('dataset/mdataset.parquet')\n",
    "print(len(mdataset_df))\n",
    "\n",
    "malware_overtime = mdataset_df.resample('1Y', on='vt_scan_date', convention='end')\n",
    "print(len(malware_overtime.count()))\n",
    "malware_overtime.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ws               = 'ws062'\n",
    "epochNum         = 30\n",
    "dataset_rootDir  = '/ws/mnt/local/data/zoo/'\n",
    "device           = torch.device('cuda:5')\n",
    "ensembleSize     = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lst     = list()\n",
    "overtime_result = list()\n",
    "initial_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[0], \n",
    "                            'label'   : malware_overtime['label'].apply(list).iloc[0] })\n",
    "\n",
    "dataset_lst.append(initial_df)\n",
    "timeTags = list(malware_overtime.count().index)\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 20141231\n",
      "68360\n",
      "0    42094\n",
      "1    26266\n",
      "Name: label, dtype: int64\n",
      "7596\n",
      "0    4686\n",
      "1    2910\n",
      "Name: label, dtype: int64\n",
      "50000\n",
      "0    30708\n",
      "1    19292\n",
      "Name: label, dtype: int64\n",
      "50000\n",
      "0    33805\n",
      "1    16195\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20141231 [0000] TF1: 94.8873, Tloss: 0.00349565, VF1: 96.4674, VLoss: 0.00145871,\n",
      "Train: train_20141231 [0001] TF1: 96.4853, Tloss: 0.00254580, VF1: 86.7964, VLoss: 0.00421035,\n",
      "Train: train_20141231 [0002] TF1: 96.7302, Tloss: 0.00235303, VF1: 94.3639, VLoss: 0.00196300,\n",
      "Train: train_20141231 [0003] TF1: 97.0137, Tloss: 0.00219596, VF1: 95.4988, VLoss: 0.00143294,\n",
      "Train: train_20141231 [0004] TF1: 97.1142, Tloss: 0.00214045, VF1: 97.8856, VLoss: 0.00100214,\n",
      "Train: train_20141231 [0005] TF1: 97.2783, Tloss: 0.00201217, VF1: 96.9729, VLoss: 0.00120001,\n",
      "Train: train_20141231 [0006] TF1: 97.3680, Tloss: 0.00190073, VF1: 97.4743, VLoss: 0.00093247,\n",
      "Train: train_20141231 [0007] TF1: 97.4997, Tloss: 0.00179884, VF1: 97.4957, VLoss: 0.00088343,\n",
      "Train: train_20141231 [0008] TF1: 97.6337, Tloss: 0.00171667, VF1: 97.7840, VLoss: 0.00096654,\n",
      "Train: train_20141231 [0009] TF1: 97.6109, Tloss: 0.00170274, VF1: 96.7107, VLoss: 0.00113967,\n",
      "Train: train_20141231 [0010] TF1: 97.8487, Tloss: 0.00165595, VF1: 97.4233, VLoss: 0.00098177,\n",
      "Train: train_20141231 [0011] TF1: 97.9834, Tloss: 0.00151323, VF1: 97.6281, VLoss: 0.00083053,\n",
      "Train: train_20141231 [0012] TF1: 97.8934, Tloss: 0.00153862, VF1: 97.6403, VLoss: 0.00101408,\n",
      "Train: train_20141231 [0013] TF1: 98.0398, Tloss: 0.00147204, VF1: 97.1388, VLoss: 0.00097029,\n",
      "Train: train_20141231 [0014] TF1: 97.9892, Tloss: 0.00145243, VF1: 97.8855, VLoss: 0.00076305,\n",
      "Train: train_20141231 [0015] TF1: 98.1216, Tloss: 0.00141398, VF1: 97.9196, VLoss: 0.00079603,\n",
      "Train: train_20141231 [0016] TF1: 98.2064, Tloss: 0.00135193, VF1: 97.9684, VLoss: 0.00081902,\n",
      "Train: train_20141231 [0017] TF1: 98.1854, Tloss: 0.00135279, VF1: 97.3886, VLoss: 0.00097756,\n",
      "Train: train_20141231 [0018] TF1: 98.2319, Tloss: 0.00131380, VF1: 97.7436, VLoss: 0.00086797,\n",
      "Train: train_20141231 [0019] TF1: 98.1911, Tloss: 0.00129574, VF1: 97.7617, VLoss: 0.00094746,\n",
      "Train: train_20141231 [0020] TF1: 98.2234, Tloss: 0.00127690, VF1: 97.9101, VLoss: 0.00083820,\n",
      "Train: train_20141231 [0021] TF1: 98.3152, Tloss: 0.00122382, VF1: 97.8517, VLoss: 0.00090089,\n",
      "Train: train_20141231 [0022] TF1: 98.3555, Tloss: 0.00120181, VF1: 97.0056, VLoss: 0.00097513,\n",
      "Train: train_20141231 [0023] TF1: 98.3372, Tloss: 0.00122488, VF1: 97.7977, VLoss: 0.00078144,\n",
      "Train: train_20141231 [0024] TF1: 98.3599, Tloss: 0.00119214, VF1: 98.2759, VLoss: 0.00068406,\n",
      "Train: train_20141231 [0025] TF1: 98.4726, Tloss: 0.00118643, VF1: 97.8425, VLoss: 0.00077423,\n",
      "Train: train_20141231 [0026] TF1: 98.4248, Tloss: 0.00112285, VF1: 98.0996, VLoss: 0.00076041,\n",
      "Train: train_20141231 [0027] TF1: 98.4482, Tloss: 0.00109822, VF1: 97.8102, VLoss: 0.00093310,\n",
      "Train: train_20141231 [0028] TF1: 98.4198, Tloss: 0.00112108, VF1: 98.0799, VLoss: 0.00073024,\n",
      "Train: train_20141231 [0029] TF1: 98.4684, Tloss: 0.00112694, VF1: 97.9521, VLoss: 0.00086174,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/ws062/model_train_20141231_024.pth Fragment=00 score=0.8931974148844029\n",
      "Evaluate: ModelPath=./traces/ws062/model_train_20141231_028.pth Fragment=00 score=0.8935929439174571\n",
      "Evaluate: ModelPath=./traces/ws062/model_train_20141231_026.pth Fragment=00 score=0.8998767114398807\n",
      "Evaluate: ModelPath=./traces/ws062/model_train_20141231_014.pth Fragment=00 score=0.9009761067472968\n",
      "Evaluate: ModelPath=./traces/ws062/model_train_20141231_025.pth Fragment=00 score=0.9041939456796525\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(timeTags)):\n",
    "    currentTag = timeTags[idx].isoformat().split('T')[0].replace('-', '')\n",
    "    \n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #\n",
    "    otest_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[idx-1], \n",
    "                              'label'   : malware_overtime['label'].apply(list).iloc[idx-1] }).sample(50000, random_state=54, replace=True)\n",
    "    \n",
    "    #\n",
    "    ntest_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[idx], \n",
    "                              'label'   : malware_overtime['label'].apply(list).iloc[idx] }).sample(50000, random_state=54, replace=True)\n",
    "\n",
    "    \n",
    "    dataset_df = pd.concat(dataset_lst)\n",
    "    trainLoader, validLoader, otestLoader, ntestLoader = getDataloaders(dataset_df, otest_df, ntest_df, trainPercentage=0.9)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "    \n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, ntestLoader, device)\n",
    "    exresult_df   = evaluate(ws, selectedModelPaths, otestLoader, device)\n",
    "    \n",
    "    extend_df, _  = extendDataset(ws, evalresult_df, probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "    _, rest_df    = extendDataset(ws, exresult_df,   probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "\n",
    "    #\n",
    "    dataset_lst.append(extend_df)\n",
    "\n",
    "    #\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df, exresult_df, dataset_lst, rest_df)], \n",
    "                                     columns=['TimeTag', 'models', 'evalResuls', \n",
    "                                              'extendResults', 'datasetList', \n",
    "                                              'restDataset'])\n",
    "    \n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
