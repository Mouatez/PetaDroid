{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def extendDataset(ws, result_df, probaUpperBorn = 0.8,  probaLowerBorn = 0.2):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    extend_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })\n",
    "    \n",
    "    rest_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })    \n",
    "    \n",
    "    return extend_df, rest_df\n",
    "\n",
    "def getDataloaders(dataset_df, otest_df, ntest_df, batchSize=32, numWorkers=16, trainPercentage = 0.8):\n",
    "    rand_idx = np.random.RandomState(seed=54).permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(otest_df))\n",
    "    print(otest_df.label.value_counts())\n",
    "    print(len(ntest_df))\n",
    "    print(ntest_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    otestDataset = SampleDataset(otest_df.filePath.values, otest_df.label.values)\n",
    "    otestLoader  = DataLoader(otestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    ntestDataset = SampleDataset(ntest_df.filePath.values, ntest_df.label.values)\n",
    "    ntestLoader  = DataLoader(ntestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, otestLoader, ntestLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ws               = 'studyWS05'\n",
    "epochNum         = 50\n",
    "dataset_rootDir  = '/ws/mnt/local/data/zoo/'\n",
    "device           = torch.device('cuda:1')\n",
    "ensembleSize     = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedf = pd.read_parquet('dataset/timedf.parquet')\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lst     = list()\n",
    "overtime_result = list()\n",
    "\n",
    "initial_df = timedf.loc[timedf.year == 2013]\n",
    "dataset_lst.append(initial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 2014\n",
      "8000\n",
      "1    4043\n",
      "0    3957\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "0    1043\n",
      "1     957\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2014 [0000] TF1: 94.7922, Tloss: 0.00454611, VF1: 91.8033, VLoss: 0.00340146,\n",
      "Train: train_2014 [0001] TF1: 97.5221, Tloss: 0.00228906, VF1: 95.9481, VLoss: 0.00193433,\n",
      "Train: train_2014 [0002] TF1: 98.1249, Tloss: 0.00181175, VF1: 85.8951, VLoss: 0.00548506,\n",
      "Train: train_2014 [0003] TF1: 98.4894, Tloss: 0.00146706, VF1: 88.3613, VLoss: 0.00493631,\n",
      "Train: train_2014 [0004] TF1: 98.6240, Tloss: 0.00126724, VF1: 78.4858, VLoss: 0.00824524,\n",
      "Train: train_2014 [0005] TF1: 98.4871, Tloss: 0.00139749, VF1: 94.4112, VLoss: 0.00258923,\n",
      "Train: train_2014 [0006] TF1: 99.1087, Tloss: 0.00096319, VF1: 96.3402, VLoss: 0.00206205,\n",
      "Train: train_2014 [0007] TF1: 98.9967, Tloss: 0.00101753, VF1: 93.2612, VLoss: 0.00300264,\n",
      "Train: train_2014 [0008] TF1: 98.7013, Tloss: 0.00117126, VF1: 95.9632, VLoss: 0.00168133,\n",
      "Train: train_2014 [0009] TF1: 99.0349, Tloss: 0.00094865, VF1: 95.7089, VLoss: 0.00225394,\n",
      "Train: train_2014 [0010] TF1: 99.2323, Tloss: 0.00078006, VF1: 81.4846, VLoss: 0.00964068,\n",
      "Train: train_2014 [0011] TF1: 98.9599, Tloss: 0.00080392, VF1: 94.8059, VLoss: 0.00285709,\n",
      "Train: train_2014 [0012] TF1: 98.9841, Tloss: 0.00087776, VF1: 96.0245, VLoss: 0.00167772,\n",
      "Train: train_2014 [0013] TF1: 99.4065, Tloss: 0.00058974, VF1: 93.4185, VLoss: 0.00318201,\n",
      "Train: train_2014 [0014] TF1: 99.0710, Tloss: 0.00090282, VF1: 93.5500, VLoss: 0.00289019,\n",
      "Train: train_2014 [0015] TF1: 99.3182, Tloss: 0.00064807, VF1: 96.4267, VLoss: 0.00181387,\n",
      "Train: train_2014 [0016] TF1: 99.2827, Tloss: 0.00061220, VF1: 96.6951, VLoss: 0.00167691,\n",
      "Train: train_2014 [0017] TF1: 99.2815, Tloss: 0.00071814, VF1: 96.5147, VLoss: 0.00164958,\n",
      "Train: train_2014 [0018] TF1: 99.1954, Tloss: 0.00067262, VF1: 86.6549, VLoss: 0.00504075,\n",
      "Train: train_2014 [0019] TF1: 99.3073, Tloss: 0.00065215, VF1: 98.1666, VLoss: 0.00108050,\n",
      "Epoch    19: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2014 [0020] TF1: 99.3803, Tloss: 0.00060073, VF1: 92.4721, VLoss: 0.00381586,\n",
      "Train: train_2014 [0021] TF1: 99.2939, Tloss: 0.00057172, VF1: 94.1000, VLoss: 0.00274010,\n",
      "Train: train_2014 [0022] TF1: 99.5548, Tloss: 0.00044062, VF1: 97.3978, VLoss: 0.00144260,\n",
      "Train: train_2014 [0023] TF1: 99.5547, Tloss: 0.00040488, VF1: 98.1191, VLoss: 0.00117383,\n",
      "Train: train_2014 [0024] TF1: 99.4059, Tloss: 0.00046852, VF1: 97.6452, VLoss: 0.00128976,\n",
      "Train: train_2014 [0025] TF1: 99.5179, Tloss: 0.00046593, VF1: 97.6793, VLoss: 0.00144404,\n",
      "Train: train_2014 [0026] TF1: 99.6043, Tloss: 0.00034958, VF1: 97.0359, VLoss: 0.00158971,\n",
      "Train: train_2014 [0027] TF1: 99.6411, Tloss: 0.00037086, VF1: 97.5636, VLoss: 0.00155383,\n",
      "Train: train_2014 [0028] TF1: 99.4928, Tloss: 0.00046494, VF1: 94.2985, VLoss: 0.00321643,\n",
      "Train: train_2014 [0029] TF1: 99.6167, Tloss: 0.00040856, VF1: 97.7261, VLoss: 0.00136383,\n",
      "Train: train_2014 [0030] TF1: 99.6908, Tloss: 0.00028690, VF1: 97.7285, VLoss: 0.00148576,\n",
      "Train: train_2014 [0031] TF1: 99.8269, Tloss: 0.00019554, VF1: 97.9915, VLoss: 0.00144666,\n",
      "Train: train_2014 [0032] TF1: 99.5176, Tloss: 0.00046701, VF1: 96.5035, VLoss: 0.00237062,\n",
      "Train: train_2014 [0033] TF1: 99.2576, Tloss: 0.00068015, VF1: 96.0858, VLoss: 0.00231365,\n",
      "Train: train_2014 [0034] TF1: 99.5915, Tloss: 0.00046746, VF1: 97.9528, VLoss: 0.00127557,\n",
      "Train: train_2014 [0035] TF1: 99.5422, Tloss: 0.00048263, VF1: 97.3656, VLoss: 0.00146324,\n",
      "Train: train_2014 [0036] TF1: 99.4926, Tloss: 0.00046552, VF1: 97.7380, VLoss: 0.00143757,\n",
      "Train: train_2014 [0037] TF1: 99.6904, Tloss: 0.00030412, VF1: 98.1723, VLoss: 0.00124118,\n",
      "Epoch    37: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2014 [0038] TF1: 99.6535, Tloss: 0.00032786, VF1: 96.4997, VLoss: 0.00270040,\n",
      "Train: train_2014 [0039] TF1: 99.6781, Tloss: 0.00031665, VF1: 97.9484, VLoss: 0.00139879,\n",
      "Train: train_2014 [0040] TF1: 99.8021, Tloss: 0.00024944, VF1: 97.6695, VLoss: 0.00147942,\n",
      "Train: train_2014 [0041] TF1: 99.7897, Tloss: 0.00018092, VF1: 97.7755, VLoss: 0.00138407,\n",
      "Train: train_2014 [0042] TF1: 99.7402, Tloss: 0.00020200, VF1: 97.3376, VLoss: 0.00200688,\n",
      "Train: train_2014 [0043] TF1: 99.7033, Tloss: 0.00030437, VF1: 97.8992, VLoss: 0.00137679,\n",
      "Train: train_2014 [0044] TF1: 99.7155, Tloss: 0.00028611, VF1: 98.2695, VLoss: 0.00120421,\n",
      "Train: train_2014 [0045] TF1: 99.6783, Tloss: 0.00028786, VF1: 97.9420, VLoss: 0.00140607,\n",
      "Train: train_2014 [0046] TF1: 99.6907, Tloss: 0.00031987, VF1: 97.5093, VLoss: 0.00198775,\n",
      "Train: train_2014 [0047] TF1: 99.6288, Tloss: 0.00030520, VF1: 97.9354, VLoss: 0.00151748,\n",
      "Epoch    47: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_2014 [0048] TF1: 99.8392, Tloss: 0.00017692, VF1: 98.3158, VLoss: 0.00148397,\n",
      "Train: train_2014 [0049] TF1: 99.8021, Tloss: 0.00018046, VF1: 97.8814, VLoss: 0.00170980,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_019.pth Fragment=00 score=0.9348238768192363\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_023.pth Fragment=00 score=0.9580628541988665\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_044.pth Fragment=00 score=0.9464005851008255\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_037.pth Fragment=00 score=0.9408810296118029\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_034.pth Fragment=00 score=0.9308589607635207\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_024.pth Fragment=00 score=0.9494155373952622\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_019.pth Fragment=00 score=0.9926846377392525\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_023.pth Fragment=00 score=0.995\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_044.pth Fragment=00 score=0.9949929901862608\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_037.pth Fragment=00 score=0.995100489951005\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_034.pth Fragment=00 score=0.9945935122146576\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2014_024.pth Fragment=00 score=0.9916184394332469\n",
      "Extend: f1score=95.0235, vcoverage=90.7000, vf1score=97.8678, vexentdSize=9070, ecoverage=9.3000, ef1score=74.9580, erestSize=930\n",
      "Extend: f1score=99.6396, vcoverage=98.0400, vf1score=99.8778, vexentdSize=9804, ecoverage=1.9600, ef1score=85.8824, erestSize=196\n",
      "########\n",
      "######## 2015\n",
      "15256\n",
      "0    7833\n",
      "1    7423\n",
      "Name: label, dtype: int64\n",
      "3814\n",
      "0    1954\n",
      "1    1860\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Train: train_2015 [0000] TF1: 96.1102, Tloss: 0.00351723, VF1: 88.1081, VLoss: 0.00439404,\n",
      "Train: train_2015 [0001] TF1: 97.8409, Tloss: 0.00193428, VF1: 95.9210, VLoss: 0.00163108,\n",
      "Train: train_2015 [0002] TF1: 98.0387, Tloss: 0.00175371, VF1: 96.8233, VLoss: 0.00143411,\n",
      "Train: train_2015 [0003] TF1: 98.5274, Tloss: 0.00138879, VF1: 98.3931, VLoss: 0.00087439,\n",
      "Train: train_2015 [0004] TF1: 98.7115, Tloss: 0.00121834, VF1: 98.4170, VLoss: 0.00082409,\n",
      "Train: train_2015 [0005] TF1: 98.4395, Tloss: 0.00137598, VF1: 97.1932, VLoss: 0.00126544,\n",
      "Train: train_2015 [0006] TF1: 98.8851, Tloss: 0.00104481, VF1: 95.0127, VLoss: 0.00229119,\n",
      "Train: train_2015 [0007] TF1: 98.7706, Tloss: 0.00109183, VF1: 95.1891, VLoss: 0.00197351,\n",
      "Train: train_2015 [0008] TF1: 98.9269, Tloss: 0.00103572, VF1: 97.8445, VLoss: 0.00106170,\n",
      "Train: train_2015 [0009] TF1: 99.0082, Tloss: 0.00090693, VF1: 86.4190, VLoss: 0.00481715,\n",
      "Train: train_2015 [0010] TF1: 98.9191, Tloss: 0.00090509, VF1: 88.7029, VLoss: 0.00473096,\n",
      "Train: train_2015 [0011] TF1: 99.0422, Tloss: 0.00090028, VF1: 98.7554, VLoss: 0.00069623,\n",
      "Train: train_2015 [0012] TF1: 99.1023, Tloss: 0.00087234, VF1: 98.0769, VLoss: 0.00083524,\n",
      "Train: train_2015 [0013] TF1: 99.1099, Tloss: 0.00085033, VF1: 98.5227, VLoss: 0.00072166,\n",
      "Train: train_2015 [0014] TF1: 99.2173, Tloss: 0.00076513, VF1: 98.0800, VLoss: 0.00086573,\n",
      "Train: train_2015 [0015] TF1: 99.1903, Tloss: 0.00077404, VF1: 98.5274, VLoss: 0.00075451,\n",
      "Train: train_2015 [0016] TF1: 99.2712, Tloss: 0.00072972, VF1: 98.2857, VLoss: 0.00091809,\n",
      "Train: train_2015 [0017] TF1: 99.2377, Tloss: 0.00071887, VF1: 95.8961, VLoss: 0.00194237,\n",
      "Train: train_2015 [0018] TF1: 99.1967, Tloss: 0.00077858, VF1: 98.7321, VLoss: 0.00076271,\n",
      "Train: train_2015 [0019] TF1: 99.3590, Tloss: 0.00065025, VF1: 96.1560, VLoss: 0.00224312,\n",
      "Train: train_2015 [0020] TF1: 99.1773, Tloss: 0.00072806, VF1: 98.3134, VLoss: 0.00088664,\n",
      "Train: train_2015 [0021] TF1: 99.2781, Tloss: 0.00073081, VF1: 98.8685, VLoss: 0.00068472,\n",
      "Train: train_2015 [0022] TF1: 99.3867, Tloss: 0.00061468, VF1: 97.1805, VLoss: 0.00120535,\n",
      "Train: train_2015 [0023] TF1: 99.3330, Tloss: 0.00064153, VF1: 98.4213, VLoss: 0.00092690,\n",
      "Train: train_2015 [0024] TF1: 99.4068, Tloss: 0.00057168, VF1: 98.2561, VLoss: 0.00098253,\n",
      "Train: train_2015 [0025] TF1: 99.4874, Tloss: 0.00052295, VF1: 98.3422, VLoss: 0.00087703,\n",
      "Train: train_2015 [0026] TF1: 99.4875, Tloss: 0.00056065, VF1: 96.2159, VLoss: 0.00185250,\n",
      "Train: train_2015 [0027] TF1: 99.4670, Tloss: 0.00055195, VF1: 96.5481, VLoss: 0.00141408,\n",
      "Train: train_2015 [0028] TF1: 99.4606, Tloss: 0.00050733, VF1: 98.8976, VLoss: 0.00060433,\n",
      "Train: train_2015 [0029] TF1: 99.4877, Tloss: 0.00046655, VF1: 98.8159, VLoss: 0.00066044,\n",
      "Train: train_2015 [0030] TF1: 99.4068, Tloss: 0.00054817, VF1: 98.2167, VLoss: 0.00087558,\n",
      "Train: train_2015 [0031] TF1: 99.3460, Tloss: 0.00061259, VF1: 98.4665, VLoss: 0.00067943,\n",
      "Train: train_2015 [0032] TF1: 99.5214, Tloss: 0.00038883, VF1: 98.5877, VLoss: 0.00085753,\n",
      "Train: train_2015 [0033] TF1: 99.3731, Tloss: 0.00058299, VF1: 94.4476, VLoss: 0.00253293,\n",
      "Train: train_2015 [0034] TF1: 99.4673, Tloss: 0.00052503, VF1: 98.8344, VLoss: 0.00088782,\n",
      "Train: train_2015 [0035] TF1: 99.5077, Tloss: 0.00051169, VF1: 98.2848, VLoss: 0.00108904,\n",
      "Train: train_2015 [0036] TF1: 99.5281, Tloss: 0.00046594, VF1: 97.3941, VLoss: 0.00113686,\n",
      "Train: train_2015 [0037] TF1: 99.5147, Tloss: 0.00044244, VF1: 67.9700, VLoss: 0.06344221,\n",
      "Train: train_2015 [0038] TF1: 99.4944, Tloss: 0.00051414, VF1: 98.6789, VLoss: 0.00069950,\n",
      "Epoch    38: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2015 [0039] TF1: 99.6496, Tloss: 0.00031116, VF1: 98.7403, VLoss: 0.00075207,\n",
      "Train: train_2015 [0040] TF1: 99.6224, Tloss: 0.00037942, VF1: 94.7126, VLoss: 0.00209998,\n",
      "Train: train_2015 [0041] TF1: 99.6832, Tloss: 0.00031132, VF1: 99.0811, VLoss: 0.00080410,\n",
      "Train: train_2015 [0042] TF1: 99.6292, Tloss: 0.00033045, VF1: 98.5083, VLoss: 0.00090240,\n",
      "Train: train_2015 [0043] TF1: 99.6495, Tloss: 0.00036686, VF1: 99.0323, VLoss: 0.00069170,\n",
      "Train: train_2015 [0044] TF1: 99.6630, Tloss: 0.00035676, VF1: 99.0579, VLoss: 0.00070087,\n",
      "Train: train_2015 [0045] TF1: 99.6630, Tloss: 0.00032031, VF1: 86.1717, VLoss: 0.00746923,\n",
      "Epoch    45: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2015 [0046] TF1: 99.6496, Tloss: 0.00035463, VF1: 98.9790, VLoss: 0.00069221,\n",
      "Train: train_2015 [0047] TF1: 99.7573, Tloss: 0.00023060, VF1: 98.7131, VLoss: 0.00070923,\n",
      "Train: train_2015 [0048] TF1: 99.7641, Tloss: 0.00022760, VF1: 98.8063, VLoss: 0.00103237,\n",
      "Train: train_2015 [0049] TF1: 99.6900, Tloss: 0.00028788, VF1: 98.8102, VLoss: 0.00089286,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_028.pth Fragment=00 score=0.9310344827586207\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_029.pth Fragment=00 score=0.933529659234329\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_031.pth Fragment=00 score=0.9362416107382551\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_021.pth Fragment=00 score=0.9277374566109183\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_043.pth Fragment=00 score=0.940379403794038\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_046.pth Fragment=00 score=0.9321676848535918\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_028.pth Fragment=00 score=0.9819947400364151\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_029.pth Fragment=00 score=0.9840532902704885\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_031.pth Fragment=00 score=0.979739945570003\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_021.pth Fragment=00 score=0.980503081119305\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_043.pth Fragment=00 score=0.9861556982343499\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2015_046.pth Fragment=00 score=0.9848729326341267\n",
      "Extend: f1score=93.7526, vcoverage=91.5200, vf1score=96.6741, vexentdSize=9152, ecoverage=8.4800, ef1score=68.8689, erestSize=848\n",
      "Extend: f1score=98.6911, vcoverage=95.8300, vf1score=99.6492, vexentdSize=9583, ecoverage=4.1700, ef1score=81.5238, erestSize=417\n",
      "########\n",
      "######## 2016\n",
      "22577\n",
      "0    11677\n",
      "1    10900\n",
      "Name: label, dtype: int64\n",
      "5645\n",
      "0    2869\n",
      "1    2776\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2016 [0000] TF1: 96.1058, Tloss: 0.00344792, VF1: 97.8945, VLoss: 0.00137077,\n",
      "Train: train_2016 [0001] TF1: 97.7201, Tloss: 0.00214620, VF1: 73.5154, VLoss: 0.01357853,\n",
      "Train: train_2016 [0002] TF1: 97.8926, Tloss: 0.00186465, VF1: 91.3123, VLoss: 0.00437684,\n",
      "Train: train_2016 [0003] TF1: 98.2197, Tloss: 0.00168025, VF1: 91.2003, VLoss: 0.00382406,\n",
      "Train: train_2016 [0004] TF1: 98.3140, Tloss: 0.00154245, VF1: 94.4882, VLoss: 0.00231056,\n",
      "Train: train_2016 [0005] TF1: 98.4630, Tloss: 0.00139949, VF1: 96.0028, VLoss: 0.00187816,\n",
      "Train: train_2016 [0006] TF1: 98.5735, Tloss: 0.00138000, VF1: 97.4378, VLoss: 0.00125164,\n",
      "Train: train_2016 [0007] TF1: 98.5773, Tloss: 0.00132088, VF1: 98.3397, VLoss: 0.00089890,\n",
      "Train: train_2016 [0008] TF1: 98.6834, Tloss: 0.00122071, VF1: 97.8145, VLoss: 0.00124237,\n",
      "Train: train_2016 [0009] TF1: 98.7075, Tloss: 0.00118822, VF1: 95.0475, VLoss: 0.00257636,\n",
      "Train: train_2016 [0010] TF1: 98.5962, Tloss: 0.00123185, VF1: 97.1175, VLoss: 0.00160856,\n",
      "Train: train_2016 [0011] TF1: 98.7901, Tloss: 0.00104045, VF1: 98.3477, VLoss: 0.00089192,\n",
      "Train: train_2016 [0012] TF1: 99.0030, Tloss: 0.00095874, VF1: 98.1591, VLoss: 0.00088790,\n",
      "Train: train_2016 [0013] TF1: 98.8374, Tloss: 0.00101436, VF1: 97.9391, VLoss: 0.00096007,\n",
      "Train: train_2016 [0014] TF1: 98.9655, Tloss: 0.00100594, VF1: 96.5300, VLoss: 0.00146015,\n",
      "Train: train_2016 [0015] TF1: 98.9517, Tloss: 0.00093475, VF1: 98.4699, VLoss: 0.00083917,\n",
      "Train: train_2016 [0016] TF1: 98.9977, Tloss: 0.00090760, VF1: 97.4858, VLoss: 0.00114801,\n",
      "Train: train_2016 [0017] TF1: 99.0945, Tloss: 0.00085716, VF1: 97.6950, VLoss: 0.00114909,\n",
      "Train: train_2016 [0018] TF1: 99.1264, Tloss: 0.00090194, VF1: 97.7224, VLoss: 0.00102280,\n",
      "Train: train_2016 [0019] TF1: 99.1783, Tloss: 0.00076794, VF1: 98.6584, VLoss: 0.00074346,\n",
      "Train: train_2016 [0020] TF1: 99.0900, Tloss: 0.00085468, VF1: 97.9465, VLoss: 0.00130417,\n",
      "Train: train_2016 [0021] TF1: 99.2697, Tloss: 0.00066844, VF1: 95.7200, VLoss: 0.00197218,\n",
      "Train: train_2016 [0022] TF1: 99.1910, Tloss: 0.00079252, VF1: 98.1766, VLoss: 0.00095274,\n",
      "Train: train_2016 [0023] TF1: 99.2235, Tloss: 0.00075663, VF1: 98.6739, VLoss: 0.00086630,\n",
      "Train: train_2016 [0024] TF1: 99.2509, Tloss: 0.00074576, VF1: 98.7520, VLoss: 0.00101168,\n",
      "Train: train_2016 [0025] TF1: 99.1774, Tloss: 0.00078591, VF1: 98.6440, VLoss: 0.00077742,\n",
      "Train: train_2016 [0026] TF1: 99.2470, Tloss: 0.00068940, VF1: 96.8269, VLoss: 0.00221993,\n",
      "Train: train_2016 [0027] TF1: 99.3482, Tloss: 0.00062361, VF1: 93.1780, VLoss: 0.00305827,\n",
      "Train: train_2016 [0028] TF1: 99.2508, Tloss: 0.00072731, VF1: 98.1586, VLoss: 0.00111914,\n",
      "Train: train_2016 [0029] TF1: 99.3251, Tloss: 0.00064110, VF1: 98.6217, VLoss: 0.00085385,\n",
      "Train: train_2016 [0030] TF1: 99.2880, Tloss: 0.00065190, VF1: 98.1132, VLoss: 0.00095155,\n",
      "Train: train_2016 [0031] TF1: 99.2740, Tloss: 0.00066916, VF1: 98.4324, VLoss: 0.00113962,\n",
      "Train: train_2016 [0032] TF1: 99.3523, Tloss: 0.00064526, VF1: 93.1154, VLoss: 0.00295589,\n",
      "Train: train_2016 [0033] TF1: 99.3292, Tloss: 0.00058242, VF1: 95.6701, VLoss: 0.00251111,\n",
      "Train: train_2016 [0034] TF1: 99.4029, Tloss: 0.00059503, VF1: 98.5188, VLoss: 0.00081493,\n",
      "Train: train_2016 [0035] TF1: 99.4210, Tloss: 0.00061095, VF1: 98.7493, VLoss: 0.00073969,\n",
      "Train: train_2016 [0036] TF1: 99.4394, Tloss: 0.00060015, VF1: 98.7131, VLoss: 0.00076790,\n",
      "Train: train_2016 [0037] TF1: 99.4444, Tloss: 0.00054964, VF1: 97.9505, VLoss: 0.00105936,\n",
      "Train: train_2016 [0038] TF1: 99.3985, Tloss: 0.00057368, VF1: 97.9475, VLoss: 0.00095542,\n",
      "Train: train_2016 [0039] TF1: 99.4123, Tloss: 0.00056344, VF1: 95.3493, VLoss: 0.00302921,\n",
      "Train: train_2016 [0040] TF1: 99.3249, Tloss: 0.00060829, VF1: 98.1911, VLoss: 0.00121210,\n",
      "Train: train_2016 [0041] TF1: 99.4627, Tloss: 0.00054407, VF1: 98.7365, VLoss: 0.00069518,\n",
      "Train: train_2016 [0042] TF1: 99.4304, Tloss: 0.00055554, VF1: 98.1409, VLoss: 0.00106321,\n",
      "Train: train_2016 [0043] TF1: 99.4950, Tloss: 0.00047294, VF1: 98.7355, VLoss: 0.00092076,\n",
      "Train: train_2016 [0044] TF1: 99.4536, Tloss: 0.00051683, VF1: 98.6671, VLoss: 0.00074940,\n",
      "Train: train_2016 [0045] TF1: 99.4902, Tloss: 0.00051805, VF1: 98.4486, VLoss: 0.00127962,\n",
      "Train: train_2016 [0046] TF1: 99.4626, Tloss: 0.00053553, VF1: 98.1387, VLoss: 0.00121230,\n",
      "Train: train_2016 [0047] TF1: 99.5411, Tloss: 0.00046726, VF1: 98.3447, VLoss: 0.00089633,\n",
      "Train: train_2016 [0048] TF1: 99.5454, Tloss: 0.00047482, VF1: 98.7734, VLoss: 0.00078172,\n",
      "Train: train_2016 [0049] TF1: 99.4718, Tloss: 0.00052659, VF1: 98.5905, VLoss: 0.00076770,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_041.pth Fragment=00 score=0.8956759715380405\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_035.pth Fragment=00 score=0.8936357630477869\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_019.pth Fragment=00 score=0.8844399955511068\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_044.pth Fragment=00 score=0.9041603136571553\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_049.pth Fragment=00 score=0.8879594981289896\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_036.pth Fragment=00 score=0.8909492273730686\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_041.pth Fragment=00 score=0.9705733643442209\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_035.pth Fragment=00 score=0.9675192854242793\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_019.pth Fragment=00 score=0.9640302472920499\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_044.pth Fragment=00 score=0.9731253145445395\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_049.pth Fragment=00 score=0.9671246819338423\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2016_036.pth Fragment=00 score=0.9703733766233766\n",
      "Extend: f1score=89.2766, vcoverage=87.6400, vf1score=94.8387, vexentdSize=8764, ecoverage=12.3600, ef1score=63.6533, erestSize=1236\n",
      "Extend: f1score=97.5319, vcoverage=93.0300, vf1score=99.3544, vexentdSize=9303, ecoverage=6.9700, ef1score=79.3792, erestSize=697\n",
      "########\n",
      "######## 2017\n",
      "29588\n",
      "0    15463\n",
      "1    14125\n",
      "Name: label, dtype: int64\n",
      "7398\n",
      "0    3935\n",
      "1    3463\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2017 [0000] TF1: 95.7288, Tloss: 0.00377621, VF1: 94.0579, VLoss: 0.00258026,\n",
      "Train: train_2017 [0001] TF1: 97.0774, Tloss: 0.00267717, VF1: 91.6277, VLoss: 0.00326410,\n",
      "Train: train_2017 [0002] TF1: 97.5134, Tloss: 0.00229932, VF1: 97.7172, VLoss: 0.00109587,\n",
      "Train: train_2017 [0003] TF1: 97.7189, Tloss: 0.00211186, VF1: 98.0761, VLoss: 0.00097673,\n",
      "Train: train_2017 [0004] TF1: 97.6515, Tloss: 0.00214519, VF1: 97.7869, VLoss: 0.00101238,\n",
      "Train: train_2017 [0005] TF1: 98.0194, Tloss: 0.00188505, VF1: 97.4711, VLoss: 0.00119125,\n",
      "Train: train_2017 [0006] TF1: 98.0391, Tloss: 0.00184528, VF1: 97.6811, VLoss: 0.00107491,\n",
      "Train: train_2017 [0007] TF1: 98.1934, Tloss: 0.00172873, VF1: 97.7263, VLoss: 0.00103715,\n",
      "Train: train_2017 [0008] TF1: 98.1058, Tloss: 0.00168446, VF1: 75.8235, VLoss: 0.01314123,\n",
      "Train: train_2017 [0009] TF1: 98.2147, Tloss: 0.00165997, VF1: 95.9899, VLoss: 0.00166055,\n",
      "Train: train_2017 [0010] TF1: 98.3479, Tloss: 0.00157660, VF1: 97.7362, VLoss: 0.00150728,\n",
      "Train: train_2017 [0011] TF1: 98.2910, Tloss: 0.00158445, VF1: 97.6924, VLoss: 0.00113124,\n",
      "Train: train_2017 [0012] TF1: 98.4059, Tloss: 0.00151493, VF1: 95.6144, VLoss: 0.00171291,\n",
      "Train: train_2017 [0013] TF1: 98.3796, Tloss: 0.00150333, VF1: 97.9322, VLoss: 0.00112277,\n",
      "Train: train_2017 [0014] TF1: 98.5661, Tloss: 0.00135915, VF1: 98.3692, VLoss: 0.00087323,\n",
      "Train: train_2017 [0015] TF1: 98.4983, Tloss: 0.00139386, VF1: 97.8521, VLoss: 0.00108966,\n",
      "Train: train_2017 [0016] TF1: 98.5271, Tloss: 0.00137308, VF1: 95.5866, VLoss: 0.00187734,\n",
      "Train: train_2017 [0017] TF1: 98.5666, Tloss: 0.00132530, VF1: 97.8766, VLoss: 0.00107393,\n",
      "Train: train_2017 [0018] TF1: 98.6411, Tloss: 0.00126193, VF1: 97.9622, VLoss: 0.00098794,\n",
      "Train: train_2017 [0019] TF1: 98.7240, Tloss: 0.00118526, VF1: 96.6403, VLoss: 0.00144899,\n",
      "Train: train_2017 [0020] TF1: 98.8340, Tloss: 0.00112949, VF1: 98.0472, VLoss: 0.00096051,\n",
      "Train: train_2017 [0021] TF1: 98.7194, Tloss: 0.00120775, VF1: 97.7353, VLoss: 0.00119359,\n",
      "Train: train_2017 [0022] TF1: 98.7654, Tloss: 0.00120094, VF1: 97.7848, VLoss: 0.00130804,\n",
      "Train: train_2017 [0023] TF1: 98.8306, Tloss: 0.00110167, VF1: 97.8723, VLoss: 0.00117793,\n",
      "Train: train_2017 [0024] TF1: 98.8516, Tloss: 0.00113417, VF1: 93.9245, VLoss: 0.00313153,\n",
      "Train: train_2017 [0025] TF1: 98.8121, Tloss: 0.00111252, VF1: 98.3411, VLoss: 0.00095694,\n",
      "Train: train_2017 [0026] TF1: 98.8771, Tloss: 0.00106929, VF1: 96.7004, VLoss: 0.00147919,\n",
      "Train: train_2017 [0027] TF1: 98.8667, Tloss: 0.00108425, VF1: 98.1633, VLoss: 0.00091144,\n",
      "Train: train_2017 [0028] TF1: 98.9691, Tloss: 0.00099094, VF1: 98.4279, VLoss: 0.00094439,\n",
      "Train: train_2017 [0029] TF1: 98.8911, Tloss: 0.00103900, VF1: 98.0930, VLoss: 0.00093492,\n",
      "Train: train_2017 [0030] TF1: 98.8810, Tloss: 0.00107111, VF1: 98.2153, VLoss: 0.00104375,\n",
      "Train: train_2017 [0031] TF1: 98.9808, Tloss: 0.00095692, VF1: 98.4741, VLoss: 0.00104235,\n",
      "Train: train_2017 [0032] TF1: 98.9587, Tloss: 0.00098446, VF1: 98.5903, VLoss: 0.00091471,\n",
      "Train: train_2017 [0033] TF1: 98.9555, Tloss: 0.00100003, VF1: 98.4687, VLoss: 0.00094681,\n",
      "Train: train_2017 [0034] TF1: 98.9513, Tloss: 0.00098804, VF1: 98.4414, VLoss: 0.00087253,\n",
      "Train: train_2017 [0035] TF1: 99.0063, Tloss: 0.00091519, VF1: 97.8567, VLoss: 0.00126589,\n",
      "Train: train_2017 [0036] TF1: 99.0017, Tloss: 0.00095656, VF1: 98.5096, VLoss: 0.00082684,\n",
      "Train: train_2017 [0037] TF1: 99.0306, Tloss: 0.00094037, VF1: 96.8699, VLoss: 0.00180613,\n",
      "Train: train_2017 [0038] TF1: 99.0441, Tloss: 0.00094818, VF1: 98.3435, VLoss: 0.00081717,\n",
      "Train: train_2017 [0039] TF1: 99.0013, Tloss: 0.00094200, VF1: 98.0077, VLoss: 0.00135268,\n",
      "Train: train_2017 [0040] TF1: 99.1016, Tloss: 0.00085492, VF1: 98.3971, VLoss: 0.00099123,\n",
      "Train: train_2017 [0041] TF1: 99.0730, Tloss: 0.00086783, VF1: 96.0455, VLoss: 0.00253361,\n",
      "Train: train_2017 [0042] TF1: 99.1086, Tloss: 0.00087436, VF1: 98.4122, VLoss: 0.00091410,\n",
      "Train: train_2017 [0043] TF1: 99.0766, Tloss: 0.00088270, VF1: 98.1265, VLoss: 0.00105634,\n",
      "Train: train_2017 [0044] TF1: 99.0765, Tloss: 0.00088156, VF1: 96.7435, VLoss: 0.00165494,\n",
      "Train: train_2017 [0045] TF1: 99.0092, Tloss: 0.00090552, VF1: 98.3773, VLoss: 0.00091494,\n",
      "Train: train_2017 [0046] TF1: 99.1051, Tloss: 0.00083223, VF1: 98.3948, VLoss: 0.00088725,\n",
      "Train: train_2017 [0047] TF1: 99.0586, Tloss: 0.00089600, VF1: 98.4118, VLoss: 0.00087435,\n",
      "Train: train_2017 [0048] TF1: 99.1120, Tloss: 0.00078392, VF1: 98.5009, VLoss: 0.00098142,\n",
      "Train: train_2017 [0049] TF1: 99.1122, Tloss: 0.00081980, VF1: 98.4557, VLoss: 0.00099013,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_038.pth Fragment=00 score=0.801292020576624\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_036.pth Fragment=00 score=0.7964431627012738\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_034.pth Fragment=00 score=0.7872985092716034\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_014.pth Fragment=00 score=0.817859673990078\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_047.pth Fragment=00 score=0.825199436884092\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_046.pth Fragment=00 score=0.8014811275680842\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_038.pth Fragment=00 score=0.9571532316630356\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_036.pth Fragment=00 score=0.9590739975196362\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_034.pth Fragment=00 score=0.9512780386019822\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_014.pth Fragment=00 score=0.9383525430897748\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_047.pth Fragment=00 score=0.9630544406709889\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2017_046.pth Fragment=00 score=0.9636774061153294\n",
      "Extend: f1score=80.8622, vcoverage=83.6400, vf1score=84.1643, vexentdSize=8364, ecoverage=16.3600, ef1score=72.9839, erestSize=1636\n",
      "Extend: f1score=96.3651, vcoverage=89.1400, vf1score=99.3850, vexentdSize=8914, ecoverage=10.8600, ef1score=80.5663, erestSize=1086\n",
      "########\n",
      "######## 2018\n",
      "36280\n",
      "0    19446\n",
      "1    16834\n",
      "Name: label, dtype: int64\n",
      "9070\n",
      "0    4889\n",
      "1    4181\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2018 [0000] TF1: 95.3188, Tloss: 0.00398589, VF1: 96.8754, VLoss: 0.00139238,\n",
      "Train: train_2018 [0001] TF1: 96.7789, Tloss: 0.00281835, VF1: 96.9897, VLoss: 0.00133436,\n",
      "Train: train_2018 [0002] TF1: 97.2918, Tloss: 0.00248648, VF1: 95.3968, VLoss: 0.00196472,\n",
      "Train: train_2018 [0003] TF1: 97.3745, Tloss: 0.00235627, VF1: 96.3901, VLoss: 0.00169874,\n",
      "Train: train_2018 [0004] TF1: 97.7104, Tloss: 0.00216185, VF1: 97.1561, VLoss: 0.00123495,\n",
      "Train: train_2018 [0005] TF1: 97.6757, Tloss: 0.00213117, VF1: 96.5331, VLoss: 0.00153073,\n",
      "Train: train_2018 [0006] TF1: 97.7060, Tloss: 0.00200023, VF1: 95.6144, VLoss: 0.00168799,\n",
      "Train: train_2018 [0007] TF1: 97.8036, Tloss: 0.00195298, VF1: 94.6973, VLoss: 0.00232399,\n",
      "Train: train_2018 [0008] TF1: 98.0491, Tloss: 0.00179048, VF1: 96.8804, VLoss: 0.00134297,\n",
      "Train: train_2018 [0009] TF1: 98.0431, Tloss: 0.00181801, VF1: 96.5021, VLoss: 0.00150104,\n",
      "Train: train_2018 [0010] TF1: 98.1236, Tloss: 0.00173439, VF1: 97.3753, VLoss: 0.00131716,\n",
      "Train: train_2018 [0011] TF1: 98.2188, Tloss: 0.00168468, VF1: 97.3362, VLoss: 0.00128649,\n",
      "Train: train_2018 [0012] TF1: 98.1223, Tloss: 0.00168640, VF1: 93.5363, VLoss: 0.00294320,\n",
      "Train: train_2018 [0013] TF1: 98.3254, Tloss: 0.00156563, VF1: 94.1659, VLoss: 0.00260389,\n",
      "Train: train_2018 [0014] TF1: 98.2538, Tloss: 0.00157642, VF1: 97.1415, VLoss: 0.00137709,\n",
      "Train: train_2018 [0015] TF1: 98.2791, Tloss: 0.00158699, VF1: 97.1305, VLoss: 0.00130701,\n",
      "Train: train_2018 [0016] TF1: 98.3142, Tloss: 0.00155612, VF1: 95.5279, VLoss: 0.00193193,\n",
      "Train: train_2018 [0017] TF1: 98.4282, Tloss: 0.00144970, VF1: 97.5446, VLoss: 0.00121811,\n",
      "Train: train_2018 [0018] TF1: 98.4630, Tloss: 0.00142249, VF1: 97.6739, VLoss: 0.00124061,\n",
      "Train: train_2018 [0019] TF1: 98.4607, Tloss: 0.00144068, VF1: 97.7788, VLoss: 0.00114071,\n",
      "Train: train_2018 [0020] TF1: 98.5236, Tloss: 0.00135532, VF1: 97.2900, VLoss: 0.00139798,\n",
      "Train: train_2018 [0021] TF1: 98.5324, Tloss: 0.00132452, VF1: 97.8348, VLoss: 0.00113208,\n",
      "Train: train_2018 [0022] TF1: 98.5306, Tloss: 0.00139218, VF1: 97.0215, VLoss: 0.00136113,\n",
      "Train: train_2018 [0023] TF1: 98.5313, Tloss: 0.00136143, VF1: 96.4947, VLoss: 0.00183550,\n",
      "Train: train_2018 [0024] TF1: 98.6480, Tloss: 0.00126237, VF1: 97.1359, VLoss: 0.00160687,\n",
      "Train: train_2018 [0025] TF1: 98.5637, Tloss: 0.00127943, VF1: 97.7314, VLoss: 0.00113772,\n",
      "Train: train_2018 [0026] TF1: 98.5786, Tloss: 0.00127073, VF1: 97.0603, VLoss: 0.00251977,\n",
      "Train: train_2018 [0027] TF1: 98.6861, Tloss: 0.00124897, VF1: 97.5773, VLoss: 0.00113469,\n",
      "Train: train_2018 [0028] TF1: 98.5968, Tloss: 0.00121894, VF1: 96.2867, VLoss: 0.00175023,\n",
      "Train: train_2018 [0029] TF1: 98.7018, Tloss: 0.00118279, VF1: 97.8482, VLoss: 0.00128020,\n",
      "Train: train_2018 [0030] TF1: 98.6179, Tloss: 0.00119985, VF1: 97.7643, VLoss: 0.00114508,\n",
      "Train: train_2018 [0031] TF1: 98.8174, Tloss: 0.00113064, VF1: 97.7735, VLoss: 0.00115536,\n",
      "Train: train_2018 [0032] TF1: 98.7725, Tloss: 0.00113461, VF1: 97.3762, VLoss: 0.00136510,\n",
      "Train: train_2018 [0033] TF1: 98.6476, Tloss: 0.00119399, VF1: 97.7429, VLoss: 0.00112303,\n",
      "Train: train_2018 [0034] TF1: 98.6204, Tloss: 0.00120304, VF1: 97.6234, VLoss: 0.00111567,\n",
      "Train: train_2018 [0035] TF1: 98.7999, Tloss: 0.00113388, VF1: 94.5983, VLoss: 0.00254473,\n",
      "Train: train_2018 [0036] TF1: 98.7349, Tloss: 0.00113129, VF1: 97.3659, VLoss: 0.00140060,\n",
      "Train: train_2018 [0037] TF1: 98.7914, Tloss: 0.00107762, VF1: 97.8269, VLoss: 0.00118680,\n",
      "Train: train_2018 [0038] TF1: 98.7321, Tloss: 0.00112318, VF1: 97.3866, VLoss: 0.00132565,\n",
      "Train: train_2018 [0039] TF1: 98.7261, Tloss: 0.00110441, VF1: 97.2235, VLoss: 0.00138611,\n",
      "Train: train_2018 [0040] TF1: 98.8574, Tloss: 0.00109208, VF1: 93.6976, VLoss: 0.00391328,\n",
      "Train: train_2018 [0041] TF1: 98.8449, Tloss: 0.00108605, VF1: 97.8677, VLoss: 0.00111994,\n",
      "Train: train_2018 [0042] TF1: 98.8726, Tloss: 0.00102726, VF1: 97.8615, VLoss: 0.00131516,\n",
      "Train: train_2018 [0043] TF1: 98.8370, Tloss: 0.00104582, VF1: 97.6975, VLoss: 0.00133223,\n",
      "Train: train_2018 [0044] TF1: 98.8960, Tloss: 0.00100126, VF1: 98.0850, VLoss: 0.00110284,\n",
      "Train: train_2018 [0045] TF1: 98.7585, Tloss: 0.00109644, VF1: 97.9307, VLoss: 0.00116855,\n",
      "Train: train_2018 [0046] TF1: 98.8601, Tloss: 0.00101506, VF1: 97.3737, VLoss: 0.00125946,\n",
      "Train: train_2018 [0047] TF1: 98.8572, Tloss: 0.00101343, VF1: 97.7709, VLoss: 0.00119682,\n",
      "Train: train_2018 [0048] TF1: 98.8785, Tloss: 0.00098898, VF1: 97.6850, VLoss: 0.00116555,\n",
      "Train: train_2018 [0049] TF1: 98.9021, Tloss: 0.00102494, VF1: 97.6998, VLoss: 0.00119611,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_044.pth Fragment=00 score=0.8462156885264729\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_034.pth Fragment=00 score=0.8178605089538171\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_041.pth Fragment=00 score=0.7974562035037197\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_033.pth Fragment=00 score=0.8213320803477036\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_021.pth Fragment=00 score=0.8423236514522823\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_027.pth Fragment=00 score=0.8527676950998186\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_044.pth Fragment=00 score=0.9535539087777893\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_034.pth Fragment=00 score=0.9451910961780764\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_041.pth Fragment=00 score=0.9322088181915235\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_033.pth Fragment=00 score=0.9429745968166965\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_021.pth Fragment=00 score=0.9443507588532883\n",
      "Evaluate: ModelPath=./traces/studyWS05/model_train_2018_027.pth Fragment=00 score=0.959520859149112\n",
      "Extend: f1score=83.5869, vcoverage=78.4700, vf1score=91.6429, vexentdSize=7847, ecoverage=21.5300, ef1score=68.5942, erestSize=2153\n",
      "Extend: f1score=95.3048, vcoverage=88.5200, vf1score=99.2252, vexentdSize=8852, ecoverage=11.4800, ef1score=77.0414, erestSize=1148\n",
      "########\n",
      "######## 2019\n",
      "42557\n",
      "0    23290\n",
      "1    19267\n",
      "Name: label, dtype: int64\n",
      "10640\n",
      "0    5860\n",
      "1    4780\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "10000\n",
      "1    5000\n",
      "0    5000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2019 [0000] TF1: 94.7168, Tloss: 0.00426340, VF1: 92.1665, VLoss: 0.00322432,\n",
      "Train: train_2019 [0001] TF1: 96.2565, Tloss: 0.00320380, VF1: 95.5142, VLoss: 0.00199127,\n",
      "Train: train_2019 [0002] TF1: 96.6574, Tloss: 0.00293152, VF1: 96.5657, VLoss: 0.00154173,\n",
      "Train: train_2019 [0003] TF1: 96.7861, Tloss: 0.00279551, VF1: 94.5411, VLoss: 0.00212885,\n",
      "Train: train_2019 [0004] TF1: 97.0638, Tloss: 0.00266429, VF1: 96.6420, VLoss: 0.00149389,\n",
      "Train: train_2019 [0005] TF1: 97.0792, Tloss: 0.00255762, VF1: 96.8019, VLoss: 0.00154508,\n",
      "Train: train_2019 [0006] TF1: 97.2017, Tloss: 0.00250247, VF1: 97.0800, VLoss: 0.00137620,\n",
      "Train: train_2019 [0007] TF1: 97.3554, Tloss: 0.00237536, VF1: 96.6835, VLoss: 0.00149194,\n",
      "Train: train_2019 [0008] TF1: 97.4103, Tloss: 0.00231167, VF1: 97.1138, VLoss: 0.00132766,\n",
      "Train: train_2019 [0009] TF1: 97.5281, Tloss: 0.00219959, VF1: 97.0678, VLoss: 0.00138919,\n",
      "Train: train_2019 [0010] TF1: 97.4959, Tloss: 0.00216682, VF1: 96.7297, VLoss: 0.00149547,\n",
      "Train: train_2019 [0011] TF1: 97.5323, Tloss: 0.00216614, VF1: 97.0236, VLoss: 0.00142843,\n",
      "Train: train_2019 [0012] TF1: 97.6934, Tloss: 0.00203056, VF1: 95.5474, VLoss: 0.00194588,\n",
      "Train: train_2019 [0013] TF1: 97.6970, Tloss: 0.00200672, VF1: 96.4589, VLoss: 0.00168438,\n",
      "Train: train_2019 [0014] TF1: 97.7916, Tloss: 0.00187864, VF1: 96.9950, VLoss: 0.00139032,\n",
      "Train: train_2019 [0015] TF1: 97.8911, Tloss: 0.00196027, VF1: 96.4586, VLoss: 0.00159562,\n",
      "Train: train_2019 [0016] TF1: 97.9381, Tloss: 0.00187024, VF1: 97.2818, VLoss: 0.00130641,\n",
      "Train: train_2019 [0017] TF1: 97.9172, Tloss: 0.00186018, VF1: 97.0469, VLoss: 0.00131953,\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(years)):\n",
    "    currentTag = str(years[idx])\n",
    "    \n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #\n",
    "    otest_df = timedf.loc[timedf.year == years[idx-1]]\n",
    "    ntest_df = timedf.loc[timedf.year == years[idx]]\n",
    "    \n",
    "    dataset_df = pd.concat(dataset_lst)\n",
    "    trainLoader, validLoader, otestLoader, ntestLoader = getDataloaders(dataset_df, otest_df, ntest_df, trainPercentage=0.8)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "    \n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, ntestLoader, device)\n",
    "    exresult_df   = evaluate(ws, selectedModelPaths, otestLoader, device)\n",
    "    \n",
    "    extend_df, _  = extendDataset(ws, evalresult_df, probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "    _, rest_df    = extendDataset(ws, exresult_df,   probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "\n",
    "    #\n",
    "    dataset_lst.append(extend_df)\n",
    "\n",
    "    #\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df, exresult_df, dataset_lst, rest_df)], \n",
    "                                     columns=['TimeTag', 'models', 'evalResuls', \n",
    "                                              'extendResults', 'datasetList', \n",
    "                                              'restDataset'])\n",
    "    \n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
