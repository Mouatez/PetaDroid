{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):int(validPercentage * len(dataset_df))]]\n",
    "    test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'comparativeWS02'\n",
    "epochNum         = 50\n",
    "device           = torch.device('cuda:1')\n",
    "ensembleSize     = 10\n",
    "\n",
    "trainPercentageParam = 0.7\n",
    "validPercentageParam = 0.8\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41712\n"
     ]
    }
   ],
   "source": [
    "mamadroid_meta_df = pd.read_parquet('dataset/mamadroid_meta.parquet')\n",
    "mamadroid_meta_df = mamadroid_meta_df[['sha256', 'year', 'tag']]\n",
    "\n",
    "drebin_df = pd.read_msgpack('/ws/mnt/habouch/datasets/android_dataset/meta/drebin_meta.msg')\n",
    "drebin_df = drebin_df[['sha256']]\n",
    "drebin_df['tag'] = 'malware'\n",
    "drebin_df['year'] = 'drebin'\n",
    "drebin_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mamadroid_df = pd.concat([mamadroid_meta_df, drebin_df], sort=False)\n",
    "mamadroid_df.drop_duplicates(subset='sha256', inplace=True)\n",
    "\n",
    "doneList = [item.split('/')[-1] for item in glob.glob('/ws/mnt/local/data/output/mamadroid/*')]\n",
    "mamadroid_df = mamadroid_df.loc[mamadroid_df.sha256.isin(doneList)]\n",
    "\n",
    "mamadroid_df['label'] = (mamadroid_df.tag == 'malware').apply(int)\n",
    "mamadroid_df['filePath'] = '/ws/mnt/local/data/output/mamadroid/' + mamadroid_df.sha256\n",
    "print(len(mamadroid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#androzoo_df = pd.read_parquet('dataset/androzoo_meta.parquet')\n",
    "df = androzoo_df.loc[androzoo_df.sha256.isin(mamadroid_df.loc[mamadroid_df.label == 0, 'sha256'])]\n",
    "df.dropna(subset=['vt_detection'], inplace=True)\n",
    "df = df.loc[df.vt_detection > 0]\n",
    "mamadroid_df = mamadroid_df.loc[~mamadroid_df.sha256.isin(df.sha256)]\n",
    "print(len(mamadroid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df = androzoo_df.loc[androzoo_df.sha256.isin(mamadroid_df.loc[mamadroid_df.label == 1, 'sha256'])]\n",
    "df.dropna(subset=['vt_detection'], inplace=True)\n",
    "df = df.loc[df.vt_detection < 4]\n",
    "mamadroid_df = mamadroid_df.loc[~mamadroid_df.sha256.isin(df.sha256)]\n",
    "print(len(mamadroid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>tag</th>\n",
       "      <th>filePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <th>1</th>\n",
       "      <td>6928</td>\n",
       "      <td>6928</td>\n",
       "      <td>6928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <th>1</th>\n",
       "      <td>13654</td>\n",
       "      <td>13654</td>\n",
       "      <td>13654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <th>1</th>\n",
       "      <td>3732</td>\n",
       "      <td>3732</td>\n",
       "      <td>3732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <th>1</th>\n",
       "      <td>2184</td>\n",
       "      <td>2184</td>\n",
       "      <td>2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drebin</th>\n",
       "      <th>1</th>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <th>0</th>\n",
       "      <td>1638</td>\n",
       "      <td>1638</td>\n",
       "      <td>1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <th>0</th>\n",
       "      <td>4986</td>\n",
       "      <td>4986</td>\n",
       "      <td>4986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sha256    tag  filePath\n",
       "year   label                         \n",
       "2013   1        6928   6928      6928\n",
       "2014   1       13654  13654     13654\n",
       "2015   1        3732   3732      3732\n",
       "2016   1        2184   2184      2184\n",
       "drebin 1        4636   4636      4636\n",
       "new    0        1638   1638      1638\n",
       "old    0        4986   4986      4986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamadroid_df.groupby(['year', 'label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "        ['drebin', 'old'],\n",
    "        ['2013',   'old'],\n",
    "        ['2014',   'old'],\n",
    "        ['2014',   'new'],\n",
    "        ['2015',   'new'],\n",
    "        ['2016',   'new'],\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## drebin_old\n",
      "6735\n",
      "0    3503\n",
      "1    3232\n",
      "Name: label, dtype: int64\n",
      "962\n",
      "0    513\n",
      "1    449\n",
      "Name: label, dtype: int64\n",
      "1925\n",
      "0    970\n",
      "1    955\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_drebin_old [0000] TF1: 95.3392, Tloss: 0.00402343, VF1: 93.7698, VLoss: 0.00329025,\n",
      "Train: train_drebin_old [0001] TF1: 98.0034, Tloss: 0.00179486, VF1: 95.8525, VLoss: 0.00190081,\n",
      "Train: train_drebin_old [0002] TF1: 98.6051, Tloss: 0.00134162, VF1: 98.6577, VLoss: 0.00119896,\n",
      "Train: train_drebin_old [0003] TF1: 98.7937, Tloss: 0.00122235, VF1: 97.2665, VLoss: 0.00219971,\n",
      "Train: train_drebin_old [0004] TF1: 99.0096, Tloss: 0.00086410, VF1: 91.9255, VLoss: 0.00349096,\n",
      "Train: train_drebin_old [0005] TF1: 99.0565, Tloss: 0.00087516, VF1: 94.8827, VLoss: 0.00245284,\n",
      "Train: train_drebin_old [0006] TF1: 99.0877, Tloss: 0.00083242, VF1: 96.8037, VLoss: 0.00164839,\n",
      "Train: train_drebin_old [0007] TF1: 99.4118, Tloss: 0.00059410, VF1: 98.2103, VLoss: 0.00119768,\n",
      "Train: train_drebin_old [0008] TF1: 99.1649, Tloss: 0.00077578, VF1: 98.5507, VLoss: 0.00181667,\n",
      "Train: train_drebin_old [0009] TF1: 99.3818, Tloss: 0.00058676, VF1: 90.2913, VLoss: 0.00504475,\n",
      "Train: train_drebin_old [0010] TF1: 99.4127, Tloss: 0.00057457, VF1: 83.0091, VLoss: 0.00643844,\n",
      "Train: train_drebin_old [0011] TF1: 99.2730, Tloss: 0.00071239, VF1: 97.1687, VLoss: 0.00138217,\n",
      "Train: train_drebin_old [0012] TF1: 99.3037, Tloss: 0.00070973, VF1: 97.6163, VLoss: 0.00195455,\n",
      "Train: train_drebin_old [0013] TF1: 99.2877, Tloss: 0.00059145, VF1: 98.4513, VLoss: 0.00097158,\n",
      "Train: train_drebin_old [0014] TF1: 99.3189, Tloss: 0.00053358, VF1: 98.4234, VLoss: 0.00111139,\n",
      "Train: train_drebin_old [0015] TF1: 99.5513, Tloss: 0.00037089, VF1: 98.2143, VLoss: 0.00167208,\n",
      "Train: train_drebin_old [0016] TF1: 99.2114, Tloss: 0.00072057, VF1: 98.5442, VLoss: 0.00213723,\n",
      "Train: train_drebin_old [0017] TF1: 99.3500, Tloss: 0.00058666, VF1: 98.4199, VLoss: 0.00163856,\n",
      "Train: train_drebin_old [0018] TF1: 99.4279, Tloss: 0.00053521, VF1: 97.6589, VLoss: 0.00126385,\n",
      "Train: train_drebin_old [0019] TF1: 99.5825, Tloss: 0.00038285, VF1: 98.1257, VLoss: 0.00153881,\n",
      "Train: train_drebin_old [0020] TF1: 99.5206, Tloss: 0.00044242, VF1: 94.3662, VLoss: 0.00338144,\n",
      "Train: train_drebin_old [0021] TF1: 99.5205, Tloss: 0.00050259, VF1: 98.4444, VLoss: 0.00105929,\n",
      "Epoch    21: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_drebin_old [0022] TF1: 99.6905, Tloss: 0.00032406, VF1: 98.8864, VLoss: 0.00076780,\n",
      "Train: train_drebin_old [0023] TF1: 99.6906, Tloss: 0.00031701, VF1: 98.2143, VLoss: 0.00092476,\n",
      "Train: train_drebin_old [0024] TF1: 99.7061, Tloss: 0.00027317, VF1: 97.5664, VLoss: 0.00180508,\n",
      "Train: train_drebin_old [0025] TF1: 99.8144, Tloss: 0.00017831, VF1: 98.3278, VLoss: 0.00134599,\n",
      "Train: train_drebin_old [0026] TF1: 99.5974, Tloss: 0.00037393, VF1: 96.7391, VLoss: 0.00196374,\n",
      "Train: train_drebin_old [0027] TF1: 99.4588, Tloss: 0.00058322, VF1: 92.0372, VLoss: 0.00330375,\n",
      "Train: train_drebin_old [0028] TF1: 99.5825, Tloss: 0.00037873, VF1: 98.9944, VLoss: 0.00102147,\n",
      "Train: train_drebin_old [0029] TF1: 99.6752, Tloss: 0.00030471, VF1: 97.9098, VLoss: 0.00131285,\n",
      "Train: train_drebin_old [0030] TF1: 99.5978, Tloss: 0.00040079, VF1: 98.3240, VLoss: 0.00144668,\n",
      "Train: train_drebin_old [0031] TF1: 99.3808, Tloss: 0.00053789, VF1: 98.2183, VLoss: 0.00125023,\n",
      "Epoch    31: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_drebin_old [0032] TF1: 99.7218, Tloss: 0.00025593, VF1: 98.6667, VLoss: 0.00129710,\n",
      "Train: train_drebin_old [0033] TF1: 99.8144, Tloss: 0.00018451, VF1: 98.4513, VLoss: 0.00147037,\n",
      "Train: train_drebin_old [0034] TF1: 99.7990, Tloss: 0.00015359, VF1: 97.2789, VLoss: 0.00194296,\n",
      "Train: train_drebin_old [0035] TF1: 99.7990, Tloss: 0.00018488, VF1: 98.5539, VLoss: 0.00189579,\n",
      "Train: train_drebin_old [0036] TF1: 99.7835, Tloss: 0.00019252, VF1: 98.1982, VLoss: 0.00108787,\n",
      "Train: train_drebin_old [0037] TF1: 99.7682, Tloss: 0.00026241, VF1: 98.8889, VLoss: 0.00131687,\n",
      "Train: train_drebin_old [0038] TF1: 99.7680, Tloss: 0.00021491, VF1: 98.2301, VLoss: 0.00156431,\n",
      "Train: train_drebin_old [0039] TF1: 99.7214, Tloss: 0.00019624, VF1: 98.6637, VLoss: 0.00165121,\n",
      "Train: train_drebin_old [0040] TF1: 99.6133, Tloss: 0.00035519, VF1: 95.7225, VLoss: 0.00518589,\n",
      "Epoch    40: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_drebin_old [0041] TF1: 99.7681, Tloss: 0.00019097, VF1: 97.5717, VLoss: 0.00157643,\n",
      "Train: train_drebin_old [0042] TF1: 99.8145, Tloss: 0.00018417, VF1: 98.6637, VLoss: 0.00140055,\n",
      "Train: train_drebin_old [0043] TF1: 99.8763, Tloss: 0.00013874, VF1: 98.0176, VLoss: 0.00147766,\n",
      "Train: train_drebin_old [0044] TF1: 99.7835, Tloss: 0.00019684, VF1: 98.5442, VLoss: 0.00118459,\n",
      "Train: train_drebin_old [0045] TF1: 99.7834, Tloss: 0.00020166, VF1: 98.6577, VLoss: 0.00136093,\n",
      "Train: train_drebin_old [0046] TF1: 99.7680, Tloss: 0.00026888, VF1: 98.5604, VLoss: 0.00125527,\n",
      "Train: train_drebin_old [0047] TF1: 99.7989, Tloss: 0.00016033, VF1: 98.4513, VLoss: 0.00135566,\n",
      "Train: train_drebin_old [0048] TF1: 99.8144, Tloss: 0.00014641, VF1: 98.4410, VLoss: 0.00154028,\n",
      "Train: train_drebin_old [0049] TF1: 99.9072, Tloss: 0.00010148, VF1: 98.4479, VLoss: 0.00142740,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_022.pth Fragment=00 score=0.9842436974789917\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_023.pth Fragment=00 score=0.9800420168067226\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_013.pth Fragment=00 score=0.9854166666666666\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_028.pth Fragment=00 score=0.9889763779527558\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_021.pth Fragment=00 score=0.9847609038360484\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_036.pth Fragment=00 score=0.9825489159175039\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_014.pth Fragment=00 score=0.9809119830328739\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_044.pth Fragment=00 score=0.9873417721518988\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_007.pth Fragment=00 score=0.9857969489742241\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_drebin_old_002.pth Fragment=00 score=0.9832460732984293\n",
      "Extend: f1score=98.9496, vcoverage=97.8701, vf1score=99.4064, vexentdSize=1884, ecoverage=2.1299, ef1score=82.3529, erestSize=41\n",
      "########\n",
      "######## 2013_old\n",
      "8339\n",
      "1    4822\n",
      "0    3517\n",
      "Name: label, dtype: int64\n",
      "1192\n",
      "1    711\n",
      "0    481\n",
      "Name: label, dtype: int64\n",
      "2383\n",
      "1    1395\n",
      "0     988\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2013_old [0000] TF1: 96.9363, Tloss: 0.00311523, VF1: 92.0261, VLoss: 0.00416387,\n",
      "Train: train_2013_old [0001] TF1: 98.3617, Tloss: 0.00163453, VF1: 76.4105, VLoss: 0.02388030,\n",
      "Train: train_2013_old [0002] TF1: 98.8678, Tloss: 0.00112766, VF1: 98.8717, VLoss: 0.00073775,\n",
      "Train: train_2013_old [0003] TF1: 99.1077, Tloss: 0.00106267, VF1: 96.6618, VLoss: 0.00193208,\n",
      "Train: train_2013_old [0004] TF1: 99.2221, Tloss: 0.00077548, VF1: 98.0583, VLoss: 0.00102235,\n",
      "Train: train_2013_old [0005] TF1: 99.1708, Tloss: 0.00084057, VF1: 96.0651, VLoss: 0.00206324,\n",
      "Train: train_2013_old [0006] TF1: 99.4402, Tloss: 0.00065382, VF1: 96.1408, VLoss: 0.00224504,\n",
      "Train: train_2013_old [0007] TF1: 99.4921, Tloss: 0.00054611, VF1: 98.4308, VLoss: 0.00095528,\n",
      "Train: train_2013_old [0008] TF1: 99.4192, Tloss: 0.00058015, VF1: 98.8685, VLoss: 0.00064873,\n",
      "Train: train_2013_old [0009] TF1: 99.4916, Tloss: 0.00052443, VF1: 98.9459, VLoss: 0.00072914,\n",
      "Train: train_2013_old [0010] TF1: 99.6577, Tloss: 0.00038582, VF1: 98.3746, VLoss: 0.00098104,\n",
      "Train: train_2013_old [0011] TF1: 99.4193, Tloss: 0.00062272, VF1: 97.9300, VLoss: 0.00107375,\n",
      "Train: train_2013_old [0012] TF1: 99.5645, Tloss: 0.00054502, VF1: 97.0528, VLoss: 0.00207514,\n",
      "Train: train_2013_old [0013] TF1: 99.4403, Tloss: 0.00059283, VF1: 97.9109, VLoss: 0.00117313,\n",
      "Train: train_2013_old [0014] TF1: 99.5749, Tloss: 0.00051002, VF1: 98.1375, VLoss: 0.00121317,\n",
      "Train: train_2013_old [0015] TF1: 99.6266, Tloss: 0.00041940, VF1: 91.5803, VLoss: 0.00468141,\n",
      "Train: train_2013_old [0016] TF1: 99.5437, Tloss: 0.00044464, VF1: 97.5104, VLoss: 0.00146275,\n",
      "Epoch    16: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2013_old [0017] TF1: 99.6785, Tloss: 0.00033813, VF1: 99.1525, VLoss: 0.00059697,\n",
      "Train: train_2013_old [0018] TF1: 99.6993, Tloss: 0.00029675, VF1: 98.6741, VLoss: 0.00069794,\n",
      "Train: train_2013_old [0019] TF1: 99.6891, Tloss: 0.00045267, VF1: 98.3950, VLoss: 0.00086902,\n",
      "Train: train_2013_old [0020] TF1: 99.6475, Tloss: 0.00039437, VF1: 96.1398, VLoss: 0.00245081,\n",
      "Train: train_2013_old [0021] TF1: 99.7408, Tloss: 0.00029287, VF1: 99.1561, VLoss: 0.00061886,\n",
      "Train: train_2013_old [0022] TF1: 99.6683, Tloss: 0.00034069, VF1: 98.3630, VLoss: 0.00099354,\n",
      "Train: train_2013_old [0023] TF1: 99.6371, Tloss: 0.00033970, VF1: 98.5816, VLoss: 0.00082604,\n",
      "Train: train_2013_old [0024] TF1: 99.7097, Tloss: 0.00032186, VF1: 98.8780, VLoss: 0.00074407,\n",
      "Train: train_2013_old [0025] TF1: 99.7720, Tloss: 0.00026573, VF1: 98.4375, VLoss: 0.00127659,\n",
      "Train: train_2013_old [0026] TF1: 99.6891, Tloss: 0.00036356, VF1: 98.7270, VLoss: 0.00082860,\n",
      "Train: train_2013_old [0027] TF1: 99.6056, Tloss: 0.00041923, VF1: 99.0155, VLoss: 0.00069821,\n",
      "Train: train_2013_old [0028] TF1: 99.8444, Tloss: 0.00020728, VF1: 98.3146, VLoss: 0.00100911,\n",
      "Train: train_2013_old [0029] TF1: 99.6683, Tloss: 0.00033989, VF1: 99.0852, VLoss: 0.00066024,\n",
      "Train: train_2013_old [0030] TF1: 99.6786, Tloss: 0.00028859, VF1: 98.2578, VLoss: 0.00109624,\n",
      "Train: train_2013_old [0031] TF1: 99.6578, Tloss: 0.00032146, VF1: 98.1263, VLoss: 0.00123242,\n",
      "Train: train_2013_old [0032] TF1: 99.7201, Tloss: 0.00028322, VF1: 98.7952, VLoss: 0.00107159,\n",
      "Train: train_2013_old [0033] TF1: 99.6785, Tloss: 0.00029849, VF1: 98.2232, VLoss: 0.00120679,\n",
      "Train: train_2013_old [0034] TF1: 99.7718, Tloss: 0.00026746, VF1: 98.8732, VLoss: 0.00080334,\n",
      "Epoch    34: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2013_old [0035] TF1: 99.8133, Tloss: 0.00020993, VF1: 98.9459, VLoss: 0.00079258,\n",
      "Train: train_2013_old [0036] TF1: 99.7927, Tloss: 0.00020953, VF1: 98.7288, VLoss: 0.00091890,\n",
      "Train: train_2013_old [0037] TF1: 99.7201, Tloss: 0.00025714, VF1: 99.0852, VLoss: 0.00089751,\n",
      "Train: train_2013_old [0038] TF1: 99.8652, Tloss: 0.00016447, VF1: 98.7234, VLoss: 0.00102243,\n",
      "Train: train_2013_old [0039] TF1: 99.7304, Tloss: 0.00020870, VF1: 98.7952, VLoss: 0.00088186,\n",
      "Train: train_2013_old [0040] TF1: 99.7718, Tloss: 0.00026190, VF1: 98.7986, VLoss: 0.00073111,\n",
      "Train: train_2013_old [0041] TF1: 99.8238, Tloss: 0.00018750, VF1: 99.0852, VLoss: 0.00072069,\n",
      "Train: train_2013_old [0042] TF1: 99.8963, Tloss: 0.00014131, VF1: 99.1525, VLoss: 0.00075293,\n",
      "Train: train_2013_old [0043] TF1: 99.8652, Tloss: 0.00016321, VF1: 99.2226, VLoss: 0.00077133,\n",
      "Train: train_2013_old [0044] TF1: 99.6062, Tloss: 0.00032566, VF1: 98.7234, VLoss: 0.00085563,\n",
      "Train: train_2013_old [0045] TF1: 99.6578, Tloss: 0.00029225, VF1: 98.9399, VLoss: 0.00069972,\n",
      "Train: train_2013_old [0046] TF1: 99.8548, Tloss: 0.00014143, VF1: 98.9384, VLoss: 0.00085002,\n",
      "Train: train_2013_old [0047] TF1: 99.7408, Tloss: 0.00028807, VF1: 98.8636, VLoss: 0.00079823,\n",
      "Train: train_2013_old [0048] TF1: 99.8031, Tloss: 0.00023194, VF1: 97.1899, VLoss: 0.00177638,\n",
      "Epoch    48: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_2013_old [0049] TF1: 99.8963, Tloss: 0.00015007, VF1: 99.1537, VLoss: 0.00066007,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_017.pth Fragment=00 score=0.9938958707360861\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_021.pth Fragment=00 score=0.9935483870967742\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_008.pth Fragment=00 score=0.9899208063354932\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_049.pth Fragment=00 score=0.9942734430923408\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_029.pth Fragment=00 score=0.9900071377587439\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_018.pth Fragment=00 score=0.9889797369356559\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_027.pth Fragment=00 score=0.9906943450250537\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_045.pth Fragment=00 score=0.9939002511661285\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_041.pth Fragment=00 score=0.9914285714285714\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2013_old_009.pth Fragment=00 score=0.9889483065953654\n",
      "Extend: f1score=99.4265, vcoverage=98.3634, vf1score=99.8190, vexentdSize=2344, ecoverage=1.6366, ef1score=59.2593, erestSize=39\n",
      "########\n",
      "######## 2014_old\n",
      "13048\n",
      "1    9564\n",
      "0    3484\n",
      "Name: label, dtype: int64\n",
      "1864\n",
      "1    1363\n",
      "0     501\n",
      "Name: label, dtype: int64\n",
      "3728\n",
      "1    2727\n",
      "0    1001\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2014_old [0000] TF1: 95.9104, Tloss: 0.00509811, VF1: 96.9763, VLoss: 0.00194866,\n",
      "Train: train_2014_old [0001] TF1: 97.6100, Tloss: 0.00300032, VF1: 90.7031, VLoss: 0.00666318,\n",
      "Train: train_2014_old [0002] TF1: 98.1426, Tloss: 0.00246880, VF1: 96.3830, VLoss: 0.00259948,\n",
      "Train: train_2014_old [0003] TF1: 98.3879, Tloss: 0.00214063, VF1: 96.6554, VLoss: 0.00206896,\n",
      "Train: train_2014_old [0004] TF1: 98.4406, Tloss: 0.00193211, VF1: 92.7913, VLoss: 0.00589190,\n",
      "Train: train_2014_old [0005] TF1: 98.6760, Tloss: 0.00175407, VF1: 94.7662, VLoss: 0.00360758,\n",
      "Train: train_2014_old [0006] TF1: 98.7494, Tloss: 0.00170564, VF1: 96.2514, VLoss: 0.00235774,\n",
      "Train: train_2014_old [0007] TF1: 98.7812, Tloss: 0.00166169, VF1: 98.5031, VLoss: 0.00121460,\n",
      "Train: train_2014_old [0008] TF1: 98.8336, Tloss: 0.00155016, VF1: 97.6208, VLoss: 0.00190520,\n",
      "Train: train_2014_old [0009] TF1: 99.0635, Tloss: 0.00120018, VF1: 97.5073, VLoss: 0.00166553,\n",
      "Train: train_2014_old [0010] TF1: 98.9432, Tloss: 0.00141558, VF1: 92.3742, VLoss: 0.00474518,\n",
      "Train: train_2014_old [0011] TF1: 99.2103, Tloss: 0.00108332, VF1: 89.3631, VLoss: 0.01200076,\n",
      "Train: train_2014_old [0012] TF1: 99.0061, Tloss: 0.00122774, VF1: 97.7697, VLoss: 0.00157909,\n",
      "Train: train_2014_old [0013] TF1: 99.0848, Tloss: 0.00119257, VF1: 97.9167, VLoss: 0.00152034,\n",
      "Train: train_2014_old [0014] TF1: 99.2892, Tloss: 0.00100359, VF1: 95.1681, VLoss: 0.00424310,\n",
      "Train: train_2014_old [0015] TF1: 99.1951, Tloss: 0.00106263, VF1: 96.5788, VLoss: 0.00287932,\n",
      "Train: train_2014_old [0016] TF1: 99.1997, Tloss: 0.00104222, VF1: 97.7697, VLoss: 0.00154413,\n",
      "Train: train_2014_old [0017] TF1: 99.1997, Tloss: 0.00107862, VF1: 91.2504, VLoss: 0.00774179,\n",
      "Train: train_2014_old [0018] TF1: 99.3308, Tloss: 0.00092027, VF1: 97.1193, VLoss: 0.00213255,\n",
      "Train: train_2014_old [0019] TF1: 99.1686, Tloss: 0.00109771, VF1: 98.2714, VLoss: 0.00110030,\n",
      "Train: train_2014_old [0020] TF1: 99.2469, Tloss: 0.00094371, VF1: 90.8484, VLoss: 0.01097769,\n",
      "Train: train_2014_old [0021] TF1: 99.3355, Tloss: 0.00086212, VF1: 97.2212, VLoss: 0.00228394,\n",
      "Train: train_2014_old [0022] TF1: 99.1212, Tloss: 0.00107486, VF1: 97.0304, VLoss: 0.00296276,\n",
      "Train: train_2014_old [0023] TF1: 99.3930, Tloss: 0.00084472, VF1: 97.4740, VLoss: 0.00176223,\n",
      "Train: train_2014_old [0024] TF1: 99.3202, Tloss: 0.00084301, VF1: 98.0748, VLoss: 0.00150909,\n",
      "Train: train_2014_old [0025] TF1: 99.2886, Tloss: 0.00091059, VF1: 97.8475, VLoss: 0.00161390,\n",
      "Train: train_2014_old [0026] TF1: 99.4351, Tloss: 0.00078289, VF1: 97.9083, VLoss: 0.00188123,\n",
      "Train: train_2014_old [0027] TF1: 99.4247, Tloss: 0.00085785, VF1: 97.8897, VLoss: 0.00164812,\n",
      "Train: train_2014_old [0028] TF1: 99.4088, Tloss: 0.00082678, VF1: 97.1695, VLoss: 0.00222328,\n",
      "Train: train_2014_old [0029] TF1: 99.3195, Tloss: 0.00087966, VF1: 98.0022, VLoss: 0.00186590,\n",
      "Train: train_2014_old [0030] TF1: 99.4505, Tloss: 0.00077932, VF1: 97.9710, VLoss: 0.00161670,\n",
      "Train: train_2014_old [0031] TF1: 99.5080, Tloss: 0.00068308, VF1: 93.9100, VLoss: 0.00692727,\n",
      "Train: train_2014_old [0032] TF1: 99.4141, Tloss: 0.00079338, VF1: 97.5171, VLoss: 0.00217748,\n",
      "Train: train_2014_old [0033] TF1: 99.5134, Tloss: 0.00067801, VF1: 96.9764, VLoss: 0.00243079,\n",
      "Train: train_2014_old [0034] TF1: 99.3827, Tloss: 0.00081054, VF1: 98.3113, VLoss: 0.00144776,\n",
      "Train: train_2014_old [0035] TF1: 99.4455, Tloss: 0.00073628, VF1: 98.1658, VLoss: 0.00143101,\n",
      "Train: train_2014_old [0036] TF1: 99.3669, Tloss: 0.00076896, VF1: 96.6292, VLoss: 0.00239675,\n",
      "Train: train_2014_old [0037] TF1: 99.5083, Tloss: 0.00074485, VF1: 98.0065, VLoss: 0.00184668,\n",
      "Train: train_2014_old [0038] TF1: 99.4561, Tloss: 0.00071367, VF1: 93.8224, VLoss: 0.00398256,\n",
      "Train: train_2014_old [0039] TF1: 99.4716, Tloss: 0.00070080, VF1: 97.1326, VLoss: 0.00232124,\n",
      "Epoch    39: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2014_old [0040] TF1: 99.5972, Tloss: 0.00051841, VF1: 98.0464, VLoss: 0.00149194,\n",
      "Train: train_2014_old [0041] TF1: 99.5033, Tloss: 0.00066788, VF1: 98.2545, VLoss: 0.00208044,\n",
      "Train: train_2014_old [0042] TF1: 99.6232, Tloss: 0.00054207, VF1: 98.3425, VLoss: 0.00126306,\n",
      "Train: train_2014_old [0043] TF1: 99.4455, Tloss: 0.00065381, VF1: 93.7241, VLoss: 0.00733816,\n",
      "Train: train_2014_old [0044] TF1: 99.5658, Tloss: 0.00059422, VF1: 94.7610, VLoss: 0.00361390,\n",
      "Train: train_2014_old [0045] TF1: 99.6861, Tloss: 0.00044767, VF1: 97.7071, VLoss: 0.00197485,\n",
      "Train: train_2014_old [0046] TF1: 99.6025, Tloss: 0.00048275, VF1: 97.9336, VLoss: 0.00163484,\n",
      "Train: train_2014_old [0047] TF1: 99.6547, Tloss: 0.00050567, VF1: 98.0278, VLoss: 0.00169678,\n",
      "Train: train_2014_old [0048] TF1: 99.5658, Tloss: 0.00057173, VF1: 98.1725, VLoss: 0.00200674,\n",
      "Train: train_2014_old [0049] TF1: 99.6078, Tloss: 0.00053548, VF1: 97.2352, VLoss: 0.00315471,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_019.pth Fragment=00 score=0.9878899082568807\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_007.pth Fragment=00 score=0.9861009509875639\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_042.pth Fragment=00 score=0.9869844179651696\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_035.pth Fragment=00 score=0.9862963639685729\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_034.pth Fragment=00 score=0.9858948525370946\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_040.pth Fragment=00 score=0.9867841409691631\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_024.pth Fragment=00 score=0.9865209471766849\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_013.pth Fragment=00 score=0.9841503870254332\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_016.pth Fragment=00 score=0.9853854585312386\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_old_012.pth Fragment=00 score=0.98406301520425\n",
      "Extend: f1score=98.9401, vcoverage=97.4249, vf1score=99.4774, vexentdSize=3632, ecoverage=2.5751, ef1score=73.6842, erestSize=96\n",
      "########\n",
      "######## 2014_new\n",
      "10704\n",
      "1    9549\n",
      "0    1155\n",
      "Name: label, dtype: int64\n",
      "1529\n",
      "1    1368\n",
      "0     161\n",
      "Name: label, dtype: int64\n",
      "3059\n",
      "1    2737\n",
      "0     322\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2014_new [0000] TF1: 98.4375, Tloss: 0.00362392, VF1: 99.4536, VLoss: 0.00097813,\n",
      "Train: train_2014_new [0001] TF1: 99.1278, Tloss: 0.00151219, VF1: 97.8390, VLoss: 0.00200550,\n",
      "Train: train_2014_new [0002] TF1: 99.4664, Tloss: 0.00104456, VF1: 99.5984, VLoss: 0.00060349,\n",
      "Train: train_2014_new [0003] TF1: 99.4195, Tloss: 0.00095885, VF1: 99.5614, VLoss: 0.00039638,\n",
      "Train: train_2014_new [0004] TF1: 99.5082, Tloss: 0.00079059, VF1: 99.5243, VLoss: 0.00054480,\n",
      "Train: train_2014_new [0005] TF1: 99.5970, Tloss: 0.00062427, VF1: 99.2738, VLoss: 0.00069281,\n",
      "Train: train_2014_new [0006] TF1: 99.6285, Tloss: 0.00066768, VF1: 99.4539, VLoss: 0.00054115,\n",
      "Train: train_2014_new [0007] TF1: 99.6126, Tloss: 0.00061271, VF1: 99.4539, VLoss: 0.00061746,\n",
      "Train: train_2014_new [0008] TF1: 99.6232, Tloss: 0.00059625, VF1: 95.3071, VLoss: 0.00279226,\n",
      "Train: train_2014_new [0009] TF1: 99.7226, Tloss: 0.00044069, VF1: 99.4508, VLoss: 0.00050862,\n",
      "Train: train_2014_new [0010] TF1: 99.7068, Tloss: 0.00044522, VF1: 99.4508, VLoss: 0.00036743,\n",
      "Train: train_2014_new [0011] TF1: 99.7278, Tloss: 0.00041820, VF1: 99.5978, VLoss: 0.00032613,\n",
      "Train: train_2014_new [0012] TF1: 99.7225, Tloss: 0.00044991, VF1: 99.5981, VLoss: 0.00055714,\n",
      "Train: train_2014_new [0013] TF1: 99.7749, Tloss: 0.00036451, VF1: 95.1798, VLoss: 0.00247291,\n",
      "Train: train_2014_new [0014] TF1: 99.7645, Tloss: 0.00040654, VF1: 99.1912, VLoss: 0.00073055,\n",
      "Train: train_2014_new [0015] TF1: 99.8011, Tloss: 0.00036168, VF1: 99.6721, VLoss: 0.00039586,\n",
      "Train: train_2014_new [0016] TF1: 99.7226, Tloss: 0.00042859, VF1: 99.2669, VLoss: 0.00052571,\n",
      "Train: train_2014_new [0017] TF1: 99.8220, Tloss: 0.00033134, VF1: 99.4139, VLoss: 0.00058591,\n",
      "Train: train_2014_new [0018] TF1: 99.8116, Tloss: 0.00033367, VF1: 99.0077, VLoss: 0.00070684,\n",
      "Train: train_2014_new [0019] TF1: 99.7958, Tloss: 0.00035788, VF1: 99.6719, VLoss: 0.00051062,\n",
      "Train: train_2014_new [0020] TF1: 99.7644, Tloss: 0.00040716, VF1: 86.5066, VLoss: 0.00762825,\n",
      "Train: train_2014_new [0021] TF1: 99.8168, Tloss: 0.00038729, VF1: 99.5268, VLoss: 0.00043334,\n",
      "Train: train_2014_new [0022] TF1: 99.7331, Tloss: 0.00042771, VF1: 99.1547, VLoss: 0.00071373,\n",
      "Train: train_2014_new [0023] TF1: 99.8377, Tloss: 0.00033609, VF1: 99.5264, VLoss: 0.00056304,\n",
      "Epoch    23: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2014_new [0024] TF1: 99.9005, Tloss: 0.00019191, VF1: 99.4135, VLoss: 0.00046379,\n",
      "Train: train_2014_new [0025] TF1: 99.8482, Tloss: 0.00025621, VF1: 99.5981, VLoss: 0.00044796,\n",
      "Train: train_2014_new [0026] TF1: 99.8429, Tloss: 0.00028557, VF1: 99.6348, VLoss: 0.00048577,\n",
      "Train: train_2014_new [0027] TF1: 99.8063, Tloss: 0.00030515, VF1: 99.5984, VLoss: 0.00037462,\n",
      "Train: train_2014_new [0028] TF1: 99.8482, Tloss: 0.00023736, VF1: 99.6345, VLoss: 0.00032185,\n",
      "Train: train_2014_new [0029] TF1: 99.8743, Tloss: 0.00023600, VF1: 99.6353, VLoss: 0.00036797,\n",
      "Train: train_2014_new [0030] TF1: 99.8482, Tloss: 0.00023796, VF1: 99.5993, VLoss: 0.00049382,\n",
      "Epoch    30: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2014_new [0031] TF1: 99.8534, Tloss: 0.00030232, VF1: 99.6350, VLoss: 0.00042518,\n",
      "Train: train_2014_new [0032] TF1: 99.9372, Tloss: 0.00016195, VF1: 99.3397, VLoss: 0.00060739,\n",
      "Train: train_2014_new [0033] TF1: 99.8744, Tloss: 0.00023474, VF1: 96.8350, VLoss: 0.00190541,\n",
      "Train: train_2014_new [0034] TF1: 99.8116, Tloss: 0.00032022, VF1: 99.2663, VLoss: 0.00064286,\n",
      "Train: train_2014_new [0035] TF1: 99.8743, Tloss: 0.00023179, VF1: 99.3464, VLoss: 0.00070752,\n",
      "Train: train_2014_new [0036] TF1: 99.8639, Tloss: 0.00018090, VF1: 98.4444, VLoss: 0.00126446,\n",
      "Train: train_2014_new [0037] TF1: 99.8901, Tloss: 0.00019349, VF1: 98.8582, VLoss: 0.00079602,\n",
      "Train: train_2014_new [0038] TF1: 99.9110, Tloss: 0.00018782, VF1: 99.5611, VLoss: 0.00050329,\n",
      "Epoch    38: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_2014_new [0039] TF1: 99.9267, Tloss: 0.00012610, VF1: 99.6353, VLoss: 0.00047005,\n",
      "Train: train_2014_new [0040] TF1: 99.9058, Tloss: 0.00016620, VF1: 99.4887, VLoss: 0.00041616,\n",
      "Train: train_2014_new [0041] TF1: 99.9476, Tloss: 0.00012445, VF1: 99.5250, VLoss: 0.00043773,\n",
      "Train: train_2014_new [0042] TF1: 99.9162, Tloss: 0.00013961, VF1: 99.7082, VLoss: 0.00044123,\n",
      "Train: train_2014_new [0043] TF1: 99.9005, Tloss: 0.00016386, VF1: 99.5614, VLoss: 0.00049573,\n",
      "Train: train_2014_new [0044] TF1: 99.8900, Tloss: 0.00015563, VF1: 99.5617, VLoss: 0.00041823,\n",
      "Train: train_2014_new [0045] TF1: 99.9110, Tloss: 0.00017073, VF1: 99.5250, VLoss: 0.00044110,\n",
      "Train: train_2014_new [0046] TF1: 99.9424, Tloss: 0.00011197, VF1: 99.6353, VLoss: 0.00040968,\n",
      "Train: train_2014_new [0047] TF1: 99.9424, Tloss: 0.00007990, VF1: 99.5996, VLoss: 0.00050917,\n",
      "Train: train_2014_new [0048] TF1: 99.9005, Tloss: 0.00017512, VF1: 99.5254, VLoss: 0.00043712,\n",
      "Train: train_2014_new [0049] TF1: 99.9424, Tloss: 0.00012873, VF1: 99.4152, VLoss: 0.00055228,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_028.pth Fragment=00 score=0.9941434846266471\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_011.pth Fragment=00 score=0.993421052631579\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_010.pth Fragment=00 score=0.9939637826961769\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_029.pth Fragment=00 score=0.9950702939565456\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_027.pth Fragment=00 score=0.9934354485776804\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_015.pth Fragment=00 score=0.9950774840474019\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_003.pth Fragment=00 score=0.9939857845817387\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_046.pth Fragment=00 score=0.9947089947089948\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_040.pth Fragment=00 score=0.9945135332845648\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2014_new_044.pth Fragment=00 score=0.995243322356385\n",
      "Extend: f1score=99.5437, vcoverage=98.5616, vf1score=99.8342, vexentdSize=3015, ecoverage=1.4384, ef1score=69.2308, erestSize=44\n",
      "########\n",
      "######## 2015_new\n",
      "3758\n",
      "1    2603\n",
      "0    1155\n",
      "Name: label, dtype: int64\n",
      "538\n",
      "1    386\n",
      "0    152\n",
      "Name: label, dtype: int64\n",
      "1074\n",
      "1    743\n",
      "0    331\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2015_new [0000] TF1: 92.5732, Tloss: 0.00788576, VF1: 91.1032, VLoss: 0.00489485,\n",
      "Train: train_2015_new [0001] TF1: 95.7622, Tloss: 0.00472340, VF1: 91.4685, VLoss: 0.00516392,\n",
      "Train: train_2015_new [0002] TF1: 96.3206, Tloss: 0.00382449, VF1: 94.2623, VLoss: 0.00390201,\n",
      "Train: train_2015_new [0003] TF1: 97.4576, Tloss: 0.00330367, VF1: 95.6975, VLoss: 0.00263213,\n",
      "Train: train_2015_new [0004] TF1: 97.5207, Tloss: 0.00299571, VF1: 96.6667, VLoss: 0.00213493,\n",
      "Train: train_2015_new [0005] TF1: 97.4933, Tloss: 0.00288172, VF1: 80.3709, VLoss: 0.01034004,\n",
      "Train: train_2015_new [0006] TF1: 97.2848, Tloss: 0.00311930, VF1: 89.7143, VLoss: 0.00618295,\n",
      "Train: train_2015_new [0007] TF1: 98.1692, Tloss: 0.00211560, VF1: 94.9495, VLoss: 0.00282204,\n",
      "Train: train_2015_new [0008] TF1: 97.8060, Tloss: 0.00236025, VF1: 92.7098, VLoss: 0.00436185,\n",
      "Train: train_2015_new [0009] TF1: 98.0769, Tloss: 0.00223929, VF1: 93.5880, VLoss: 0.00361802,\n",
      "Train: train_2015_new [0010] TF1: 98.2098, Tloss: 0.00206086, VF1: 97.6562, VLoss: 0.00160113,\n",
      "Train: train_2015_new [0011] TF1: 98.0596, Tloss: 0.00198496, VF1: 85.9688, VLoss: 0.02100789,\n",
      "Train: train_2015_new [0012] TF1: 98.2105, Tloss: 0.00190152, VF1: 96.5517, VLoss: 0.00226530,\n",
      "Train: train_2015_new [0013] TF1: 98.3410, Tloss: 0.00189610, VF1: 96.7575, VLoss: 0.00210751,\n",
      "Train: train_2015_new [0014] TF1: 98.4237, Tloss: 0.00185327, VF1: 73.6498, VLoss: 0.01225320,\n",
      "Train: train_2015_new [0015] TF1: 98.2476, Tloss: 0.00197966, VF1: 96.1538, VLoss: 0.00215911,\n",
      "Train: train_2015_new [0016] TF1: 98.7878, Tloss: 0.00157013, VF1: 96.8912, VLoss: 0.00198927,\n",
      "Train: train_2015_new [0017] TF1: 98.2875, Tloss: 0.00185609, VF1: 96.5340, VLoss: 0.00203393,\n",
      "Train: train_2015_new [0018] TF1: 98.6344, Tloss: 0.00167270, VF1: 97.6923, VLoss: 0.00189635,\n",
      "Train: train_2015_new [0019] TF1: 98.6918, Tloss: 0.00155210, VF1: 96.6408, VLoss: 0.00181256,\n",
      "Train: train_2015_new [0020] TF1: 98.6928, Tloss: 0.00146378, VF1: 97.5484, VLoss: 0.00169599,\n",
      "Train: train_2015_new [0021] TF1: 98.5569, Tloss: 0.00166464, VF1: 95.0336, VLoss: 0.00303186,\n",
      "Train: train_2015_new [0022] TF1: 98.8475, Tloss: 0.00127719, VF1: 96.1792, VLoss: 0.00256889,\n",
      "Train: train_2015_new [0023] TF1: 98.5189, Tloss: 0.00152819, VF1: 96.1440, VLoss: 0.00185630,\n",
      "Train: train_2015_new [0024] TF1: 98.6338, Tloss: 0.00190356, VF1: 96.1385, VLoss: 0.00228791,\n",
      "Train: train_2015_new [0025] TF1: 98.8280, Tloss: 0.00150566, VF1: 91.6865, VLoss: 0.00616024,\n",
      "Train: train_2015_new [0026] TF1: 99.2308, Tloss: 0.00106598, VF1: 94.5679, VLoss: 0.00392073,\n",
      "Train: train_2015_new [0027] TF1: 99.0000, Tloss: 0.00126328, VF1: 93.6430, VLoss: 0.00459128,\n",
      "Train: train_2015_new [0028] TF1: 98.7702, Tloss: 0.00139056, VF1: 96.1487, VLoss: 0.00266501,\n",
      "Train: train_2015_new [0029] TF1: 98.7887, Tloss: 0.00142383, VF1: 96.1691, VLoss: 0.00284856,\n",
      "Train: train_2015_new [0030] TF1: 99.0374, Tloss: 0.00129536, VF1: 96.4798, VLoss: 0.00251501,\n",
      "Train: train_2015_new [0031] TF1: 98.7678, Tloss: 0.00138401, VF1: 96.7825, VLoss: 0.00219487,\n",
      "Train: train_2015_new [0032] TF1: 99.0586, Tloss: 0.00112214, VF1: 96.3061, VLoss: 0.00244755,\n",
      "Epoch    32: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2015_new [0033] TF1: 99.0564, Tloss: 0.00125040, VF1: 97.3890, VLoss: 0.00196058,\n",
      "Train: train_2015_new [0034] TF1: 98.9421, Tloss: 0.00131336, VF1: 96.1282, VLoss: 0.00285407,\n",
      "Train: train_2015_new [0035] TF1: 99.1542, Tloss: 0.00095246, VF1: 96.4557, VLoss: 0.00254981,\n",
      "Train: train_2015_new [0036] TF1: 99.1926, Tloss: 0.00097998, VF1: 97.0246, VLoss: 0.00229446,\n",
      "Train: train_2015_new [0037] TF1: 99.0194, Tloss: 0.00115973, VF1: 97.2692, VLoss: 0.00255053,\n",
      "Train: train_2015_new [0038] TF1: 99.1739, Tloss: 0.00098519, VF1: 95.1432, VLoss: 0.00358977,\n",
      "Train: train_2015_new [0039] TF1: 99.1748, Tloss: 0.00100430, VF1: 96.7005, VLoss: 0.00230484,\n",
      "Train: train_2015_new [0040] TF1: 98.9413, Tloss: 0.00127344, VF1: 97.5228, VLoss: 0.00196183,\n",
      "Train: train_2015_new [0041] TF1: 99.1723, Tloss: 0.00103520, VF1: 97.6562, VLoss: 0.00215112,\n",
      "Epoch    41: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2015_new [0042] TF1: 99.3271, Tloss: 0.00083880, VF1: 97.2763, VLoss: 0.00191791,\n",
      "Train: train_2015_new [0043] TF1: 99.2120, Tloss: 0.00100558, VF1: 97.2973, VLoss: 0.00209295,\n",
      "Train: train_2015_new [0044] TF1: 99.4233, Tloss: 0.00074467, VF1: 96.9935, VLoss: 0.00247000,\n",
      "Train: train_2015_new [0045] TF1: 99.4235, Tloss: 0.00070963, VF1: 97.3684, VLoss: 0.00173597,\n",
      "Train: train_2015_new [0046] TF1: 99.4805, Tloss: 0.00079229, VF1: 97.0169, VLoss: 0.00207835,\n",
      "Train: train_2015_new [0047] TF1: 99.4042, Tloss: 0.00074841, VF1: 96.3338, VLoss: 0.00296482,\n",
      "Train: train_2015_new [0048] TF1: 99.4035, Tloss: 0.00080505, VF1: 97.5484, VLoss: 0.00185153,\n",
      "Train: train_2015_new [0049] TF1: 99.4611, Tloss: 0.00068416, VF1: 98.1959, VLoss: 0.00166964,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_010.pth Fragment=00 score=0.9625167336010709\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_049.pth Fragment=00 score=0.9719626168224298\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_020.pth Fragment=00 score=0.9733688415446071\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_045.pth Fragment=00 score=0.971350613915416\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_019.pth Fragment=00 score=0.9648774022531478\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_048.pth Fragment=00 score=0.9745308310991957\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_023.pth Fragment=00 score=0.9647371922821024\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_018.pth Fragment=00 score=0.9696169088507265\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_042.pth Fragment=00 score=0.9726848767488342\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2015_new_033.pth Fragment=00 score=0.9710047201618341\n",
      "Extend: f1score=97.9893, vcoverage=93.2961, vf1score=98.9518, vexentdSize=1002, ecoverage=6.7039, ef1score=75.4098, erestSize=72\n",
      "########\n",
      "######## 2016_new\n",
      "2675\n",
      "1    1515\n",
      "0    1160\n",
      "Name: label, dtype: int64\n",
      "382\n",
      "1    213\n",
      "0    169\n",
      "Name: label, dtype: int64\n",
      "765\n",
      "1    456\n",
      "0    309\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2016_new [0000] TF1: 89.7133, Tloss: 0.00949826, VF1: 94.3396, VLoss: 0.00273960,\n",
      "Train: train_2016_new [0001] TF1: 94.3472, Tloss: 0.00558561, VF1: 95.7143, VLoss: 0.00209092,\n",
      "Train: train_2016_new [0002] TF1: 95.1333, Tloss: 0.00466713, VF1: 91.6456, VLoss: 0.00340826,\n",
      "Train: train_2016_new [0003] TF1: 95.9123, Tloss: 0.00398269, VF1: 88.6555, VLoss: 0.00446925,\n",
      "Train: train_2016_new [0004] TF1: 96.0771, Tloss: 0.00388119, VF1: 96.0373, VLoss: 0.00185632,\n",
      "Train: train_2016_new [0005] TF1: 96.0901, Tloss: 0.00375275, VF1: 80.8349, VLoss: 0.00801830,\n",
      "Train: train_2016_new [0006] TF1: 96.1039, Tloss: 0.00346414, VF1: 89.9743, VLoss: 0.00348346,\n",
      "Train: train_2016_new [0007] TF1: 96.4416, Tloss: 0.00343197, VF1: 75.4448, VLoss: 0.01228726,\n",
      "Train: train_2016_new [0008] TF1: 97.1125, Tloss: 0.00278624, VF1: 96.3504, VLoss: 0.00174328,\n",
      "Train: train_2016_new [0009] TF1: 97.0060, Tloss: 0.00286472, VF1: 95.8904, VLoss: 0.00226928,\n",
      "Train: train_2016_new [0010] TF1: 97.1808, Tloss: 0.00266373, VF1: 96.6346, VLoss: 0.00146667,\n",
      "Train: train_2016_new [0011] TF1: 97.1770, Tloss: 0.00284160, VF1: 71.8381, VLoss: 0.03027840,\n",
      "Train: train_2016_new [0012] TF1: 97.6667, Tloss: 0.00256729, VF1: 89.3617, VLoss: 0.00458064,\n",
      "Train: train_2016_new [0013] TF1: 97.8088, Tloss: 0.00216224, VF1: 97.4118, VLoss: 0.00135131,\n",
      "Train: train_2016_new [0014] TF1: 97.6729, Tloss: 0.00226028, VF1: 96.6184, VLoss: 0.00161279,\n",
      "Train: train_2016_new [0015] TF1: 97.5383, Tloss: 0.00245347, VF1: 97.2093, VLoss: 0.00129288,\n",
      "Train: train_2016_new [0016] TF1: 97.7719, Tloss: 0.00224513, VF1: 96.6981, VLoss: 0.00146088,\n",
      "Train: train_2016_new [0017] TF1: 98.1082, Tloss: 0.00197234, VF1: 95.4128, VLoss: 0.00187382,\n",
      "Train: train_2016_new [0018] TF1: 97.5432, Tloss: 0.00236531, VF1: 97.8622, VLoss: 0.00128335,\n",
      "Train: train_2016_new [0019] TF1: 97.3457, Tloss: 0.00255595, VF1: 84.5238, VLoss: 0.00871312,\n",
      "Train: train_2016_new [0020] TF1: 97.9063, Tloss: 0.00221001, VF1: 95.6720, VLoss: 0.00174262,\n",
      "Train: train_2016_new [0021] TF1: 97.7778, Tloss: 0.00221068, VF1: 91.3420, VLoss: 0.00378208,\n",
      "Train: train_2016_new [0022] TF1: 97.8541, Tloss: 0.00205327, VF1: 94.7846, VLoss: 0.00265480,\n",
      "Train: train_2016_new [0023] TF1: 97.2361, Tloss: 0.00231664, VF1: 96.4029, VLoss: 0.00153066,\n",
      "Epoch    23: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2016_new [0024] TF1: 97.9781, Tloss: 0.00188952, VF1: 94.5946, VLoss: 0.00213669,\n",
      "Train: train_2016_new [0025] TF1: 98.5757, Tloss: 0.00149314, VF1: 93.2331, VLoss: 0.00320139,\n",
      "Train: train_2016_new [0026] TF1: 98.2747, Tloss: 0.00149493, VF1: 94.6429, VLoss: 0.00271209,\n",
      "Train: train_2016_new [0027] TF1: 98.2804, Tloss: 0.00184529, VF1: 97.8824, VLoss: 0.00083849,\n",
      "Train: train_2016_new [0028] TF1: 98.4808, Tloss: 0.00153127, VF1: 96.5358, VLoss: 0.00230441,\n",
      "Train: train_2016_new [0029] TF1: 98.8064, Tloss: 0.00134328, VF1: 96.6184, VLoss: 0.00140211,\n",
      "Train: train_2016_new [0030] TF1: 98.0782, Tloss: 0.00179636, VF1: 96.7442, VLoss: 0.00171028,\n",
      "Train: train_2016_new [0031] TF1: 98.0782, Tloss: 0.00174318, VF1: 97.3872, VLoss: 0.00124802,\n",
      "Train: train_2016_new [0032] TF1: 98.4758, Tloss: 0.00161037, VF1: 97.1831, VLoss: 0.00130459,\n",
      "Train: train_2016_new [0033] TF1: 98.6383, Tloss: 0.00142389, VF1: 97.6303, VLoss: 0.00114902,\n",
      "Train: train_2016_new [0034] TF1: 98.5401, Tloss: 0.00157734, VF1: 88.8889, VLoss: 0.00454291,\n",
      "Train: train_2016_new [0035] TF1: 98.7124, Tloss: 0.00126127, VF1: 94.8905, VLoss: 0.00226069,\n",
      "Train: train_2016_new [0036] TF1: 98.2084, Tloss: 0.00171690, VF1: 96.1905, VLoss: 0.00134795,\n",
      "Train: train_2016_new [0037] TF1: 98.5747, Tloss: 0.00142232, VF1: 97.8824, VLoss: 0.00128191,\n",
      "Train: train_2016_new [0038] TF1: 98.7090, Tloss: 0.00153531, VF1: 96.6019, VLoss: 0.00177022,\n",
      "Train: train_2016_new [0039] TF1: 98.4778, Tloss: 0.00153323, VF1: 97.1831, VLoss: 0.00143699,\n",
      "Train: train_2016_new [0040] TF1: 98.5085, Tloss: 0.00130629, VF1: 97.8622, VLoss: 0.00111805,\n",
      "Train: train_2016_new [0041] TF1: 98.7409, Tloss: 0.00124960, VF1: 98.1221, VLoss: 0.00101820,\n",
      "Train: train_2016_new [0042] TF1: 98.7401, Tloss: 0.00129838, VF1: 90.9871, VLoss: 0.00557667,\n",
      "Train: train_2016_new [0043] TF1: 98.6755, Tloss: 0.00134540, VF1: 96.0739, VLoss: 0.00169182,\n",
      "Train: train_2016_new [0044] TF1: 98.6755, Tloss: 0.00143310, VF1: 84.5238, VLoss: 0.00848937,\n",
      "Train: train_2016_new [0045] TF1: 98.9390, Tloss: 0.00127898, VF1: 96.3470, VLoss: 0.00184605,\n",
      "Train: train_2016_new [0046] TF1: 98.6746, Tloss: 0.00141926, VF1: 92.5110, VLoss: 0.00368342,\n",
      "Train: train_2016_new [0047] TF1: 98.5757, Tloss: 0.00136691, VF1: 97.6190, VLoss: 0.00143257,\n",
      "Epoch    47: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2016_new [0048] TF1: 98.8414, Tloss: 0.00122073, VF1: 96.5675, VLoss: 0.00173398,\n",
      "Train: train_2016_new [0049] TF1: 98.7712, Tloss: 0.00121331, VF1: 97.2477, VLoss: 0.00153560,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_027.pth Fragment=00 score=0.9675977653631285\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_041.pth Fragment=00 score=0.9656699889258029\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_040.pth Fragment=00 score=0.9646799116997793\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_033.pth Fragment=00 score=0.9690949227373069\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_031.pth Fragment=00 score=0.9594594594594595\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_037.pth Fragment=00 score=0.9633740288568257\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_018.pth Fragment=00 score=0.9642058165548097\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_015.pth Fragment=00 score=0.9606986899563318\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_032.pth Fragment=00 score=0.9658213891951488\n",
      "Evaluate: ModelPath=./traces/comparativeWS02/model_train_2016_new_036.pth Fragment=00 score=0.9675977653631285\n",
      "Extend: f1score=97.4416, vcoverage=94.7712, vf1score=98.6047, vexentdSize=725, ecoverage=5.2288, ef1score=71.7949, erestSize=40\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for tag in tags:\n",
    "    currentTag = tag[0] + '_' + tag[1]\n",
    "\n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    #\n",
    "    dataset_df = mamadroid_df.loc[(mamadroid_df.year == tag[0]) | (mamadroid_df.year == tag[1])]\n",
    "\n",
    "    #\n",
    "    trainLoader, validLoader, testLoader = getDataloaders(dataset_df, trainPercentage=trainPercentageParam, \n",
    "                                                                      validPercentage=validPercentageParam)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "    #\n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "    #\n",
    "    evalDataset(ws, evalresult_df, probaUpperBorn=0.8,  probaLowerBorn=0.2)\n",
    "\n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    #\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
