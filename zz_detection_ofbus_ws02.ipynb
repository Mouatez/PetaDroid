{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):int(validPercentage * len(dataset_df))]]\n",
    "    test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'obfusWS02'\n",
    "epochNum         = 100\n",
    "device           = torch.device('cuda:4')\n",
    "ensembleSize     = 20\n",
    "\n",
    "trainPercentageParam = 0.4\n",
    "validPercentageParam = 0.5\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## praguard\n",
      "11814\n",
      "0    7870\n",
      "1    3944\n",
      "Name: label, dtype: int64\n",
      "2953\n",
      "0    1971\n",
      "1     982\n",
      "Name: label, dtype: int64\n",
      "14768\n",
      "0    9849\n",
      "1    4919\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_praguard [0000] TF1: 96.6924, Tloss: 0.00210249, VF1: 51.8793, VLoss: 0.02009798,\n",
      "Train: train_praguard [0001] TF1: 98.7303, Tloss: 0.00079997, VF1: 93.8248, VLoss: 0.00256995,\n",
      "Train: train_praguard [0002] TF1: 99.1238, Tloss: 0.00054068, VF1: 98.5830, VLoss: 0.00034164,\n",
      "Train: train_praguard [0003] TF1: 99.5058, Tloss: 0.00036194, VF1: 98.6357, VLoss: 0.00040918,\n",
      "Train: train_praguard [0004] TF1: 99.2769, Tloss: 0.00045897, VF1: 97.7092, VLoss: 0.00073779,\n",
      "Train: train_praguard [0005] TF1: 99.5815, Tloss: 0.00030447, VF1: 97.0235, VLoss: 0.00084420,\n",
      "Train: train_praguard [0006] TF1: 99.4675, Tloss: 0.00034181, VF1: 98.9346, VLoss: 0.00031536,\n",
      "Train: train_praguard [0007] TF1: 99.6956, Tloss: 0.00020632, VF1: 99.4908, VLoss: 0.00024324,\n",
      "Train: train_praguard [0008] TF1: 99.5308, Tloss: 0.00029770, VF1: 98.9389, VLoss: 0.00037489,\n",
      "Train: train_praguard [0009] TF1: 99.5311, Tloss: 0.00034094, VF1: 71.9266, VLoss: 0.01083552,\n",
      "Train: train_praguard [0010] TF1: 99.5942, Tloss: 0.00025457, VF1: 99.0854, VLoss: 0.00026387,\n",
      "Train: train_praguard [0011] TF1: 99.7335, Tloss: 0.00017702, VF1: 95.8231, VLoss: 0.00131561,\n",
      "Train: train_praguard [0012] TF1: 99.5058, Tloss: 0.00031428, VF1: 99.4893, VLoss: 0.00022889,\n",
      "Train: train_praguard [0013] TF1: 99.3787, Tloss: 0.00033475, VF1: 99.1331, VLoss: 0.00028312,\n",
      "Train: train_praguard [0014] TF1: 99.3163, Tloss: 0.00041627, VF1: 98.7642, VLoss: 0.00033740,\n",
      "Train: train_praguard [0015] TF1: 99.5817, Tloss: 0.00031283, VF1: 99.4908, VLoss: 0.00020244,\n",
      "Train: train_praguard [0016] TF1: 99.7845, Tloss: 0.00013057, VF1: 99.2835, VLoss: 0.00026423,\n",
      "Train: train_praguard [0017] TF1: 99.7972, Tloss: 0.00013556, VF1: 99.1803, VLoss: 0.00020357,\n",
      "Train: train_praguard [0018] TF1: 99.8225, Tloss: 0.00013189, VF1: 99.4408, VLoss: 0.00016661,\n",
      "Train: train_praguard [0019] TF1: 99.5565, Tloss: 0.00031182, VF1: 98.4848, VLoss: 0.00047013,\n",
      "Train: train_praguard [0020] TF1: 99.5691, Tloss: 0.00025107, VF1: 99.0891, VLoss: 0.00027856,\n",
      "Train: train_praguard [0021] TF1: 99.6829, Tloss: 0.00020170, VF1: 97.9654, VLoss: 0.00067168,\n",
      "Train: train_praguard [0022] TF1: 99.6198, Tloss: 0.00024551, VF1: 99.0798, VLoss: 0.00024148,\n",
      "Epoch    22: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_praguard [0023] TF1: 99.7720, Tloss: 0.00012885, VF1: 99.1340, VLoss: 0.00028832,\n",
      "Train: train_praguard [0024] TF1: 99.8351, Tloss: 0.00012541, VF1: 99.2886, VLoss: 0.00025412,\n",
      "Train: train_praguard [0025] TF1: 99.8986, Tloss: 0.00011030, VF1: 99.0779, VLoss: 0.00026359,\n",
      "Train: train_praguard [0026] TF1: 99.7719, Tloss: 0.00015717, VF1: 98.1323, VLoss: 0.00055456,\n",
      "Train: train_praguard [0027] TF1: 99.7592, Tloss: 0.00016262, VF1: 99.3890, VLoss: 0.00025558,\n",
      "Train: train_praguard [0028] TF1: 99.8479, Tloss: 0.00011794, VF1: 99.2359, VLoss: 0.00029456,\n",
      "Train: train_praguard [0029] TF1: 99.7972, Tloss: 0.00014233, VF1: 99.2879, VLoss: 0.00030444,\n",
      "Train: train_praguard [0030] TF1: 99.7845, Tloss: 0.00017277, VF1: 99.2864, VLoss: 0.00024118,\n",
      "Train: train_praguard [0031] TF1: 99.9113, Tloss: 0.00006998, VF1: 99.2327, VLoss: 0.00030546,\n",
      "Train: train_praguard [0032] TF1: 99.7591, Tloss: 0.00016410, VF1: 96.6060, VLoss: 0.00114829,\n",
      "Train: train_praguard [0033] TF1: 99.6830, Tloss: 0.00022060, VF1: 98.4879, VLoss: 0.00058197,\n",
      "Train: train_praguard [0034] TF1: 99.8479, Tloss: 0.00010657, VF1: 98.8378, VLoss: 0.00047737,\n",
      "Train: train_praguard [0035] TF1: 99.8859, Tloss: 0.00008521, VF1: 99.4919, VLoss: 0.00018517,\n",
      "Train: train_praguard [0036] TF1: 99.9240, Tloss: 0.00005359, VF1: 99.2879, VLoss: 0.00028272,\n",
      "Train: train_praguard [0037] TF1: 99.6579, Tloss: 0.00023868, VF1: 97.7046, VLoss: 0.00079270,\n",
      "Train: train_praguard [0038] TF1: 99.7846, Tloss: 0.00023771, VF1: 99.3909, VLoss: 0.00015701,\n",
      "Train: train_praguard [0039] TF1: 99.5310, Tloss: 0.00030007, VF1: 99.1331, VLoss: 0.00022378,\n",
      "Train: train_praguard [0040] TF1: 99.8605, Tloss: 0.00011831, VF1: 96.9727, VLoss: 0.00097913,\n",
      "Train: train_praguard [0041] TF1: 99.8352, Tloss: 0.00018792, VF1: 99.3350, VLoss: 0.00024960,\n",
      "Train: train_praguard [0042] TF1: 99.7845, Tloss: 0.00015521, VF1: 98.8366, VLoss: 0.00038987,\n",
      "Epoch    42: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_praguard [0043] TF1: 99.8606, Tloss: 0.00008420, VF1: 99.1853, VLoss: 0.00029284,\n",
      "Train: train_praguard [0044] TF1: 99.9493, Tloss: 0.00005899, VF1: 99.3371, VLoss: 0.00021585,\n",
      "Train: train_praguard [0045] TF1: 99.8986, Tloss: 0.00005737, VF1: 99.3902, VLoss: 0.00018233,\n",
      "Train: train_praguard [0046] TF1: 99.8859, Tloss: 0.00015389, VF1: 95.6607, VLoss: 0.00160887,\n",
      "Train: train_praguard [0047] TF1: 99.7338, Tloss: 0.00016350, VF1: 99.1313, VLoss: 0.00026155,\n",
      "Train: train_praguard [0048] TF1: 99.8859, Tloss: 0.00007045, VF1: 99.4396, VLoss: 0.00021425,\n",
      "Epoch    48: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_praguard [0049] TF1: 99.9113, Tloss: 0.00006985, VF1: 99.2893, VLoss: 0.00018452,\n",
      "Train: train_praguard [0050] TF1: 99.9239, Tloss: 0.00008150, VF1: 99.2312, VLoss: 0.00033461,\n",
      "Train: train_praguard [0051] TF1: 99.8986, Tloss: 0.00006646, VF1: 99.4402, VLoss: 0.00026580,\n",
      "Train: train_praguard [0052] TF1: 99.8986, Tloss: 0.00007851, VF1: 99.0389, VLoss: 0.00034345,\n",
      "Train: train_praguard [0053] TF1: 99.8479, Tloss: 0.00015799, VF1: 96.3583, VLoss: 0.00112199,\n",
      "Train: train_praguard [0054] TF1: 99.9239, Tloss: 0.00008295, VF1: 99.3865, VLoss: 0.00030159,\n",
      "Epoch    54: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_praguard [0055] TF1: 99.8606, Tloss: 0.00009137, VF1: 99.3384, VLoss: 0.00027134,\n",
      "Train: train_praguard [0056] TF1: 99.9493, Tloss: 0.00004562, VF1: 99.4903, VLoss: 0.00024299,\n",
      "Train: train_praguard [0057] TF1: 99.9747, Tloss: 0.00003746, VF1: 99.2374, VLoss: 0.00025956,\n",
      "Train: train_praguard [0058] TF1: 99.9366, Tloss: 0.00003596, VF1: 99.4396, VLoss: 0.00023163,\n",
      "Train: train_praguard [0059] TF1: 99.8986, Tloss: 0.00018270, VF1: 99.3391, VLoss: 0.00021663,\n",
      "Train: train_praguard [0060] TF1: 99.8732, Tloss: 0.00012394, VF1: 99.5910, VLoss: 0.00027222,\n",
      "Train: train_praguard [0061] TF1: 99.8607, Tloss: 0.00009327, VF1: 99.0769, VLoss: 0.00031985,\n",
      "Train: train_praguard [0062] TF1: 99.8606, Tloss: 0.00010446, VF1: 99.2872, VLoss: 0.00026906,\n",
      "Train: train_praguard [0063] TF1: 99.9493, Tloss: 0.00004929, VF1: 99.2382, VLoss: 0.00030001,\n",
      "Train: train_praguard [0064] TF1: 99.9239, Tloss: 0.00004446, VF1: 99.4402, VLoss: 0.00019694,\n",
      "Epoch    64: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_praguard [0065] TF1: 99.9112, Tloss: 0.00006939, VF1: 99.1375, VLoss: 0.00027560,\n",
      "Train: train_praguard [0066] TF1: 99.9366, Tloss: 0.00004836, VF1: 99.4396, VLoss: 0.00016581,\n",
      "Train: train_praguard [0067] TF1: 99.9366, Tloss: 0.00005350, VF1: 99.5415, VLoss: 0.00022046,\n",
      "Train: train_praguard [0068] TF1: 99.9747, Tloss: 0.00003188, VF1: 99.4402, VLoss: 0.00021359,\n",
      "Train: train_praguard [0069] TF1: 99.9620, Tloss: 0.00003238, VF1: 99.2397, VLoss: 0.00023761,\n",
      "Train: train_praguard [0070] TF1: 99.9366, Tloss: 0.00005300, VF1: 99.4413, VLoss: 0.00018754,\n",
      "Train: train_praguard [0071] TF1: 99.8986, Tloss: 0.00008066, VF1: 99.2382, VLoss: 0.00025607,\n",
      "Train: train_praguard [0072] TF1: 99.9620, Tloss: 0.00003637, VF1: 99.2893, VLoss: 0.00031291,\n",
      "Train: train_praguard [0073] TF1: 99.9366, Tloss: 0.00007607, VF1: 99.4391, VLoss: 0.00021693,\n",
      "Train: train_praguard [0074] TF1: 99.8605, Tloss: 0.00008605, VF1: 99.5411, VLoss: 0.00021928,\n",
      "Epoch    74: reducing learning rate of group 0 to 7.8643e-05.\n",
      "Train: train_praguard [0075] TF1: 99.9113, Tloss: 0.00013214, VF1: 99.3890, VLoss: 0.00026539,\n",
      "Train: train_praguard [0076] TF1: 99.9493, Tloss: 0.00004415, VF1: 99.4402, VLoss: 0.00021846,\n",
      "Train: train_praguard [0077] TF1: 99.9620, Tloss: 0.00005116, VF1: 99.6434, VLoss: 0.00017929,\n",
      "Train: train_praguard [0078] TF1: 99.8859, Tloss: 0.00009926, VF1: 99.4396, VLoss: 0.00024572,\n",
      "Train: train_praguard [0079] TF1: 99.9493, Tloss: 0.00004024, VF1: 99.4402, VLoss: 0.00022652,\n",
      "Train: train_praguard [0080] TF1: 99.9493, Tloss: 0.00003965, VF1: 99.4903, VLoss: 0.00023880,\n",
      "Epoch    80: reducing learning rate of group 0 to 6.2915e-05.\n",
      "Train: train_praguard [0081] TF1: 99.9113, Tloss: 0.00006457, VF1: 99.1384, VLoss: 0.00025693,\n",
      "Train: train_praguard [0082] TF1: 99.9620, Tloss: 0.00003579, VF1: 99.5923, VLoss: 0.00019406,\n",
      "Train: train_praguard [0083] TF1: 99.9747, Tloss: 0.00002697, VF1: 99.2886, VLoss: 0.00024909,\n",
      "Train: train_praguard [0084] TF1: 99.9747, Tloss: 0.00002953, VF1: 99.5411, VLoss: 0.00027119,\n",
      "Train: train_praguard [0085] TF1: 99.9493, Tloss: 0.00005092, VF1: 99.4903, VLoss: 0.00025652,\n",
      "Train: train_praguard [0086] TF1: 99.8859, Tloss: 0.00007228, VF1: 99.4408, VLoss: 0.00018239,\n",
      "Train: train_praguard [0087] TF1: 99.9493, Tloss: 0.00003767, VF1: 99.4413, VLoss: 0.00021081,\n",
      "Train: train_praguard [0088] TF1: 99.9620, Tloss: 0.00004021, VF1: 99.4396, VLoss: 0.00019865,\n",
      "Train: train_praguard [0089] TF1: 99.9493, Tloss: 0.00004682, VF1: 99.3890, VLoss: 0.00020541,\n",
      "Epoch    89: reducing learning rate of group 0 to 5.0332e-05.\n",
      "Train: train_praguard [0090] TF1: 99.9620, Tloss: 0.00004278, VF1: 99.4408, VLoss: 0.00014132,\n",
      "Train: train_praguard [0091] TF1: 99.9493, Tloss: 0.00003945, VF1: 99.4396, VLoss: 0.00021953,\n",
      "Train: train_praguard [0092] TF1: 99.9747, Tloss: 0.00002895, VF1: 99.4919, VLoss: 0.00016987,\n",
      "Train: train_praguard [0093] TF1: 99.9747, Tloss: 0.00002562, VF1: 99.3896, VLoss: 0.00018222,\n",
      "Train: train_praguard [0094] TF1: 99.9620, Tloss: 0.00003519, VF1: 99.5923, VLoss: 0.00016393,\n",
      "Train: train_praguard [0095] TF1: 99.9620, Tloss: 0.00003077, VF1: 99.3884, VLoss: 0.00020399,\n",
      "Train: train_praguard [0096] TF1: 99.9620, Tloss: 0.00004308, VF1: 99.5420, VLoss: 0.00013300,\n",
      "Train: train_praguard [0097] TF1: 99.9493, Tloss: 0.00003375, VF1: 99.4898, VLoss: 0.00018369,\n",
      "Train: train_praguard [0098] TF1: 99.9620, Tloss: 0.00002999, VF1: 99.3902, VLoss: 0.00016576,\n",
      "Train: train_praguard [0099] TF1: 99.9747, Tloss: 0.00003006, VF1: 99.5411, VLoss: 0.00020471,\n",
      "Epoch    99: reducing learning rate of group 0 to 4.0265e-05.\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_096.pth Fragment=00 score=0.9942067283260495\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_090.pth Fragment=00 score=0.9942043721403152\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_038.pth Fragment=00 score=0.994615462765417\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_094.pth Fragment=00 score=0.9945990013247733\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_098.pth Fragment=00 score=0.9935995123437976\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_066.pth Fragment=00 score=0.9939912414706181\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_018.pth Fragment=00 score=0.9939049167005282\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_092.pth Fragment=00 score=0.994911459393446\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_077.pth Fragment=00 score=0.9932858596134282\n",
      "Evaluate: ModelPath=./traces/obfusWS02/model_train_praguard_093.pth Fragment=00 score=0.9940022364542035\n"
     ]
    }
   ],
   "source": [
    "currentTag = 'praguard'\n",
    "\n",
    "message  = '######## '\n",
    "message += currentTag\n",
    "\n",
    "with open(outputlogFilePath, 'a') as writer:\n",
    "    writer.write(message + '\\n')\n",
    "print(message)\n",
    "\n",
    "#\n",
    "dataset_df = pd.read_parquet('/ws/mnt/local/data/output/obfus/praguard_meta.parquet')\n",
    "\n",
    "#\n",
    "trainLoader, validLoader, testLoader = getDataloaders(dataset_df, trainPercentage=trainPercentageParam, \n",
    "                                                                  validPercentage=validPercentageParam)\n",
    "\n",
    "#\n",
    "models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "#\n",
    "evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "#\n",
    "evalDataset(ws, evalresult_df, probaUpperBorn=0.8,  probaLowerBorn=0.2)\n",
    "\n",
    "#\n",
    "outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "currentResults.to_pickle(outputPath)\n",
    "\n",
    "#\n",
    "message = '########'\n",
    "with open(outputlogFilePath, 'a') as writer:\n",
    "    writer.write(message + '\\n')\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
