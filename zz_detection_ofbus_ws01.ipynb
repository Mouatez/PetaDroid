{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, test_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "    #test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'obfusWS01'\n",
    "epochNum         = 100\n",
    "device           = torch.device('cuda:3')\n",
    "ensembleSize     = 20\n",
    "\n",
    "trainPercentageParam = 0.8\n",
    "validPercentageParam = 0.2\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obfus_df = pd.read_parquet('/ws/mnt/local/data/output/obfus/drebinObfus_meta.parquet')\n",
    "obfusTypes = list(obfus_df.obfustype.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## noObfus\n",
      "8690\n",
      "0    4408\n",
      "1    4282\n",
      "Name: label, dtype: int64\n",
      "2173\n",
      "0    1091\n",
      "1    1082\n",
      "Name: label, dtype: int64\n",
      "90602\n",
      "1    49535\n",
      "0    41067\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_noObfus [0000] TF1: 94.9172, Tloss: 0.00422192, VF1: 85.1735, VLoss: 0.00652380,\n",
      "Train: train_noObfus [0001] TF1: 97.4988, Tloss: 0.00210172, VF1: 83.4749, VLoss: 0.00719986,\n",
      "Train: train_noObfus [0002] TF1: 98.2247, Tloss: 0.00149190, VF1: 97.3636, VLoss: 0.00119003,\n",
      "Train: train_noObfus [0003] TF1: 98.7493, Tloss: 0.00118663, VF1: 96.3661, VLoss: 0.00169184,\n",
      "Train: train_noObfus [0004] TF1: 98.7614, Tloss: 0.00111458, VF1: 98.3333, VLoss: 0.00074463,\n",
      "Train: train_noObfus [0005] TF1: 99.0182, Tloss: 0.00092047, VF1: 98.1430, VLoss: 0.00093340,\n",
      "Train: train_noObfus [0006] TF1: 99.0997, Tloss: 0.00086855, VF1: 98.1499, VLoss: 0.00088025,\n",
      "Train: train_noObfus [0007] TF1: 99.0062, Tloss: 0.00088945, VF1: 94.4093, VLoss: 0.00233288,\n",
      "Train: train_noObfus [0008] TF1: 99.2054, Tloss: 0.00080079, VF1: 97.9499, VLoss: 0.00101707,\n",
      "Train: train_noObfus [0009] TF1: 99.0656, Tloss: 0.00086268, VF1: 97.4118, VLoss: 0.00126955,\n",
      "Train: train_noObfus [0010] TF1: 99.4162, Tloss: 0.00056192, VF1: 94.5662, VLoss: 0.00265915,\n",
      "Train: train_noObfus [0011] TF1: 99.4171, Tloss: 0.00058454, VF1: 87.4242, VLoss: 0.00672393,\n",
      "Train: train_noObfus [0012] TF1: 99.1936, Tloss: 0.00062717, VF1: 49.1986, VLoss: 0.02111113,\n",
      "Train: train_noObfus [0013] TF1: 98.9155, Tloss: 0.00098337, VF1: 98.4317, VLoss: 0.00082233,\n",
      "Train: train_noObfus [0014] TF1: 99.4632, Tloss: 0.00052093, VF1: 94.3396, VLoss: 0.00280457,\n",
      "Train: train_noObfus [0015] TF1: 99.3580, Tloss: 0.00061247, VF1: 96.6383, VLoss: 0.00226311,\n",
      "Train: train_noObfus [0016] TF1: 99.5215, Tloss: 0.00049439, VF1: 89.9492, VLoss: 0.00448450,\n",
      "Train: train_noObfus [0017] TF1: 99.1115, Tloss: 0.00082988, VF1: 95.0330, VLoss: 0.00304205,\n",
      "Train: train_noObfus [0018] TF1: 99.4398, Tloss: 0.00051100, VF1: 95.9929, VLoss: 0.00227048,\n",
      "Train: train_noObfus [0019] TF1: 99.4978, Tloss: 0.00045864, VF1: 97.7486, VLoss: 0.00127608,\n",
      "Train: train_noObfus [0020] TF1: 99.5799, Tloss: 0.00042593, VF1: 98.2193, VLoss: 0.00116818,\n",
      "Train: train_noObfus [0021] TF1: 99.5327, Tloss: 0.00044696, VF1: 98.7025, VLoss: 0.00082316,\n",
      "Train: train_noObfus [0022] TF1: 99.3229, Tloss: 0.00057634, VF1: 98.4673, VLoss: 0.00094697,\n",
      "Train: train_noObfus [0023] TF1: 99.3461, Tloss: 0.00061707, VF1: 97.6919, VLoss: 0.00121958,\n",
      "Train: train_noObfus [0024] TF1: 99.5677, Tloss: 0.00041313, VF1: 97.8863, VLoss: 0.00135930,\n",
      "Train: train_noObfus [0025] TF1: 99.7198, Tloss: 0.00034583, VF1: 98.5668, VLoss: 0.00073722,\n",
      "Train: train_noObfus [0026] TF1: 99.5913, Tloss: 0.00042415, VF1: 95.9002, VLoss: 0.00195147,\n",
      "Train: train_noObfus [0027] TF1: 99.4395, Tloss: 0.00053875, VF1: 97.7528, VLoss: 0.00125502,\n",
      "Train: train_noObfus [0028] TF1: 99.5914, Tloss: 0.00045519, VF1: 98.4346, VLoss: 0.00086429,\n",
      "Train: train_noObfus [0029] TF1: 99.3112, Tloss: 0.00057970, VF1: 96.2071, VLoss: 0.00204653,\n",
      "Train: train_noObfus [0030] TF1: 99.7198, Tloss: 0.00029378, VF1: 96.6443, VLoss: 0.00192151,\n",
      "Train: train_noObfus [0031] TF1: 99.5793, Tloss: 0.00051745, VF1: 98.0356, VLoss: 0.00098243,\n",
      "Train: train_noObfus [0032] TF1: 99.5209, Tloss: 0.00048861, VF1: 98.1447, VLoss: 0.00100505,\n",
      "Train: train_noObfus [0033] TF1: 99.6730, Tloss: 0.00036207, VF1: 97.8627, VLoss: 0.00122047,\n",
      "Train: train_noObfus [0034] TF1: 99.5564, Tloss: 0.00049533, VF1: 98.5281, VLoss: 0.00095069,\n",
      "Train: train_noObfus [0035] TF1: 99.6496, Tloss: 0.00041624, VF1: 98.2718, VLoss: 0.00102476,\n",
      "Train: train_noObfus [0036] TF1: 99.6265, Tloss: 0.00039208, VF1: 98.4644, VLoss: 0.00087341,\n",
      "Epoch    36: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_noObfus [0037] TF1: 99.7431, Tloss: 0.00027610, VF1: 98.9824, VLoss: 0.00070945,\n",
      "Train: train_noObfus [0038] TF1: 99.6963, Tloss: 0.00024085, VF1: 98.1273, VLoss: 0.00110621,\n",
      "Train: train_noObfus [0039] TF1: 99.7781, Tloss: 0.00024778, VF1: 98.2585, VLoss: 0.00086393,\n",
      "Train: train_noObfus [0040] TF1: 99.7666, Tloss: 0.00027328, VF1: 98.0319, VLoss: 0.00130554,\n",
      "Train: train_noObfus [0041] TF1: 99.7082, Tloss: 0.00028877, VF1: 98.4332, VLoss: 0.00093764,\n",
      "Train: train_noObfus [0042] TF1: 99.5328, Tloss: 0.00047871, VF1: 98.5116, VLoss: 0.00101252,\n",
      "Train: train_noObfus [0043] TF1: 99.5794, Tloss: 0.00036752, VF1: 98.2085, VLoss: 0.00101071,\n",
      "Train: train_noObfus [0044] TF1: 99.8015, Tloss: 0.00024062, VF1: 98.0538, VLoss: 0.00130461,\n",
      "Train: train_noObfus [0045] TF1: 99.8130, Tloss: 0.00019924, VF1: 98.7985, VLoss: 0.00094279,\n",
      "Train: train_noObfus [0046] TF1: 99.6845, Tloss: 0.00026768, VF1: 98.1115, VLoss: 0.00097275,\n",
      "Train: train_noObfus [0047] TF1: 99.7665, Tloss: 0.00028976, VF1: 98.5668, VLoss: 0.00097034,\n",
      "Train: train_noObfus [0048] TF1: 99.7548, Tloss: 0.00025835, VF1: 96.2930, VLoss: 0.00271720,\n",
      "Train: train_noObfus [0049] TF1: 99.5797, Tloss: 0.00038035, VF1: 98.5199, VLoss: 0.00083582,\n",
      "Train: train_noObfus [0050] TF1: 99.8482, Tloss: 0.00018243, VF1: 98.5734, VLoss: 0.00093363,\n",
      "Train: train_noObfus [0051] TF1: 99.7547, Tloss: 0.00022441, VF1: 98.6175, VLoss: 0.00084838,\n",
      "Train: train_noObfus [0052] TF1: 99.5562, Tloss: 0.00036433, VF1: 98.7097, VLoss: 0.00074675,\n",
      "Train: train_noObfus [0053] TF1: 99.6963, Tloss: 0.00030295, VF1: 98.5144, VLoss: 0.00085342,\n",
      "Train: train_noObfus [0054] TF1: 99.8482, Tloss: 0.00017490, VF1: 98.6630, VLoss: 0.00101395,\n",
      "Train: train_noObfus [0055] TF1: 99.7664, Tloss: 0.00017909, VF1: 98.7506, VLoss: 0.00085773,\n",
      "Train: train_noObfus [0056] TF1: 99.7897, Tloss: 0.00016496, VF1: 94.8526, VLoss: 0.00304956,\n",
      "Train: train_noObfus [0057] TF1: 99.5795, Tloss: 0.00037724, VF1: 98.0447, VLoss: 0.00128103,\n",
      "Train: train_noObfus [0058] TF1: 99.8132, Tloss: 0.00019314, VF1: 98.6137, VLoss: 0.00086828,\n",
      "Train: train_noObfus [0059] TF1: 99.7432, Tloss: 0.00023416, VF1: 97.1971, VLoss: 0.00189711,\n",
      "Train: train_noObfus [0060] TF1: 99.7431, Tloss: 0.00027049, VF1: 98.4142, VLoss: 0.00133656,\n",
      "Train: train_noObfus [0061] TF1: 99.6380, Tloss: 0.00033965, VF1: 97.5106, VLoss: 0.00125531,\n",
      "Train: train_noObfus [0062] TF1: 99.5210, Tloss: 0.00046592, VF1: 98.8421, VLoss: 0.00068532,\n",
      "Epoch    62: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_noObfus [0063] TF1: 99.7200, Tloss: 0.00026670, VF1: 98.8388, VLoss: 0.00078775,\n",
      "Train: train_noObfus [0064] TF1: 99.8949, Tloss: 0.00011370, VF1: 99.1208, VLoss: 0.00068966,\n",
      "Train: train_noObfus [0065] TF1: 99.7082, Tloss: 0.00026891, VF1: 98.1702, VLoss: 0.00109695,\n",
      "Train: train_noObfus [0066] TF1: 99.8366, Tloss: 0.00016108, VF1: 98.5171, VLoss: 0.00088086,\n",
      "Train: train_noObfus [0067] TF1: 99.8716, Tloss: 0.00017543, VF1: 98.6111, VLoss: 0.00088539,\n",
      "Train: train_noObfus [0068] TF1: 99.7899, Tloss: 0.00015633, VF1: 98.5534, VLoss: 0.00112104,\n",
      "Train: train_noObfus [0069] TF1: 99.8715, Tloss: 0.00012068, VF1: 98.7494, VLoss: 0.00080525,\n",
      "Train: train_noObfus [0070] TF1: 99.8715, Tloss: 0.00013081, VF1: 97.6829, VLoss: 0.00123012,\n",
      "Epoch    70: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_noObfus [0071] TF1: 99.8248, Tloss: 0.00018885, VF1: 98.6642, VLoss: 0.00077625,\n",
      "Train: train_noObfus [0072] TF1: 99.8832, Tloss: 0.00012169, VF1: 98.8421, VLoss: 0.00074540,\n",
      "Train: train_noObfus [0073] TF1: 99.8716, Tloss: 0.00012427, VF1: 98.8879, VLoss: 0.00080260,\n",
      "Train: train_noObfus [0074] TF1: 99.9183, Tloss: 0.00009671, VF1: 98.4800, VLoss: 0.00093340,\n",
      "Train: train_noObfus [0075] TF1: 99.8832, Tloss: 0.00012487, VF1: 98.7552, VLoss: 0.00090376,\n",
      "Train: train_noObfus [0076] TF1: 99.8366, Tloss: 0.00014932, VF1: 98.6654, VLoss: 0.00103089,\n",
      "Train: train_noObfus [0077] TF1: 99.9183, Tloss: 0.00008727, VF1: 98.5226, VLoss: 0.00095800,\n",
      "Train: train_noObfus [0078] TF1: 99.9533, Tloss: 0.00009090, VF1: 95.2657, VLoss: 0.00326580,\n",
      "Train: train_noObfus [0079] TF1: 99.8249, Tloss: 0.00015596, VF1: 98.2585, VLoss: 0.00122788,\n",
      "Train: train_noObfus [0080] TF1: 99.7313, Tloss: 0.00022609, VF1: 98.3471, VLoss: 0.00090972,\n",
      "Train: train_noObfus [0081] TF1: 99.8481, Tloss: 0.00014359, VF1: 98.6124, VLoss: 0.00101068,\n",
      "Train: train_noObfus [0082] TF1: 99.8833, Tloss: 0.00013828, VF1: 98.3020, VLoss: 0.00096578,\n",
      "Train: train_noObfus [0083] TF1: 99.8016, Tloss: 0.00016273, VF1: 89.0705, VLoss: 0.00483565,\n",
      "Epoch    83: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_noObfus [0084] TF1: 99.8949, Tloss: 0.00010871, VF1: 98.7506, VLoss: 0.00095688,\n",
      "Train: train_noObfus [0085] TF1: 99.9300, Tloss: 0.00009972, VF1: 98.4758, VLoss: 0.00114590,\n",
      "Train: train_noObfus [0086] TF1: 99.8949, Tloss: 0.00010803, VF1: 98.2783, VLoss: 0.00127872,\n",
      "Train: train_noObfus [0087] TF1: 99.9066, Tloss: 0.00009942, VF1: 98.8421, VLoss: 0.00084922,\n",
      "Train: train_noObfus [0088] TF1: 99.8247, Tloss: 0.00019326, VF1: 98.7963, VLoss: 0.00092696,\n",
      "Train: train_noObfus [0089] TF1: 99.8949, Tloss: 0.00009901, VF1: 98.4389, VLoss: 0.00089402,\n",
      "Epoch    89: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_noObfus [0090] TF1: 99.8599, Tloss: 0.00011166, VF1: 98.7085, VLoss: 0.00090938,\n",
      "Train: train_noObfus [0091] TF1: 99.8949, Tloss: 0.00009808, VF1: 98.7985, VLoss: 0.00093277,\n",
      "Train: train_noObfus [0092] TF1: 99.9183, Tloss: 0.00009298, VF1: 98.6188, VLoss: 0.00088752,\n",
      "Train: train_noObfus [0093] TF1: 99.9416, Tloss: 0.00006886, VF1: 98.5734, VLoss: 0.00110570,\n",
      "Train: train_noObfus [0094] TF1: 99.9183, Tloss: 0.00007258, VF1: 98.7985, VLoss: 0.00088056,\n",
      "Train: train_noObfus [0095] TF1: 99.8132, Tloss: 0.00018098, VF1: 98.6989, VLoss: 0.00102882,\n",
      "Train: train_noObfus [0096] TF1: 99.8599, Tloss: 0.00014018, VF1: 98.6605, VLoss: 0.00103875,\n",
      "Train: train_noObfus [0097] TF1: 99.8716, Tloss: 0.00010206, VF1: 98.3774, VLoss: 0.00093406,\n",
      "Train: train_noObfus [0098] TF1: 99.8949, Tloss: 0.00011191, VF1: 98.5240, VLoss: 0.00090893,\n",
      "Train: train_noObfus [0099] TF1: 99.9299, Tloss: 0.00008094, VF1: 98.7974, VLoss: 0.00091417,\n",
      "Epoch    99: reducing learning rate of group 0 to 7.8643e-05.\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_062.pth Fragment=00 score=0.9972419506379884\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_064.pth Fragment=00 score=0.9976771900058576\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_037.pth Fragment=00 score=0.9953743534722713\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_025.pth Fragment=00 score=0.9958611778482163\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_004.pth Fragment=00 score=0.9937905786695254\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_072.pth Fragment=00 score=0.9973531075729412\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_052.pth Fragment=00 score=0.9974579861601469\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_071.pth Fragment=00 score=0.9973865046770467\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_063.pth Fragment=00 score=0.9960323886639677\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_073.pth Fragment=00 score=0.99695839775265\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_069.pth Fragment=00 score=0.997344855784277\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_013.pth Fragment=00 score=0.9955763831377381\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_021.pth Fragment=00 score=0.994661921708185\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_049.pth Fragment=00 score=0.9962605109961191\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_051.pth Fragment=00 score=0.9962786288411307\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_087.pth Fragment=00 score=0.9968773685008336\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_053.pth Fragment=00 score=0.9956292114368966\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_055.pth Fragment=00 score=0.9972431204241352\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_039.pth Fragment=00 score=0.9956396958864105\n",
      "Evaluate: ModelPath=./traces/obfusWS01/model_train_noObfus_028.pth Fragment=00 score=0.9959273372447026\n",
      "Extend: f1score=99.8010, vcoverage=99.6126, vf1score=99.8957, vexentdSize=90251, ecoverage=0.3874, ef1score=66.4286, erestSize=351\n",
      "########\n",
      "######## remDebugInfo\n",
      "8053\n",
      "1    4403\n",
      "0    3650\n",
      "Name: label, dtype: int64\n",
      "2014\n",
      "1    1101\n",
      "0     913\n",
      "Name: label, dtype: int64\n",
      "91398\n",
      "1    49395\n",
      "0    42003\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_remDebugInfo [0000] TF1: 95.3961, Tloss: 0.00420590, VF1: 71.5215, VLoss: 0.02439119,\n",
      "Train: train_remDebugInfo [0001] TF1: 98.1826, Tloss: 0.00199599, VF1: 97.5964, VLoss: 0.00133628,\n",
      "Train: train_remDebugInfo [0002] TF1: 98.5010, Tloss: 0.00150275, VF1: 92.0371, VLoss: 0.00377591,\n",
      "Train: train_remDebugInfo [0003] TF1: 98.3853, Tloss: 0.00155429, VF1: 87.7727, VLoss: 0.00421989,\n",
      "Train: train_remDebugInfo [0004] TF1: 98.7514, Tloss: 0.00122219, VF1: 97.4220, VLoss: 0.00122951,\n",
      "Train: train_remDebugInfo [0005] TF1: 99.1935, Tloss: 0.00086647, VF1: 98.0892, VLoss: 0.00090695,\n",
      "Train: train_remDebugInfo [0006] TF1: 99.1030, Tloss: 0.00083889, VF1: 97.2925, VLoss: 0.00140537,\n",
      "Train: train_remDebugInfo [0007] TF1: 99.0921, Tloss: 0.00095576, VF1: 86.2442, VLoss: 0.00536148,\n",
      "Train: train_remDebugInfo [0008] TF1: 99.0578, Tloss: 0.00093273, VF1: 97.8300, VLoss: 0.00102100,\n",
      "Train: train_remDebugInfo [0009] TF1: 99.3756, Tloss: 0.00060819, VF1: 97.5347, VLoss: 0.00145935,\n",
      "Train: train_remDebugInfo [0010] TF1: 99.2162, Tloss: 0.00078980, VF1: 97.8056, VLoss: 0.00119947,\n"
     ]
    }
   ],
   "source": [
    "for obtype in obfusTypes:\n",
    "    currentTag = obtype\n",
    "\n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    #\n",
    "    dataset_df = obfus_df.loc[obfus_df.obfustype == obtype]\n",
    "    test_df    = obfus_df.loc[obfus_df.obfustype != obtype]\n",
    "\n",
    "    #\n",
    "    trainLoader, validLoader, testLoader = getDataloaders(dataset_df, test_df, trainPercentage=trainPercentageParam, \n",
    "                                                                               validPercentage=validPercentageParam)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "    #\n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "    #\n",
    "    evalDataset(ws, evalresult_df, probaUpperBorn=0.8,  probaLowerBorn=0.2)\n",
    "\n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    #\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
