{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):int(validPercentage * len(dataset_df))]]\n",
    "    test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'detectionWS02'\n",
    "epochNum         = 100\n",
    "device           = torch.device('cuda:1')\n",
    "ensembleSize     = 20\n",
    "trainPercentageParam = 0.4\n",
    "validPercentageParam = 0.5\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## drebin\n",
      "6436\n",
      "0    4325\n",
      "1    2111\n",
      "Name: label, dtype: int64\n",
      "1610\n",
      "0    1106\n",
      "1     504\n",
      "Name: label, dtype: int64\n",
      "8046\n",
      "0    5297\n",
      "1    2749\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_drebin [0000] TF1: 92.9788, Tloss: 0.00433009, VF1: 96.3340, VLoss: 0.00119627,\n",
      "Train: train_drebin [0001] TF1: 96.7681, Tloss: 0.00217893, VF1: 93.4823, VLoss: 0.00234574,\n",
      "Train: train_drebin [0002] TF1: 98.0336, Tloss: 0.00142484, VF1: 94.1176, VLoss: 0.00212234,\n",
      "Train: train_drebin [0003] TF1: 97.8158, Tloss: 0.00125441, VF1: 97.4975, VLoss: 0.00098092,\n",
      "Train: train_drebin [0004] TF1: 98.9578, Tloss: 0.00081581, VF1: 96.2751, VLoss: 0.00145863,\n",
      "Train: train_drebin [0005] TF1: 98.1499, Tloss: 0.00117286, VF1: 97.2919, VLoss: 0.00081207,\n",
      "Train: train_drebin [0006] TF1: 99.1234, Tloss: 0.00060291, VF1: 97.2710, VLoss: 0.00074253,\n",
      "Train: train_drebin [0007] TF1: 98.9569, Tloss: 0.00063585, VF1: 95.9847, VLoss: 0.00159446,\n",
      "Train: train_drebin [0008] TF1: 98.7918, Tloss: 0.00101746, VF1: 97.2000, VLoss: 0.00089121,\n",
      "Train: train_drebin [0009] TF1: 98.7672, Tloss: 0.00085689, VF1: 97.2919, VLoss: 0.00094348,\n",
      "Train: train_drebin [0010] TF1: 98.8870, Tloss: 0.00066250, VF1: 96.0539, VLoss: 0.00131096,\n",
      "Train: train_drebin [0011] TF1: 98.9818, Tloss: 0.00086292, VF1: 94.1839, VLoss: 0.00205860,\n",
      "Train: train_drebin [0012] TF1: 98.2439, Tloss: 0.00097065, VF1: 93.8606, VLoss: 0.00170021,\n",
      "Epoch    12: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_drebin [0013] TF1: 99.3374, Tloss: 0.00043600, VF1: 96.7546, VLoss: 0.00111384,\n",
      "Train: train_drebin [0014] TF1: 99.1951, Tloss: 0.00063634, VF1: 97.4000, VLoss: 0.00078360,\n",
      "Train: train_drebin [0015] TF1: 99.2670, Tloss: 0.00050172, VF1: 94.7150, VLoss: 0.00173842,\n",
      "Train: train_drebin [0016] TF1: 98.8615, Tloss: 0.00071867, VF1: 94.2615, VLoss: 0.00193597,\n",
      "Train: train_drebin [0017] TF1: 99.4551, Tloss: 0.00038390, VF1: 97.6190, VLoss: 0.00087531,\n",
      "Train: train_drebin [0018] TF1: 99.6446, Tloss: 0.00044240, VF1: 97.4359, VLoss: 0.00068472,\n",
      "Train: train_drebin [0019] TF1: 98.8402, Tloss: 0.00083719, VF1: 96.3462, VLoss: 0.00105551,\n",
      "Train: train_drebin [0020] TF1: 99.0310, Tloss: 0.00051412, VF1: 97.7023, VLoss: 0.00069124,\n",
      "Train: train_drebin [0021] TF1: 99.5501, Tloss: 0.00029643, VF1: 92.2232, VLoss: 0.00322201,\n",
      "Train: train_drebin [0022] TF1: 99.6684, Tloss: 0.00028693, VF1: 97.7340, VLoss: 0.00076687,\n",
      "Train: train_drebin [0023] TF1: 99.6212, Tloss: 0.00021638, VF1: 97.8261, VLoss: 0.00078023,\n",
      "Train: train_drebin [0024] TF1: 99.6920, Tloss: 0.00022271, VF1: 96.7118, VLoss: 0.00101880,\n",
      "Train: train_drebin [0025] TF1: 99.5027, Tloss: 0.00028093, VF1: 96.2283, VLoss: 0.00134936,\n",
      "Train: train_drebin [0026] TF1: 99.4318, Tloss: 0.00033033, VF1: 97.5369, VLoss: 0.00090601,\n",
      "Train: train_drebin [0027] TF1: 99.5027, Tloss: 0.00032623, VF1: 95.7184, VLoss: 0.00167120,\n",
      "Train: train_drebin [0028] TF1: 99.4551, Tloss: 0.00048469, VF1: 97.1762, VLoss: 0.00104345,\n",
      "Train: train_drebin [0029] TF1: 99.0769, Tloss: 0.00050261, VF1: 97.0000, VLoss: 0.00108925,\n",
      "Epoch    29: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_drebin [0030] TF1: 99.5501, Tloss: 0.00032520, VF1: 97.9021, VLoss: 0.00072357,\n",
      "Train: train_drebin [0031] TF1: 99.6922, Tloss: 0.00021130, VF1: 97.5124, VLoss: 0.00085880,\n",
      "Train: train_drebin [0032] TF1: 99.5025, Tloss: 0.00026351, VF1: 96.0690, VLoss: 0.00133223,\n",
      "Train: train_drebin [0033] TF1: 99.6922, Tloss: 0.00024636, VF1: 97.2755, VLoss: 0.00093238,\n",
      "Train: train_drebin [0034] TF1: 99.6214, Tloss: 0.00027413, VF1: 97.1660, VLoss: 0.00107342,\n",
      "Train: train_drebin [0035] TF1: 99.6687, Tloss: 0.00020615, VF1: 97.0930, VLoss: 0.00127458,\n",
      "Train: train_drebin [0036] TF1: 99.8104, Tloss: 0.00018484, VF1: 98.1132, VLoss: 0.00085066,\n",
      "Train: train_drebin [0037] TF1: 99.5743, Tloss: 0.00023802, VF1: 97.2549, VLoss: 0.00104680,\n",
      "Train: train_drebin [0038] TF1: 99.7158, Tloss: 0.00021943, VF1: 97.5466, VLoss: 0.00113088,\n",
      "Train: train_drebin [0039] TF1: 99.3374, Tloss: 0.00044821, VF1: 90.1280, VLoss: 0.00336372,\n",
      "Train: train_drebin [0040] TF1: 98.9808, Tloss: 0.00045837, VF1: 97.3710, VLoss: 0.00085954,\n",
      "Train: train_drebin [0041] TF1: 99.7395, Tloss: 0.00020748, VF1: 97.2973, VLoss: 0.00098622,\n",
      "Train: train_drebin [0042] TF1: 99.7633, Tloss: 0.00026694, VF1: 97.8474, VLoss: 0.00091867,\n",
      "Epoch    42: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_drebin [0043] TF1: 99.7159, Tloss: 0.00024324, VF1: 98.1244, VLoss: 0.00074927,\n",
      "Train: train_drebin [0044] TF1: 99.7394, Tloss: 0.00016933, VF1: 97.8000, VLoss: 0.00087059,\n",
      "Train: train_drebin [0045] TF1: 99.8108, Tloss: 0.00013872, VF1: 97.9021, VLoss: 0.00094372,\n",
      "Train: train_drebin [0046] TF1: 99.8105, Tloss: 0.00013948, VF1: 98.0198, VLoss: 0.00077356,\n",
      "Train: train_drebin [0047] TF1: 99.7398, Tloss: 0.00027824, VF1: 97.9021, VLoss: 0.00081137,\n",
      "Train: train_drebin [0048] TF1: 99.4792, Tloss: 0.00030603, VF1: 97.6190, VLoss: 0.00091149,\n",
      "Train: train_drebin [0049] TF1: 99.8104, Tloss: 0.00015275, VF1: 97.7429, VLoss: 0.00086803,\n",
      "Train: train_drebin [0050] TF1: 99.8105, Tloss: 0.00013358, VF1: 97.5562, VLoss: 0.00092937,\n",
      "Train: train_drebin [0051] TF1: 99.8343, Tloss: 0.00026905, VF1: 97.3346, VLoss: 0.00110813,\n",
      "Train: train_drebin [0052] TF1: 99.5027, Tloss: 0.00053497, VF1: 97.4155, VLoss: 0.00085737,\n",
      "Train: train_drebin [0053] TF1: 99.7631, Tloss: 0.00023964, VF1: 97.7295, VLoss: 0.00088062,\n",
      "Train: train_drebin [0054] TF1: 99.6922, Tloss: 0.00022580, VF1: 98.0198, VLoss: 0.00083130,\n",
      "Train: train_drebin [0055] TF1: 99.8342, Tloss: 0.00013444, VF1: 97.5514, VLoss: 0.00086873,\n",
      "Train: train_drebin [0056] TF1: 99.8579, Tloss: 0.00011683, VF1: 97.9104, VLoss: 0.00086267,\n",
      "Train: train_drebin [0057] TF1: 99.8816, Tloss: 0.00013076, VF1: 98.5134, VLoss: 0.00072225,\n",
      "Train: train_drebin [0058] TF1: 99.8343, Tloss: 0.00010834, VF1: 97.5466, VLoss: 0.00090921,\n",
      "Train: train_drebin [0059] TF1: 99.7870, Tloss: 0.00017970, VF1: 97.8304, VLoss: 0.00070456,\n",
      "Train: train_drebin [0060] TF1: 99.7398, Tloss: 0.00031838, VF1: 98.3118, VLoss: 0.00080249,\n",
      "Train: train_drebin [0061] TF1: 99.6917, Tloss: 0.00021376, VF1: 95.5624, VLoss: 0.00186874,\n",
      "Train: train_drebin [0062] TF1: 99.5507, Tloss: 0.00023789, VF1: 98.0040, VLoss: 0.00089023,\n",
      "Train: train_drebin [0063] TF1: 99.7870, Tloss: 0.00017554, VF1: 98.0080, VLoss: 0.00086098,\n",
      "Train: train_drebin [0064] TF1: 99.8578, Tloss: 0.00015397, VF1: 97.5514, VLoss: 0.00112110,\n",
      "Epoch    64: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_drebin [0065] TF1: 99.8343, Tloss: 0.00016255, VF1: 96.5251, VLoss: 0.00161908,\n",
      "Train: train_drebin [0066] TF1: 99.6923, Tloss: 0.00021196, VF1: 96.7118, VLoss: 0.00172038,\n",
      "Train: train_drebin [0067] TF1: 99.3374, Tloss: 0.00036659, VF1: 97.5369, VLoss: 0.00085040,\n",
      "Train: train_drebin [0068] TF1: 99.8106, Tloss: 0.00013031, VF1: 97.6143, VLoss: 0.00090161,\n",
      "Train: train_drebin [0069] TF1: 99.9053, Tloss: 0.00007265, VF1: 97.5466, VLoss: 0.00086324,\n",
      "Train: train_drebin [0070] TF1: 99.9053, Tloss: 0.00017583, VF1: 97.6977, VLoss: 0.00115938,\n",
      "Train: train_drebin [0071] TF1: 99.4559, Tloss: 0.00028392, VF1: 93.5010, VLoss: 0.00273544,\n",
      "Train: train_drebin [0072] TF1: 99.7870, Tloss: 0.00013216, VF1: 97.9392, VLoss: 0.00082281,\n",
      "Train: train_drebin [0073] TF1: 99.8343, Tloss: 0.00009074, VF1: 97.1817, VLoss: 0.00105561,\n",
      "Train: train_drebin [0074] TF1: 99.7630, Tloss: 0.00015646, VF1: 97.8389, VLoss: 0.00082877,\n",
      "Train: train_drebin [0075] TF1: 99.8817, Tloss: 0.00009651, VF1: 96.8932, VLoss: 0.00141723,\n",
      "Epoch    75: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_drebin [0076] TF1: 99.8580, Tloss: 0.00031973, VF1: 97.4510, VLoss: 0.00110661,\n",
      "Train: train_drebin [0077] TF1: 99.6679, Tloss: 0.00023227, VF1: 95.1475, VLoss: 0.00236594,\n",
      "Train: train_drebin [0078] TF1: 99.8106, Tloss: 0.00013029, VF1: 98.0198, VLoss: 0.00085757,\n",
      "Train: train_drebin [0079] TF1: 99.8341, Tloss: 0.00147320, VF1: 97.5466, VLoss: 0.00101870,\n",
      "Train: train_drebin [0080] TF1: 99.8817, Tloss: 0.00015868, VF1: 96.7992, VLoss: 0.00151231,\n",
      "Train: train_drebin [0081] TF1: 99.9053, Tloss: 0.00008573, VF1: 97.9104, VLoss: 0.00080965,\n",
      "Epoch    81: reducing learning rate of group 0 to 7.8643e-05.\n",
      "Train: train_drebin [0082] TF1: 99.8579, Tloss: 0.00008683, VF1: 97.6285, VLoss: 0.00104983,\n",
      "Train: train_drebin [0083] TF1: 99.9053, Tloss: 0.00009385, VF1: 97.7340, VLoss: 0.00097038,\n",
      "Train: train_drebin [0084] TF1: 99.8342, Tloss: 0.00011356, VF1: 98.2213, VLoss: 0.00077954,\n",
      "Train: train_drebin [0085] TF1: 99.8580, Tloss: 0.00009628, VF1: 97.9269, VLoss: 0.00081878,\n",
      "Train: train_drebin [0086] TF1: 99.7871, Tloss: 0.00014434, VF1: 97.9269, VLoss: 0.00089689,\n",
      "Train: train_drebin [0087] TF1: 99.8342, Tloss: 0.00009453, VF1: 97.6378, VLoss: 0.00099098,\n",
      "Epoch    87: reducing learning rate of group 0 to 6.2915e-05.\n",
      "Train: train_drebin [0088] TF1: 99.8342, Tloss: 0.00012677, VF1: 98.2036, VLoss: 0.00085127,\n",
      "Train: train_drebin [0089] TF1: 99.8580, Tloss: 0.00009222, VF1: 97.2710, VLoss: 0.00114955,\n",
      "Train: train_drebin [0090] TF1: 99.8105, Tloss: 0.00016967, VF1: 97.7384, VLoss: 0.00089613,\n",
      "Train: train_drebin [0091] TF1: 99.8107, Tloss: 0.00021554, VF1: 96.9874, VLoss: 0.00116686,\n",
      "Train: train_drebin [0092] TF1: 99.6441, Tloss: 0.00028863, VF1: 97.9146, VLoss: 0.00084827,\n",
      "Train: train_drebin [0093] TF1: 99.8343, Tloss: 0.00011371, VF1: 97.4460, VLoss: 0.00095213,\n",
      "Epoch    93: reducing learning rate of group 0 to 5.0332e-05.\n",
      "Train: train_drebin [0094] TF1: 99.7868, Tloss: 0.00013580, VF1: 97.6378, VLoss: 0.00090344,\n",
      "Train: train_drebin [0095] TF1: 99.8580, Tloss: 0.00009034, VF1: 97.7295, VLoss: 0.00072526,\n",
      "Train: train_drebin [0096] TF1: 99.8344, Tloss: 0.00019953, VF1: 97.6471, VLoss: 0.00100076,\n",
      "Train: train_drebin [0097] TF1: 99.9053, Tloss: 0.00011797, VF1: 98.1169, VLoss: 0.00073322,\n",
      "Train: train_drebin [0098] TF1: 99.8817, Tloss: 0.00025697, VF1: 97.8389, VLoss: 0.00078501,\n",
      "Train: train_drebin [0099] TF1: 99.8816, Tloss: 0.00009753, VF1: 97.4409, VLoss: 0.00090205,\n",
      "Epoch    99: reducing learning rate of group 0 to 4.0265e-05.\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_018.pth Fragment=00 score=0.9784146562670052\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_020.pth Fragment=00 score=0.9778022381214456\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_059.pth Fragment=00 score=0.978359701763957\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_057.pth Fragment=00 score=0.9805647231389805\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_030.pth Fragment=00 score=0.9775156653151492\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_095.pth Fragment=00 score=0.9806991988346686\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_097.pth Fragment=00 score=0.9773887673231219\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_006.pth Fragment=00 score=0.9772481040086672\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_043.pth Fragment=00 score=0.9779879934509732\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_022.pth Fragment=00 score=0.9770551038843722\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_046.pth Fragment=00 score=0.9791250680704302\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_084.pth Fragment=00 score=0.9795096963044273\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_023.pth Fragment=00 score=0.9796806966618288\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_014.pth Fragment=00 score=0.9793908444282328\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_098.pth Fragment=00 score=0.9798291840814102\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_060.pth Fragment=00 score=0.9802197802197802\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_081.pth Fragment=00 score=0.9771355405158222\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_047.pth Fragment=00 score=0.9774187626216265\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_005.pth Fragment=00 score=0.9712070874861573\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_drebin_085.pth Fragment=00 score=0.9781021897810218\n",
      "Extend: f1score=98.2104, vcoverage=98.0611, vf1score=99.3044, vexentdSize=7890, ecoverage=1.9389, ef1score=61.1465, erestSize=156\n",
      "########\n",
      "######## genome\n",
      "3024\n",
      "0    2506\n",
      "1     518\n",
      "Name: label, dtype: int64\n",
      "756\n",
      "0    634\n",
      "1    122\n",
      "Name: label, dtype: int64\n",
      "3780\n",
      "0    3160\n",
      "1     620\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_genome [0000] TF1: 88.9900, Tloss: 0.00561019, VF1: 94.8207, VLoss: 0.00125553,\n",
      "Train: train_genome [0001] TF1: 97.3812, Tloss: 0.00160975, VF1: 97.4790, VLoss: 0.00041454,\n",
      "Train: train_genome [0002] TF1: 97.6744, Tloss: 0.00099990, VF1: 97.9079, VLoss: 0.00055384,\n",
      "Train: train_genome [0003] TF1: 98.5479, Tloss: 0.00073471, VF1: 98.3333, VLoss: 0.00034192,\n",
      "Train: train_genome [0004] TF1: 99.5178, Tloss: 0.00040072, VF1: 99.5918, VLoss: 0.00031592,\n",
      "Train: train_genome [0005] TF1: 99.8073, Tloss: 0.00027541, VF1: 98.7755, VLoss: 0.00023754,\n",
      "Train: train_genome [0006] TF1: 99.8069, Tloss: 0.00023543, VF1: 92.7757, VLoss: 0.00136773,\n",
      "Train: train_genome [0007] TF1: 99.3224, Tloss: 0.00044846, VF1: 96.1702, VLoss: 0.00088299,\n",
      "Train: train_genome [0008] TF1: 97.9827, Tloss: 0.00058102, VF1: 96.6667, VLoss: 0.00152813,\n",
      "Train: train_genome [0009] TF1: 98.0620, Tloss: 0.00073055, VF1: 98.7552, VLoss: 0.00019088,\n",
      "Train: train_genome [0010] TF1: 99.3263, Tloss: 0.00024627, VF1: 99.1870, VLoss: 0.00022370,\n",
      "Train: train_genome [0011] TF1: 99.7101, Tloss: 0.00015964, VF1: 98.7552, VLoss: 0.00022093,\n",
      "Train: train_genome [0012] TF1: 99.3250, Tloss: 0.00023514, VF1: 94.9020, VLoss: 0.00082922,\n",
      "Train: train_genome [0013] TF1: 99.7113, Tloss: 0.00013445, VF1: 96.8254, VLoss: 0.00048279,\n",
      "Train: train_genome [0014] TF1: 99.7101, Tloss: 0.00015075, VF1: 99.1736, VLoss: 0.00014242,\n",
      "Train: train_genome [0015] TF1: 99.4208, Tloss: 0.00016614, VF1: 88.5845, VLoss: 0.00164332,\n",
      "Train: train_genome [0016] TF1: 99.8066, Tloss: 0.00012487, VF1: 99.1736, VLoss: 0.00012242,\n",
      "Train: train_genome [0017] TF1: 100.0000, Tloss: 0.00004893, VF1: 100.0000, VLoss: 0.00006880,\n",
      "Train: train_genome [0018] TF1: 99.9036, Tloss: 0.00006736, VF1: 98.7552, VLoss: 0.00014809,\n",
      "Train: train_genome [0019] TF1: 99.9036, Tloss: 0.00006300, VF1: 99.5918, VLoss: 0.00006406,\n",
      "Train: train_genome [0020] TF1: 100.0000, Tloss: 0.00006173, VF1: 98.7654, VLoss: 0.00014884,\n",
      "Train: train_genome [0021] TF1: 100.0000, Tloss: 0.00004329, VF1: 99.1736, VLoss: 0.00012904,\n",
      "Train: train_genome [0022] TF1: 100.0000, Tloss: 0.00004021, VF1: 99.1736, VLoss: 0.00007067,\n",
      "Train: train_genome [0023] TF1: 100.0000, Tloss: 0.00002907, VF1: 99.1736, VLoss: 0.00012746,\n",
      "Train: train_genome [0024] TF1: 99.2263, Tloss: 0.00029043, VF1: 95.7265, VLoss: 0.00095887,\n",
      "Train: train_genome [0025] TF1: 98.6486, Tloss: 0.00035786, VF1: 97.0464, VLoss: 0.00054122,\n",
      "Train: train_genome [0026] TF1: 98.5507, Tloss: 0.00046711, VF1: 98.3471, VLoss: 0.00023355,\n",
      "Train: train_genome [0027] TF1: 99.3250, Tloss: 0.00026429, VF1: 83.2536, VLoss: 0.00262001,\n",
      "Train: train_genome [0028] TF1: 99.3276, Tloss: 0.00030024, VF1: 97.5207, VLoss: 0.00053808,\n",
      "Train: train_genome [0029] TF1: 99.3237, Tloss: 0.00029123, VF1: 96.8000, VLoss: 0.00060482,\n",
      "Epoch    29: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_genome [0030] TF1: 99.7101, Tloss: 0.00019528, VF1: 98.7654, VLoss: 0.00021679,\n",
      "Train: train_genome [0031] TF1: 99.7107, Tloss: 0.00017427, VF1: 98.3607, VLoss: 0.00020782,\n",
      "Train: train_genome [0032] TF1: 99.9036, Tloss: 0.00006164, VF1: 98.3607, VLoss: 0.00021663,\n",
      "Train: train_genome [0033] TF1: 100.0000, Tloss: 0.00003635, VF1: 98.7654, VLoss: 0.00019388,\n",
      "Train: train_genome [0034] TF1: 100.0000, Tloss: 0.00002629, VF1: 98.7654, VLoss: 0.00020572,\n",
      "Train: train_genome [0035] TF1: 100.0000, Tloss: 0.00001891, VF1: 98.3607, VLoss: 0.00023253,\n",
      "Train: train_genome [0036] TF1: 100.0000, Tloss: 0.00001514, VF1: 99.1736, VLoss: 0.00019139,\n",
      "Train: train_genome [0037] TF1: 100.0000, Tloss: 0.00001996, VF1: 98.7654, VLoss: 0.00026541,\n",
      "Train: train_genome [0038] TF1: 100.0000, Tloss: 0.00002108, VF1: 98.7654, VLoss: 0.00026959,\n",
      "Train: train_genome [0039] TF1: 99.8069, Tloss: 0.00006746, VF1: 98.3607, VLoss: 0.00024123,\n",
      "Train: train_genome [0040] TF1: 100.0000, Tloss: 0.00001424, VF1: 98.3607, VLoss: 0.00024888,\n",
      "Train: train_genome [0041] TF1: 100.0000, Tloss: 0.00002532, VF1: 98.3607, VLoss: 0.00030013,\n",
      "Train: train_genome [0042] TF1: 100.0000, Tloss: 0.00001446, VF1: 98.7654, VLoss: 0.00026361,\n",
      "Train: train_genome [0043] TF1: 100.0000, Tloss: 0.00001397, VF1: 98.7654, VLoss: 0.00020182,\n",
      "Train: train_genome [0044] TF1: 100.0000, Tloss: 0.00001375, VF1: 99.1803, VLoss: 0.00014144,\n",
      "Train: train_genome [0045] TF1: 100.0000, Tloss: 0.00001148, VF1: 98.7654, VLoss: 0.00016718,\n",
      "Train: train_genome [0046] TF1: 99.9034, Tloss: 0.00004639, VF1: 98.3471, VLoss: 0.00029518,\n",
      "Train: train_genome [0047] TF1: 99.6139, Tloss: 0.00008109, VF1: 98.3471, VLoss: 0.00022931,\n",
      "Train: train_genome [0048] TF1: 99.1304, Tloss: 0.00033198, VF1: 84.3602, VLoss: 0.00221013,\n",
      "Train: train_genome [0049] TF1: 99.6124, Tloss: 0.00018969, VF1: 21.8978, VLoss: 0.01254794,\n",
      "Train: train_genome [0050] TF1: 99.0366, Tloss: 0.00037354, VF1: 98.7552, VLoss: 0.00021352,\n",
      "Train: train_genome [0051] TF1: 100.0000, Tloss: 0.00004761, VF1: 97.9757, VLoss: 0.00016691,\n",
      "Epoch    51: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_genome [0052] TF1: 99.8066, Tloss: 0.00009929, VF1: 99.5885, VLoss: 0.00006131,\n",
      "Train: train_genome [0053] TF1: 100.0000, Tloss: 0.00003078, VF1: 99.5885, VLoss: 0.00011386,\n",
      "Train: train_genome [0054] TF1: 100.0000, Tloss: 0.00002309, VF1: 98.7755, VLoss: 0.00019078,\n",
      "Train: train_genome [0055] TF1: 99.9034, Tloss: 0.00003129, VF1: 99.1870, VLoss: 0.00013919,\n",
      "Train: train_genome [0056] TF1: 100.0000, Tloss: 0.00001500, VF1: 98.7755, VLoss: 0.00016969,\n",
      "Train: train_genome [0057] TF1: 100.0000, Tloss: 0.00001775, VF1: 99.5918, VLoss: 0.00014816,\n",
      "Epoch    57: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_genome [0058] TF1: 100.0000, Tloss: 0.00002133, VF1: 97.9592, VLoss: 0.00020588,\n",
      "Train: train_genome [0059] TF1: 99.8069, Tloss: 0.00006579, VF1: 98.3333, VLoss: 0.00037798,\n",
      "Train: train_genome [0060] TF1: 99.9036, Tloss: 0.00005718, VF1: 98.7654, VLoss: 0.00009740,\n",
      "Train: train_genome [0061] TF1: 99.8073, Tloss: 0.00007863, VF1: 98.3607, VLoss: 0.00021862,\n",
      "Train: train_genome [0062] TF1: 99.8069, Tloss: 0.00004941, VF1: 98.3607, VLoss: 0.00020800,\n",
      "Train: train_genome [0063] TF1: 99.9034, Tloss: 0.00006564, VF1: 98.7654, VLoss: 0.00018447,\n",
      "Epoch    63: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_genome [0064] TF1: 100.0000, Tloss: 0.00002205, VF1: 98.7755, VLoss: 0.00018611,\n",
      "Train: train_genome [0065] TF1: 99.9034, Tloss: 0.00002803, VF1: 99.1736, VLoss: 0.00010937,\n",
      "Train: train_genome [0066] TF1: 100.0000, Tloss: 0.00001864, VF1: 99.1736, VLoss: 0.00011979,\n",
      "Train: train_genome [0067] TF1: 100.0000, Tloss: 0.00001348, VF1: 98.3607, VLoss: 0.00020182,\n",
      "Train: train_genome [0068] TF1: 100.0000, Tloss: 0.00001189, VF1: 99.1736, VLoss: 0.00020048,\n",
      "Train: train_genome [0069] TF1: 100.0000, Tloss: 0.00001255, VF1: 99.1736, VLoss: 0.00014520,\n",
      "Epoch    69: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_genome [0070] TF1: 100.0000, Tloss: 0.00001145, VF1: 99.1736, VLoss: 0.00021213,\n",
      "Train: train_genome [0071] TF1: 99.9036, Tloss: 0.00002411, VF1: 99.1736, VLoss: 0.00015624,\n",
      "Train: train_genome [0072] TF1: 100.0000, Tloss: 0.00001001, VF1: 98.3607, VLoss: 0.00019368,\n",
      "Train: train_genome [0073] TF1: 100.0000, Tloss: 0.00000839, VF1: 99.1736, VLoss: 0.00021110,\n",
      "Train: train_genome [0074] TF1: 100.0000, Tloss: 0.00000882, VF1: 99.1736, VLoss: 0.00017172,\n",
      "Train: train_genome [0075] TF1: 100.0000, Tloss: 0.00001083, VF1: 98.7654, VLoss: 0.00021713,\n",
      "Train: train_genome [0076] TF1: 100.0000, Tloss: 0.00001037, VF1: 98.7654, VLoss: 0.00024401,\n",
      "Train: train_genome [0077] TF1: 99.8069, Tloss: 0.00006836, VF1: 99.1736, VLoss: 0.00027062,\n",
      "Train: train_genome [0078] TF1: 100.0000, Tloss: 0.00001464, VF1: 99.1736, VLoss: 0.00020701,\n",
      "Train: train_genome [0079] TF1: 100.0000, Tloss: 0.00000942, VF1: 98.3607, VLoss: 0.00019763,\n",
      "Epoch    79: reducing learning rate of group 0 to 7.8643e-05.\n",
      "Train: train_genome [0080] TF1: 100.0000, Tloss: 0.00001097, VF1: 97.9592, VLoss: 0.00030232,\n",
      "Train: train_genome [0081] TF1: 100.0000, Tloss: 0.00000954, VF1: 98.7654, VLoss: 0.00019351,\n",
      "Train: train_genome [0082] TF1: 100.0000, Tloss: 0.00001823, VF1: 99.1736, VLoss: 0.00016096,\n",
      "Train: train_genome [0083] TF1: 99.9034, Tloss: 0.00001833, VF1: 96.0317, VLoss: 0.00048291,\n",
      "Train: train_genome [0084] TF1: 99.6139, Tloss: 0.00013871, VF1: 98.7755, VLoss: 0.00028795,\n",
      "Train: train_genome [0085] TF1: 100.0000, Tloss: 0.00001946, VF1: 98.7654, VLoss: 0.00024600,\n",
      "Epoch    85: reducing learning rate of group 0 to 6.2915e-05.\n",
      "Train: train_genome [0086] TF1: 99.9036, Tloss: 0.00002924, VF1: 99.1736, VLoss: 0.00025848,\n",
      "Train: train_genome [0087] TF1: 100.0000, Tloss: 0.00001794, VF1: 98.7654, VLoss: 0.00018796,\n",
      "Train: train_genome [0088] TF1: 100.0000, Tloss: 0.00001771, VF1: 98.7654, VLoss: 0.00027775,\n",
      "Train: train_genome [0089] TF1: 99.8069, Tloss: 0.00005614, VF1: 98.7654, VLoss: 0.00021248,\n",
      "Train: train_genome [0090] TF1: 100.0000, Tloss: 0.00001463, VF1: 98.3607, VLoss: 0.00020095,\n",
      "Train: train_genome [0091] TF1: 100.0000, Tloss: 0.00001064, VF1: 99.1803, VLoss: 0.00020542,\n",
      "Epoch    91: reducing learning rate of group 0 to 5.0332e-05.\n",
      "Train: train_genome [0092] TF1: 99.9036, Tloss: 0.00003020, VF1: 98.7654, VLoss: 0.00029677,\n",
      "Train: train_genome [0093] TF1: 100.0000, Tloss: 0.00001220, VF1: 99.1736, VLoss: 0.00020052,\n",
      "Train: train_genome [0094] TF1: 100.0000, Tloss: 0.00001061, VF1: 98.7654, VLoss: 0.00023536,\n",
      "Train: train_genome [0095] TF1: 99.9034, Tloss: 0.00002088, VF1: 98.7654, VLoss: 0.00017946,\n",
      "Train: train_genome [0096] TF1: 100.0000, Tloss: 0.00001135, VF1: 98.7654, VLoss: 0.00025668,\n",
      "Train: train_genome [0097] TF1: 100.0000, Tloss: 0.00000876, VF1: 98.7654, VLoss: 0.00023367,\n",
      "Epoch    97: reducing learning rate of group 0 to 4.0265e-05.\n",
      "Train: train_genome [0098] TF1: 100.0000, Tloss: 0.00000607, VF1: 98.7654, VLoss: 0.00027387,\n",
      "Train: train_genome [0099] TF1: 100.0000, Tloss: 0.00001133, VF1: 98.7654, VLoss: 0.00025710,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_052.pth Fragment=00 score=0.984689766317486\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_019.pth Fragment=00 score=0.9840510366826155\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_017.pth Fragment=00 score=0.9855305466237942\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_022.pth Fragment=00 score=0.9861900893582453\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_060.pth Fragment=00 score=0.983974358974359\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_065.pth Fragment=00 score=0.9854838709677419\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_053.pth Fragment=00 score=0.9871175523349437\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_066.pth Fragment=00 score=0.9863013698630136\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_016.pth Fragment=00 score=0.9886363636363636\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_023.pth Fragment=00 score=0.9894222945484134\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_021.pth Fragment=00 score=0.9886547811993517\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_055.pth Fragment=00 score=0.9847878302642114\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_044.pth Fragment=00 score=0.9887278582930757\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_014.pth Fragment=00 score=0.982940698619009\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_069.pth Fragment=00 score=0.9862348178137651\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_018.pth Fragment=00 score=0.9771241830065359\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_057.pth Fragment=00 score=0.9879711307137128\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_020.pth Fragment=00 score=0.9862792574656981\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_071.pth Fragment=00 score=0.9863013698630136\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_genome_082.pth Fragment=00 score=0.9902912621359222\n",
      "Extend: f1score=98.8691, vcoverage=99.3915, vf1score=99.5885, vexentdSize=3757, ecoverage=0.6085, ef1score=60.8696, erestSize=23\n",
      "########\n",
      "######## amd\n",
      "27913\n",
      "0    18566\n",
      "1     9347\n",
      "Name: label, dtype: int64\n",
      "6978\n",
      "0    4712\n",
      "1    2266\n",
      "Name: label, dtype: int64\n",
      "34892\n",
      "0    23244\n",
      "1    11648\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_amd [0000] TF1: 94.5531, Tloss: 0.00330223, VF1: 89.3592, VLoss: 0.00313947,\n",
      "Train: train_amd [0001] TF1: 97.4100, Tloss: 0.00164368, VF1: 77.2828, VLoss: 0.00906947,\n",
      "Train: train_amd [0002] TF1: 97.8354, Tloss: 0.00137562, VF1: 97.5664, VLoss: 0.00079620,\n",
      "Train: train_amd [0003] TF1: 98.1506, Tloss: 0.00113326, VF1: 91.1806, VLoss: 0.00238410,\n",
      "Train: train_amd [0004] TF1: 98.3601, Tloss: 0.00106285, VF1: 68.2236, VLoss: 0.01087538,\n",
      "Train: train_amd [0005] TF1: 98.6080, Tloss: 0.00092575, VF1: 97.8630, VLoss: 0.00062334,\n",
      "Train: train_amd [0006] TF1: 98.7039, Tloss: 0.00091912, VF1: 96.9930, VLoss: 0.00090659,\n",
      "Train: train_amd [0007] TF1: 98.6545, Tloss: 0.00090032, VF1: 95.6337, VLoss: 0.00137836,\n",
      "Train: train_amd [0008] TF1: 98.7518, Tloss: 0.00082795, VF1: 97.9592, VLoss: 0.00072778,\n",
      "Train: train_amd [0009] TF1: 98.6768, Tloss: 0.00085773, VF1: 98.6761, VLoss: 0.00051959,\n",
      "Train: train_amd [0010] TF1: 98.7794, Tloss: 0.00082289, VF1: 97.7991, VLoss: 0.00075367,\n",
      "Train: train_amd [0011] TF1: 98.8266, Tloss: 0.00075075, VF1: 97.2034, VLoss: 0.00090981,\n",
      "Train: train_amd [0012] TF1: 98.8904, Tloss: 0.00079255, VF1: 98.2620, VLoss: 0.00061420,\n",
      "Train: train_amd [0013] TF1: 98.9881, Tloss: 0.00068591, VF1: 98.2740, VLoss: 0.00058172,\n",
      "Train: train_amd [0014] TF1: 98.9937, Tloss: 0.00071014, VF1: 98.8729, VLoss: 0.00041931,\n",
      "Train: train_amd [0015] TF1: 99.0044, Tloss: 0.00066996, VF1: 96.9410, VLoss: 0.00088425,\n",
      "Train: train_amd [0016] TF1: 98.9183, Tloss: 0.00071877, VF1: 98.3760, VLoss: 0.00058183,\n",
      "Train: train_amd [0017] TF1: 99.0576, Tloss: 0.00061865, VF1: 98.1490, VLoss: 0.00062504,\n",
      "Train: train_amd [0018] TF1: 99.0033, Tloss: 0.00069343, VF1: 94.9053, VLoss: 0.00156589,\n",
      "Train: train_amd [0019] TF1: 99.2235, Tloss: 0.00056627, VF1: 96.1211, VLoss: 0.00125378,\n",
      "Train: train_amd [0020] TF1: 99.2286, Tloss: 0.00058111, VF1: 98.6151, VLoss: 0.00056943,\n",
      "Train: train_amd [0021] TF1: 99.1060, Tloss: 0.00061189, VF1: 98.7437, VLoss: 0.00045068,\n",
      "Train: train_amd [0022] TF1: 99.2719, Tloss: 0.00050442, VF1: 98.3938, VLoss: 0.00058449,\n",
      "Train: train_amd [0023] TF1: 99.0895, Tloss: 0.00061773, VF1: 98.1066, VLoss: 0.00059445,\n",
      "Train: train_amd [0024] TF1: 99.1275, Tloss: 0.00060835, VF1: 98.4848, VLoss: 0.00048930,\n",
      "Train: train_amd [0025] TF1: 99.1913, Tloss: 0.00054266, VF1: 98.2969, VLoss: 0.00054952,\n",
      "Train: train_amd [0026] TF1: 99.1326, Tloss: 0.00059867, VF1: 97.8647, VLoss: 0.00087325,\n",
      "Train: train_amd [0027] TF1: 99.2555, Tloss: 0.00053195, VF1: 98.6579, VLoss: 0.00057857,\n",
      "Train: train_amd [0028] TF1: 99.2935, Tloss: 0.00050384, VF1: 97.2216, VLoss: 0.00089709,\n",
      "Train: train_amd [0029] TF1: 99.3471, Tloss: 0.00045891, VF1: 98.8724, VLoss: 0.00049626,\n",
      "Train: train_amd [0030] TF1: 99.3035, Tloss: 0.00051760, VF1: 98.2441, VLoss: 0.00065912,\n",
      "Train: train_amd [0031] TF1: 99.2562, Tloss: 0.00052293, VF1: 97.7847, VLoss: 0.00079270,\n",
      "Train: train_amd [0032] TF1: 99.2609, Tloss: 0.00052933, VF1: 98.7007, VLoss: 0.00048402,\n",
      "Train: train_amd [0033] TF1: 99.3143, Tloss: 0.00050305, VF1: 98.7888, VLoss: 0.00043848,\n",
      "Train: train_amd [0034] TF1: 99.2235, Tloss: 0.00053906, VF1: 98.6040, VLoss: 0.00048821,\n",
      "Train: train_amd [0035] TF1: 99.2983, Tloss: 0.00048191, VF1: 97.5716, VLoss: 0.00079187,\n",
      "Epoch    35: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_amd [0036] TF1: 99.4377, Tloss: 0.00038769, VF1: 99.0728, VLoss: 0.00038213,\n",
      "Train: train_amd [0037] TF1: 99.4703, Tloss: 0.00038180, VF1: 98.7453, VLoss: 0.00045075,\n",
      "Train: train_amd [0038] TF1: 99.5718, Tloss: 0.00035554, VF1: 98.7802, VLoss: 0.00051376,\n",
      "Train: train_amd [0039] TF1: 99.4864, Tloss: 0.00035782, VF1: 98.6204, VLoss: 0.00053292,\n",
      "Train: train_amd [0040] TF1: 99.2985, Tloss: 0.00047662, VF1: 98.9381, VLoss: 0.00040357,\n",
      "Train: train_amd [0041] TF1: 99.5127, Tloss: 0.00037005, VF1: 98.7414, VLoss: 0.00045684,\n",
      "Train: train_amd [0042] TF1: 99.4696, Tloss: 0.00038357, VF1: 98.7883, VLoss: 0.00046884,\n",
      "Train: train_amd [0043] TF1: 99.4594, Tloss: 0.00036311, VF1: 98.7600, VLoss: 0.00047228,\n",
      "Train: train_amd [0044] TF1: 99.5022, Tloss: 0.00037486, VF1: 98.0480, VLoss: 0.00087608,\n",
      "Epoch    44: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_amd [0045] TF1: 99.4915, Tloss: 0.00036333, VF1: 98.6381, VLoss: 0.00056395,\n",
      "Train: train_amd [0046] TF1: 99.5931, Tloss: 0.00029191, VF1: 98.7770, VLoss: 0.00050052,\n",
      "Train: train_amd [0047] TF1: 99.6092, Tloss: 0.00029370, VF1: 98.9850, VLoss: 0.00040067,\n",
      "Train: train_amd [0048] TF1: 99.5343, Tloss: 0.00033924, VF1: 99.0931, VLoss: 0.00037156,\n",
      "Train: train_amd [0049] TF1: 99.5989, Tloss: 0.00029914, VF1: 99.1381, VLoss: 0.00037652,\n",
      "Train: train_amd [0050] TF1: 99.5665, Tloss: 0.00029332, VF1: 98.9176, VLoss: 0.00044912,\n",
      "Train: train_amd [0051] TF1: 99.5234, Tloss: 0.00031928, VF1: 98.8679, VLoss: 0.00044649,\n",
      "Train: train_amd [0052] TF1: 99.6147, Tloss: 0.00029321, VF1: 99.1366, VLoss: 0.00041416,\n",
      "Epoch    52: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_amd [0053] TF1: 99.6787, Tloss: 0.00025692, VF1: 98.9890, VLoss: 0.00038914,\n",
      "Train: train_amd [0054] TF1: 99.6736, Tloss: 0.00023840, VF1: 98.3599, VLoss: 0.00056522,\n",
      "Train: train_amd [0055] TF1: 99.6413, Tloss: 0.00025957, VF1: 98.9832, VLoss: 0.00039280,\n",
      "Train: train_amd [0056] TF1: 99.6254, Tloss: 0.00027238, VF1: 99.1597, VLoss: 0.00035934,\n",
      "Train: train_amd [0057] TF1: 99.6575, Tloss: 0.00024577, VF1: 98.6708, VLoss: 0.00047572,\n",
      "Train: train_amd [0058] TF1: 99.6736, Tloss: 0.00025020, VF1: 98.9850, VLoss: 0.00041855,\n",
      "Train: train_amd [0059] TF1: 99.6308, Tloss: 0.00024837, VF1: 98.6898, VLoss: 0.00050401,\n",
      "Train: train_amd [0060] TF1: 99.6147, Tloss: 0.00028511, VF1: 99.1351, VLoss: 0.00039022,\n",
      "Epoch    60: reducing learning rate of group 0 to 1.2288e-04.\n",
      "Train: train_amd [0061] TF1: 99.6145, Tloss: 0.00027752, VF1: 98.7925, VLoss: 0.00043946,\n",
      "Train: train_amd [0062] TF1: 99.6736, Tloss: 0.00021871, VF1: 99.1393, VLoss: 0.00033225,\n",
      "Train: train_amd [0063] TF1: 99.6896, Tloss: 0.00021937, VF1: 98.7093, VLoss: 0.00046174,\n",
      "Train: train_amd [0064] TF1: 99.6521, Tloss: 0.00026245, VF1: 99.1574, VLoss: 0.00041526,\n",
      "Train: train_amd [0065] TF1: 99.7325, Tloss: 0.00020247, VF1: 98.8899, VLoss: 0.00043050,\n",
      "Train: train_amd [0066] TF1: 99.7056, Tloss: 0.00022157, VF1: 98.8864, VLoss: 0.00051910,\n",
      "Train: train_amd [0067] TF1: 99.7271, Tloss: 0.00023589, VF1: 99.0967, VLoss: 0.00041128,\n",
      "Train: train_amd [0068] TF1: 99.7325, Tloss: 0.00020661, VF1: 99.0024, VLoss: 0.00046984,\n",
      "Train: train_amd [0069] TF1: 99.6627, Tloss: 0.00025981, VF1: 98.8591, VLoss: 0.00041930,\n",
      "Train: train_amd [0070] TF1: 99.6844, Tloss: 0.00022809, VF1: 99.2279, VLoss: 0.00038528,\n",
      "Train: train_amd [0071] TF1: 99.7056, Tloss: 0.00020764, VF1: 99.0227, VLoss: 0.00048539,\n",
      "Epoch    71: reducing learning rate of group 0 to 9.8304e-05.\n",
      "Train: train_amd [0072] TF1: 99.7270, Tloss: 0.00020236, VF1: 99.1150, VLoss: 0.00039303,\n",
      "Train: train_amd [0073] TF1: 99.7003, Tloss: 0.00020629, VF1: 98.9609, VLoss: 0.00043820,\n",
      "Train: train_amd [0074] TF1: 99.6788, Tloss: 0.00022501, VF1: 99.0724, VLoss: 0.00040397,\n",
      "Train: train_amd [0075] TF1: 99.6843, Tloss: 0.00022128, VF1: 98.9576, VLoss: 0.00047620,\n",
      "Train: train_amd [0076] TF1: 99.7913, Tloss: 0.00016570, VF1: 99.0082, VLoss: 0.00042573,\n",
      "Train: train_amd [0077] TF1: 99.7484, Tloss: 0.00019751, VF1: 98.8085, VLoss: 0.00047247,\n",
      "Train: train_amd [0078] TF1: 99.7539, Tloss: 0.00017978, VF1: 99.1131, VLoss: 0.00043123,\n",
      "Train: train_amd [0079] TF1: 99.7538, Tloss: 0.00018915, VF1: 98.6590, VLoss: 0.00052549,\n",
      "Train: train_amd [0080] TF1: 99.7110, Tloss: 0.00021970, VF1: 99.1158, VLoss: 0.00036858,\n",
      "Train: train_amd [0081] TF1: 99.7218, Tloss: 0.00018275, VF1: 98.9604, VLoss: 0.00046552,\n",
      "Train: train_amd [0082] TF1: 99.7271, Tloss: 0.00018210, VF1: 98.9390, VLoss: 0.00039776,\n",
      "Epoch    82: reducing learning rate of group 0 to 7.8643e-05.\n",
      "Train: train_amd [0083] TF1: 99.7645, Tloss: 0.00017025, VF1: 98.9814, VLoss: 0.00046826,\n",
      "Train: train_amd [0084] TF1: 99.7430, Tloss: 0.00019945, VF1: 98.9195, VLoss: 0.00040189,\n",
      "Train: train_amd [0085] TF1: 99.7111, Tloss: 0.00018629, VF1: 99.0493, VLoss: 0.00040619,\n",
      "Train: train_amd [0086] TF1: 99.7966, Tloss: 0.00018825, VF1: 98.9627, VLoss: 0.00039429,\n",
      "Train: train_amd [0087] TF1: 99.7753, Tloss: 0.00017380, VF1: 98.5767, VLoss: 0.00058791,\n",
      "Train: train_amd [0088] TF1: 99.7807, Tloss: 0.00015959, VF1: 99.0947, VLoss: 0.00042228,\n",
      "Train: train_amd [0089] TF1: 99.8128, Tloss: 0.00015520, VF1: 98.9613, VLoss: 0.00046227,\n",
      "Train: train_amd [0090] TF1: 99.7967, Tloss: 0.00016967, VF1: 98.8111, VLoss: 0.00048121,\n",
      "Train: train_amd [0091] TF1: 99.8181, Tloss: 0.00015250, VF1: 99.0951, VLoss: 0.00039202,\n",
      "Train: train_amd [0092] TF1: 99.7485, Tloss: 0.00017113, VF1: 98.9645, VLoss: 0.00042301,\n",
      "Train: train_amd [0093] TF1: 99.7806, Tloss: 0.00015978, VF1: 98.9399, VLoss: 0.00042638,\n",
      "Train: train_amd [0094] TF1: 99.7860, Tloss: 0.00015537, VF1: 98.8298, VLoss: 0.00047914,\n",
      "Train: train_amd [0095] TF1: 99.7860, Tloss: 0.00018090, VF1: 98.8354, VLoss: 0.00045308,\n",
      "Train: train_amd [0096] TF1: 99.7806, Tloss: 0.00015559, VF1: 98.9641, VLoss: 0.00039807,\n",
      "Train: train_amd [0097] TF1: 99.7164, Tloss: 0.00020577, VF1: 98.9872, VLoss: 0.00040152,\n",
      "Epoch    97: reducing learning rate of group 0 to 6.2915e-05.\n",
      "Train: train_amd [0098] TF1: 99.7860, Tloss: 0.00014639, VF1: 98.9395, VLoss: 0.00046895,\n",
      "Train: train_amd [0099] TF1: 99.7699, Tloss: 0.00017489, VF1: 99.1798, VLoss: 0.00043794,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_062.pth Fragment=00 score=0.9914948453608248\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_056.pth Fragment=00 score=0.9907032796763364\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_080.pth Fragment=00 score=0.9909716251074808\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_048.pth Fragment=00 score=0.9895245074794154\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_049.pth Fragment=00 score=0.9903171665877694\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_036.pth Fragment=00 score=0.9893461637597731\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_070.pth Fragment=00 score=0.9909871244635193\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_053.pth Fragment=00 score=0.9895655148819705\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_060.pth Fragment=00 score=0.9900340825747443\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_091.pth Fragment=00 score=0.9918272539573296\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_055.pth Fragment=00 score=0.9907964906244624\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_072.pth Fragment=00 score=0.9916110991611099\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_086.pth Fragment=00 score=0.9908051903411532\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_082.pth Fragment=00 score=0.9916566316875968\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_096.pth Fragment=00 score=0.9916788195933773\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_047.pth Fragment=00 score=0.9906509992280642\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_097.pth Fragment=00 score=0.9912194286203795\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_084.pth Fragment=00 score=0.99119831694646\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_040.pth Fragment=00 score=0.9897341179502598\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_amd_074.pth Fragment=00 score=0.9919250923460184\n",
      "Extend: f1score=99.3812, vcoverage=99.2921, vf1score=99.6696, vexentdSize=34645, ecoverage=0.7079, ef1score=74.6269, erestSize=247\n",
      "########\n",
      "######## maldozer\n",
      "23820\n",
      "0    15729\n",
      "1     8091\n",
      "Name: label, dtype: int64\n",
      "5955\n",
      "0    3989\n",
      "1    1966\n",
      "Name: label, dtype: int64\n",
      "29775\n",
      "0    19982\n",
      "1     9793\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_maldozer [0000] TF1: 93.2027, Tloss: 0.00397259, VF1: 89.8016, VLoss: 0.00280319,\n",
      "Train: train_maldozer [0001] TF1: 95.9995, Tloss: 0.00251708, VF1: 95.5683, VLoss: 0.00134092,\n",
      "Train: train_maldozer [0002] TF1: 96.5923, Tloss: 0.00210315, VF1: 96.2564, VLoss: 0.00122365,\n",
      "Train: train_maldozer [0003] TF1: 96.8235, Tloss: 0.00191728, VF1: 79.8122, VLoss: 0.00762384,\n",
      "Train: train_maldozer [0004] TF1: 97.0800, Tloss: 0.00178361, VF1: 90.5608, VLoss: 0.00295036,\n",
      "Train: train_maldozer [0005] TF1: 97.0822, Tloss: 0.00175188, VF1: 96.9115, VLoss: 0.00113212,\n",
      "Train: train_maldozer [0006] TF1: 97.3866, Tloss: 0.00159069, VF1: 94.3002, VLoss: 0.00176557,\n",
      "Train: train_maldozer [0007] TF1: 97.3866, Tloss: 0.00157832, VF1: 93.1128, VLoss: 0.00223769,\n",
      "Train: train_maldozer [0008] TF1: 97.6285, Tloss: 0.00146875, VF1: 96.5068, VLoss: 0.00108040,\n",
      "Train: train_maldozer [0009] TF1: 97.6476, Tloss: 0.00136963, VF1: 96.9573, VLoss: 0.00098711,\n",
      "Train: train_maldozer [0010] TF1: 97.8219, Tloss: 0.00135018, VF1: 95.0271, VLoss: 0.00239795,\n",
      "Train: train_maldozer [0011] TF1: 97.8483, Tloss: 0.00126078, VF1: 96.8612, VLoss: 0.00111702,\n",
      "Train: train_maldozer [0012] TF1: 97.7894, Tloss: 0.00130369, VF1: 95.6370, VLoss: 0.00126584,\n",
      "Train: train_maldozer [0013] TF1: 97.9283, Tloss: 0.00123520, VF1: 96.6915, VLoss: 0.00105870,\n",
      "Train: train_maldozer [0014] TF1: 97.9521, Tloss: 0.00123141, VF1: 95.1651, VLoss: 0.00162812,\n",
      "Train: train_maldozer [0015] TF1: 97.9579, Tloss: 0.00120155, VF1: 96.4277, VLoss: 0.00109285,\n",
      "Train: train_maldozer [0016] TF1: 98.2508, Tloss: 0.00115201, VF1: 96.9417, VLoss: 0.00116772,\n",
      "Train: train_maldozer [0017] TF1: 98.2149, Tloss: 0.00109015, VF1: 96.7692, VLoss: 0.00104899,\n",
      "Train: train_maldozer [0018] TF1: 98.1710, Tloss: 0.00108764, VF1: 91.8033, VLoss: 0.00411582,\n",
      "Train: train_maldozer [0019] TF1: 98.1338, Tloss: 0.00111310, VF1: 96.6007, VLoss: 0.00125513,\n",
      "Train: train_maldozer [0020] TF1: 98.3133, Tloss: 0.00100135, VF1: 96.7330, VLoss: 0.00128069,\n",
      "Train: train_maldozer [0021] TF1: 98.4137, Tloss: 0.00105894, VF1: 96.6001, VLoss: 0.00116832,\n",
      "Train: train_maldozer [0022] TF1: 98.3765, Tloss: 0.00102726, VF1: 96.7791, VLoss: 0.00155247,\n",
      "Train: train_maldozer [0023] TF1: 98.4208, Tloss: 0.00096515, VF1: 93.0019, VLoss: 0.00191927,\n",
      "Train: train_maldozer [0024] TF1: 98.2763, Tloss: 0.00102734, VF1: 97.2575, VLoss: 0.00098952,\n",
      "Train: train_maldozer [0025] TF1: 98.4943, Tloss: 0.00097359, VF1: 95.4114, VLoss: 0.00175151,\n",
      "Train: train_maldozer [0026] TF1: 98.3698, Tloss: 0.00092396, VF1: 96.6778, VLoss: 0.00119311,\n",
      "Train: train_maldozer [0027] TF1: 98.4509, Tloss: 0.00095573, VF1: 90.3548, VLoss: 0.00291313,\n",
      "Train: train_maldozer [0028] TF1: 98.5682, Tloss: 0.00086743, VF1: 97.0581, VLoss: 0.00105650,\n",
      "Train: train_maldozer [0029] TF1: 98.3753, Tloss: 0.00095163, VF1: 97.3111, VLoss: 0.00097240,\n",
      "Train: train_maldozer [0030] TF1: 98.5620, Tloss: 0.00091209, VF1: 97.0939, VLoss: 0.00109091,\n",
      "Train: train_maldozer [0031] TF1: 98.4764, Tloss: 0.00087283, VF1: 96.3526, VLoss: 0.00111596,\n",
      "Train: train_maldozer [0032] TF1: 98.5947, Tloss: 0.00084306, VF1: 90.6224, VLoss: 0.00459995,\n",
      "Train: train_maldozer [0033] TF1: 98.5881, Tloss: 0.00082877, VF1: 97.2477, VLoss: 0.00107267,\n",
      "Train: train_maldozer [0034] TF1: 98.6179, Tloss: 0.00087954, VF1: 97.3137, VLoss: 0.00096026,\n",
      "Train: train_maldozer [0035] TF1: 98.6492, Tloss: 0.00084016, VF1: 97.2124, VLoss: 0.00097710,\n",
      "Train: train_maldozer [0036] TF1: 98.6558, Tloss: 0.00084256, VF1: 97.2208, VLoss: 0.00102544,\n",
      "Train: train_maldozer [0037] TF1: 98.6746, Tloss: 0.00084539, VF1: 95.0358, VLoss: 0.00185473,\n",
      "Train: train_maldozer [0038] TF1: 98.6794, Tloss: 0.00079501, VF1: 97.3993, VLoss: 0.00100571,\n",
      "Train: train_maldozer [0039] TF1: 98.5940, Tloss: 0.00085689, VF1: 95.6910, VLoss: 0.00176066,\n",
      "Train: train_maldozer [0040] TF1: 98.6118, Tloss: 0.00078761, VF1: 96.7110, VLoss: 0.00114930,\n",
      "Train: train_maldozer [0041] TF1: 98.7492, Tloss: 0.00074046, VF1: 97.0384, VLoss: 0.00135080,\n",
      "Train: train_maldozer [0042] TF1: 98.5748, Tloss: 0.00079948, VF1: 96.5587, VLoss: 0.00116839,\n",
      "Train: train_maldozer [0043] TF1: 98.8110, Tloss: 0.00076405, VF1: 96.5909, VLoss: 0.00151138,\n",
      "Train: train_maldozer [0044] TF1: 98.7804, Tloss: 0.00072439, VF1: 96.6944, VLoss: 0.00109553,\n",
      "Train: train_maldozer [0045] TF1: 98.6814, Tloss: 0.00077723, VF1: 95.9027, VLoss: 0.00143852,\n",
      "Train: train_maldozer [0046] TF1: 98.7859, Tloss: 0.00070573, VF1: 91.7769, VLoss: 0.00301277,\n",
      "Train: train_maldozer [0047] TF1: 98.8170, Tloss: 0.00072346, VF1: 96.0882, VLoss: 0.00194656,\n",
      "Train: train_maldozer [0048] TF1: 98.8857, Tloss: 0.00067692, VF1: 95.1523, VLoss: 0.00187906,\n",
      "Train: train_maldozer [0049] TF1: 98.8679, Tloss: 0.00070223, VF1: 97.0145, VLoss: 0.00107479,\n",
      "Train: train_maldozer [0050] TF1: 98.8548, Tloss: 0.00068394, VF1: 97.0327, VLoss: 0.00114485,\n",
      "Train: train_maldozer [0051] TF1: 98.8479, Tloss: 0.00072884, VF1: 96.2614, VLoss: 0.00141289,\n",
      "Train: train_maldozer [0052] TF1: 98.8796, Tloss: 0.00066939, VF1: 97.3624, VLoss: 0.00104368,\n",
      "Train: train_maldozer [0053] TF1: 98.7995, Tloss: 0.00072243, VF1: 96.9886, VLoss: 0.00114701,\n",
      "Train: train_maldozer [0054] TF1: 98.9410, Tloss: 0.00066289, VF1: 97.4903, VLoss: 0.00113189,\n",
      "Train: train_maldozer [0055] TF1: 98.9659, Tloss: 0.00066089, VF1: 93.8644, VLoss: 0.00239141,\n",
      "Train: train_maldozer [0056] TF1: 98.9654, Tloss: 0.00066693, VF1: 97.6555, VLoss: 0.00094752,\n",
      "Train: train_maldozer [0057] TF1: 98.8676, Tloss: 0.00073687, VF1: 97.0513, VLoss: 0.00106997,\n",
      "Train: train_maldozer [0058] TF1: 98.8103, Tloss: 0.00065928, VF1: 97.3501, VLoss: 0.00119230,\n",
      "Train: train_maldozer [0059] TF1: 98.9780, Tloss: 0.00066102, VF1: 96.8750, VLoss: 0.00124596,\n",
      "Train: train_maldozer [0060] TF1: 98.8667, Tloss: 0.00066373, VF1: 93.9313, VLoss: 0.00251647,\n",
      "Train: train_maldozer [0061] TF1: 98.9102, Tloss: 0.00066166, VF1: 96.6332, VLoss: 0.00116275,\n",
      "Train: train_maldozer [0062] TF1: 99.0586, Tloss: 0.00064487, VF1: 96.9514, VLoss: 0.00120359,\n",
      "Train: train_maldozer [0063] TF1: 98.7920, Tloss: 0.00072279, VF1: 96.3893, VLoss: 0.00138646,\n",
      "Train: train_maldozer [0064] TF1: 98.9781, Tloss: 0.00064630, VF1: 97.1784, VLoss: 0.00115087,\n",
      "Train: train_maldozer [0065] TF1: 98.8608, Tloss: 0.00072113, VF1: 97.3860, VLoss: 0.00111531,\n",
      "Train: train_maldozer [0066] TF1: 98.9471, Tloss: 0.00066016, VF1: 97.4016, VLoss: 0.00103539,\n",
      "Train: train_maldozer [0067] TF1: 98.9601, Tloss: 0.00064634, VF1: 96.8639, VLoss: 0.00120367,\n",
      "Train: train_maldozer [0068] TF1: 99.0024, Tloss: 0.00062203, VF1: 97.1575, VLoss: 0.00103304,\n",
      "Train: train_maldozer [0069] TF1: 99.0900, Tloss: 0.00056029, VF1: 97.1240, VLoss: 0.00116562,\n",
      "Train: train_maldozer [0070] TF1: 98.9900, Tloss: 0.00063943, VF1: 89.9554, VLoss: 0.00379286,\n",
      "Train: train_maldozer [0071] TF1: 99.0771, Tloss: 0.00060435, VF1: 96.4312, VLoss: 0.00136495,\n",
      "Train: train_maldozer [0072] TF1: 99.0405, Tloss: 0.00064324, VF1: 97.2122, VLoss: 0.00133207,\n",
      "Train: train_maldozer [0073] TF1: 98.8600, Tloss: 0.00067329, VF1: 97.4043, VLoss: 0.00108262,\n",
      "Train: train_maldozer [0074] TF1: 99.1275, Tloss: 0.00056747, VF1: 97.1370, VLoss: 0.00122134,\n",
      "Train: train_maldozer [0075] TF1: 98.9407, Tloss: 0.00065716, VF1: 97.6970, VLoss: 0.00100376,\n",
      "Epoch    75: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_maldozer [0076] TF1: 99.1831, Tloss: 0.00050813, VF1: 97.2306, VLoss: 0.00106386,\n",
      "Train: train_maldozer [0077] TF1: 99.0706, Tloss: 0.00055588, VF1: 96.1730, VLoss: 0.00138747,\n",
      "Train: train_maldozer [0078] TF1: 99.1646, Tloss: 0.00050381, VF1: 97.2723, VLoss: 0.00111997,\n",
      "Train: train_maldozer [0079] TF1: 99.0528, Tloss: 0.00056430, VF1: 97.4306, VLoss: 0.00105754,\n",
      "Train: train_maldozer [0080] TF1: 99.1886, Tloss: 0.00049120, VF1: 96.8481, VLoss: 0.00148809,\n",
      "Train: train_maldozer [0081] TF1: 99.2447, Tloss: 0.00048490, VF1: 97.3286, VLoss: 0.00107688,\n",
      "Train: train_maldozer [0082] TF1: 99.1709, Tloss: 0.00049805, VF1: 97.4147, VLoss: 0.00121028,\n",
      "Train: train_maldozer [0083] TF1: 99.0712, Tloss: 0.00053973, VF1: 97.3564, VLoss: 0.00104506,\n",
      "Train: train_maldozer [0084] TF1: 99.1895, Tloss: 0.00050394, VF1: 97.1977, VLoss: 0.00133775,\n",
      "Train: train_maldozer [0085] TF1: 99.1893, Tloss: 0.00052083, VF1: 97.6589, VLoss: 0.00116023,\n",
      "Train: train_maldozer [0086] TF1: 99.1958, Tloss: 0.00043381, VF1: 97.3015, VLoss: 0.00122105,\n",
      "Train: train_maldozer [0087] TF1: 99.1215, Tloss: 0.00050430, VF1: 97.5236, VLoss: 0.00103132,\n",
      "Train: train_maldozer [0088] TF1: 99.1087, Tloss: 0.00053813, VF1: 97.0093, VLoss: 0.00143349,\n",
      "Train: train_maldozer [0089] TF1: 99.2331, Tloss: 0.00046772, VF1: 97.6543, VLoss: 0.00102386,\n",
      "Train: train_maldozer [0090] TF1: 99.2635, Tloss: 0.00043240, VF1: 97.0849, VLoss: 0.00130295,\n",
      "Train: train_maldozer [0091] TF1: 99.2322, Tloss: 0.00048525, VF1: 97.4674, VLoss: 0.00107831,\n",
      "Train: train_maldozer [0092] TF1: 99.2514, Tloss: 0.00048444, VF1: 97.3637, VLoss: 0.00114096,\n",
      "Train: train_maldozer [0093] TF1: 99.0906, Tloss: 0.00050576, VF1: 97.0796, VLoss: 0.00114655,\n",
      "Train: train_maldozer [0094] TF1: 99.1830, Tloss: 0.00045024, VF1: 96.6537, VLoss: 0.00148836,\n",
      "Train: train_maldozer [0095] TF1: 99.1827, Tloss: 0.00047455, VF1: 97.2917, VLoss: 0.00121737,\n",
      "Train: train_maldozer [0096] TF1: 99.1834, Tloss: 0.00048981, VF1: 97.6578, VLoss: 0.00106969,\n",
      "Epoch    96: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_maldozer [0097] TF1: 99.2941, Tloss: 0.00045613, VF1: 97.5245, VLoss: 0.00119093,\n",
      "Train: train_maldozer [0098] TF1: 99.2196, Tloss: 0.00045297, VF1: 97.4162, VLoss: 0.00116436,\n",
      "Train: train_maldozer [0099] TF1: 99.3995, Tloss: 0.00037504, VF1: 97.5858, VLoss: 0.00123366,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_056.pth Fragment=00 score=0.974244348488697\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_034.pth Fragment=00 score=0.9742602351033643\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_029.pth Fragment=00 score=0.9753161974704202\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_035.pth Fragment=00 score=0.9719739433419179\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_009.pth Fragment=00 score=0.9710551840318546\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_024.pth Fragment=00 score=0.9742108729796828\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_075.pth Fragment=00 score=0.9777298850574714\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_038.pth Fragment=00 score=0.9727217000408664\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_089.pth Fragment=00 score=0.9771801140994295\n",
      "Evaluate: ModelPath=./traces/detectionWS02/model_train_maldozer_036.pth Fragment=00 score=0.9747856005751553\n"
     ]
    }
   ],
   "source": [
    "dataset_metaList = glob.glob('/ws/mnt/local/data/output/meta/*')\n",
    "for datasetMetaPath in dataset_metaList:\n",
    "\n",
    "    currentTag = datasetMetaPath.split('/')[-1].split('.')[0]\n",
    "\n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    #\n",
    "    dataset_df = pd.read_parquet(datasetMetaPath)\n",
    "\n",
    "    #\n",
    "    trainLoader, validLoader, testLoader = getDataloaders(dataset_df, trainPercentage=trainPercentageParam, \n",
    "                                                                      validPercentage=validPercentageParam)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "    #\n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "    #\n",
    "    evalDataset(ws, evalresult_df, probaUpperBorn = 0.8,  probaLowerBorn = 0.2)\n",
    "\n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    #\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
