{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def extendDataset(ws, result_df, probaUpperBorn = 0.8,  probaLowerBorn = 0.2):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    extend_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })\n",
    "    \n",
    "    rest_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })    \n",
    "    \n",
    "    return extend_df, rest_df\n",
    "\n",
    "def getDataloaders(dataset_df, otest_df, ntest_df, batchSize=32, numWorkers=16, trainPercentage = 0.8):\n",
    "    rand_idx = np.random.RandomState(seed=54).permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(otest_df))\n",
    "    print(otest_df.label.value_counts())\n",
    "    print(len(ntest_df))\n",
    "    print(ntest_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    otestDataset = SampleDataset(otest_df.filePath.values, otest_df.label.values)\n",
    "    otestLoader  = DataLoader(otestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    ntestDataset = SampleDataset(ntest_df.filePath.values, ntest_df.label.values)\n",
    "    ntestLoader  = DataLoader(ntestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, otestLoader, ntestLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962411\n",
      "94083\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>sha1</th>\n",
       "      <th>md5</th>\n",
       "      <th>dex_date</th>\n",
       "      <th>apk_size</th>\n",
       "      <th>pkg_name</th>\n",
       "      <th>vercode</th>\n",
       "      <th>vt_detection</th>\n",
       "      <th>vt_scan_date</th>\n",
       "      <th>dex_size</th>\n",
       "      <th>markets</th>\n",
       "      <th>label</th>\n",
       "      <th>filePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vt_scan_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "      <td>4823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "      <td>12461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "      <td>35568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "      <td>13315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "      <td>9523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "      <td>14364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "      <td>4029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sha256   sha1    md5  dex_date  apk_size  pkg_name  vercode  \\\n",
       "vt_scan_date                                                                \n",
       "2019-01-31      4823   4823   4823      4823      4823      4823     4823   \n",
       "2019-02-28     12461  12461  12461     12461     12461     12461    12461   \n",
       "2019-03-31     35568  35568  35568     35568     35568     35568    35568   \n",
       "2019-04-30     13315  13315  13315     13315     13315     13315    13315   \n",
       "2019-05-31      9523   9523   9523      9523      9523      9523     9523   \n",
       "2019-06-30     14364  14364  14364     14364     14364     14364    14364   \n",
       "2019-07-31      4029   4029   4029      4029      4029      4029     4029   \n",
       "\n",
       "              vt_detection  vt_scan_date  dex_size  markets  label  filePath  \n",
       "vt_scan_date                                                                  \n",
       "2019-01-31            4823          4823      4823     4823   4823      4823  \n",
       "2019-02-28           12461         12461     12461    12461  12461     12461  \n",
       "2019-03-31           35568         35568     35568    35568  35568     35568  \n",
       "2019-04-30           13315         13315     13315    13315  13315     13315  \n",
       "2019-05-31            9523          9523      9523     9523   9523      9523  \n",
       "2019-06-30           14364         14364     14364    14364  14364     14364  \n",
       "2019-07-31            4029          4029      4029     4029   4029      4029  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdataset_df = pd.read_parquet('dataset/mdataset.parquet')\n",
    "print(len(mdataset_df))\n",
    "\n",
    "mdataset_df = mdataset_df[mdataset_df.vt_scan_date.dt.year == 2019]\n",
    "print(len(mdataset_df))\n",
    "\n",
    "malware_overtime = mdataset_df.resample('1m', on='vt_scan_date', convention='end')\n",
    "print(len(malware_overtime.count()))\n",
    "malware_overtime.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ws               = 'ws063'\n",
    "epochNum         = 30\n",
    "dataset_rootDir  = '/ws/mnt/local/data/zoo/'\n",
    "device           = torch.device('cuda:7')\n",
    "ensembleSize     = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lst     = list()\n",
    "overtime_result = list()\n",
    "initial_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[0], \n",
    "                            'label'   : malware_overtime['label'].apply(list).iloc[0] })\n",
    "\n",
    "dataset_lst.append(initial_df)\n",
    "timeTags = list(malware_overtime.count().index)\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 20190228\n",
      "3858\n",
      "0    3534\n",
      "1     324\n",
      "Name: label, dtype: int64\n",
      "965\n",
      "0    886\n",
      "1     79\n",
      "Name: label, dtype: int64\n",
      "4823\n",
      "0    4420\n",
      "1     403\n",
      "Name: label, dtype: int64\n",
      "12461\n",
      "0    8233\n",
      "1    4228\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20190228 [0000] TF1: 58.3016, Tloss: 0.00924130, VF1: 30.9278, VLoss: 0.00295058,\n",
      "Train: train_20190228 [0001] TF1: 73.8854, Tloss: 0.00431721, VF1: 64.6154, VLoss: 0.00222544,\n",
      "Train: train_20190228 [0002] TF1: 78.0000, Tloss: 0.00336756, VF1: 51.7857, VLoss: 0.00217278,\n",
      "Train: train_20190228 [0003] TF1: 81.3115, Tloss: 0.00286925, VF1: 70.7071, VLoss: 0.00333887,\n",
      "Train: train_20190228 [0004] TF1: 83.7971, Tloss: 0.00249497, VF1: 76.8212, VLoss: 0.00176929,\n",
      "Train: train_20190228 [0005] TF1: 83.4138, Tloss: 0.00237065, VF1: 67.3077, VLoss: 0.00355450,\n",
      "Train: train_20190228 [0006] TF1: 88.3797, Tloss: 0.00201540, VF1: 38.2872, VLoss: 0.00972945,\n",
      "Train: train_20190228 [0007] TF1: 89.4155, Tloss: 0.00202597, VF1: 78.3626, VLoss: 0.00222841,\n",
      "Train: train_20190228 [0008] TF1: 86.8590, Tloss: 0.00194722, VF1: 41.3408, VLoss: 0.00895031,\n",
      "Train: train_20190228 [0009] TF1: 88.6400, Tloss: 0.00184080, VF1: 48.1481, VLoss: 0.00299884,\n",
      "Train: train_20190228 [0010] TF1: 88.6792, Tloss: 0.00169674, VF1: 77.0950, VLoss: 0.00225036,\n",
      "Train: train_20190228 [0011] TF1: 91.4557, Tloss: 0.00148442, VF1: 77.9874, VLoss: 0.00221732,\n",
      "Train: train_20190228 [0012] TF1: 89.0650, Tloss: 0.00171274, VF1: 68.4564, VLoss: 0.00252711,\n",
      "Train: train_20190228 [0013] TF1: 88.6076, Tloss: 0.00164700, VF1: 71.7949, VLoss: 0.00309577,\n",
      "Train: train_20190228 [0014] TF1: 91.3112, Tloss: 0.00149429, VF1: 60.0000, VLoss: 0.00559142,\n",
      "Train: train_20190228 [0015] TF1: 91.6006, Tloss: 0.00138787, VF1: 60.3448, VLoss: 0.00249087,\n",
      "Train: train_20190228 [0016] TF1: 89.2063, Tloss: 0.00155061, VF1: 83.2298, VLoss: 0.00145616,\n",
      "Train: train_20190228 [0017] TF1: 92.5515, Tloss: 0.00128346, VF1: 65.5738, VLoss: 0.00234757,\n",
      "Train: train_20190228 [0018] TF1: 91.8919, Tloss: 0.00138044, VF1: 55.6452, VLoss: 0.00483111,\n",
      "Train: train_20190228 [0019] TF1: 90.8517, Tloss: 0.00135912, VF1: 72.9560, VLoss: 0.00204976,\n",
      "Train: train_20190228 [0020] TF1: 93.7600, Tloss: 0.00110014, VF1: 80.8219, VLoss: 0.00146148,\n",
      "Train: train_20190228 [0021] TF1: 91.3658, Tloss: 0.00127630, VF1: 72.5926, VLoss: 0.00181836,\n",
      "Train: train_20190228 [0022] TF1: 91.8919, Tloss: 0.00131672, VF1: 18.1818, VLoss: 0.00730565,\n",
      "Train: train_20190228 [0023] TF1: 92.9797, Tloss: 0.00111896, VF1: 71.2121, VLoss: 0.00245297,\n",
      "Train: train_20190228 [0024] TF1: 93.7107, Tloss: 0.00111376, VF1: 79.0419, VLoss: 0.00169568,\n",
      "Train: train_20190228 [0025] TF1: 94.0625, Tloss: 0.00105850, VF1: 75.4098, VLoss: 0.00202120,\n",
      "Train: train_20190228 [0026] TF1: 91.4826, Tloss: 0.00104055, VF1: 78.7879, VLoss: 0.00170190,\n",
      "Train: train_20190228 [0027] TF1: 92.4290, Tloss: 0.00101627, VF1: 72.7273, VLoss: 0.00273278,\n",
      "Train: train_20190228 [0028] TF1: 92.9577, Tloss: 0.00115791, VF1: 73.5294, VLoss: 0.00220791,\n",
      "Train: train_20190228 [0029] TF1: 94.1915, Tloss: 0.00101094, VF1: 56.7901, VLoss: 0.00431442,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_016.pth Fragment=00 score=0.9163195759182128\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_020.pth Fragment=00 score=0.9038560411311054\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_024.pth Fragment=00 score=0.9099225003176217\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_026.pth Fragment=00 score=0.92625\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_004.pth Fragment=00 score=0.9096247127904008\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_021.pth Fragment=00 score=0.8984192796061156\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_016.pth Fragment=00 score=0.9065656565656566\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_020.pth Fragment=00 score=0.8832214765100671\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_024.pth Fragment=00 score=0.9325\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_026.pth Fragment=00 score=0.905\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_004.pth Fragment=00 score=0.8418230563002681\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190228_021.pth Fragment=00 score=0.8653846153846153\n",
      "Extend: f1score=91.4322, vcoverage=91.8546, vf1score=95.1911, vexentdSize=11446, ecoverage=8.1454, ef1score=73.5918, erestSize=1015\n",
      "Extend: f1score=92.7835, vcoverage=93.4066, vf1score=97.1867, vexentdSize=4505, ecoverage=6.5934, ef1score=88.3117, erestSize=318\n",
      "########\n",
      "######## 20190331\n",
      "13015\n",
      "0    9967\n",
      "1    3048\n",
      "Name: label, dtype: int64\n",
      "3254\n",
      "0    2504\n",
      "1     750\n",
      "Name: label, dtype: int64\n",
      "12461\n",
      "0    8233\n",
      "1    4228\n",
      "Name: label, dtype: int64\n",
      "35568\n",
      "0    33694\n",
      "1     1874\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20190331 [0000] TF1: 93.0640, Tloss: 0.00329008, VF1: 92.9222, VLoss: 0.00136855,\n",
      "Train: train_20190331 [0001] TF1: 94.9566, Tloss: 0.00230321, VF1: 93.2051, VLoss: 0.00174280,\n",
      "Train: train_20190331 [0002] TF1: 96.0732, Tloss: 0.00188226, VF1: 95.4116, VLoss: 0.00106403,\n",
      "Train: train_20190331 [0003] TF1: 96.1506, Tloss: 0.00190819, VF1: 93.9522, VLoss: 0.00128584,\n",
      "Train: train_20190331 [0004] TF1: 96.3173, Tloss: 0.00178588, VF1: 95.6760, VLoss: 0.00104718,\n",
      "Train: train_20190331 [0005] TF1: 96.3295, Tloss: 0.00170257, VF1: 95.8475, VLoss: 0.00104169,\n",
      "Train: train_20190331 [0006] TF1: 96.4909, Tloss: 0.00166594, VF1: 94.4924, VLoss: 0.00133457,\n",
      "Train: train_20190331 [0007] TF1: 96.9566, Tloss: 0.00149117, VF1: 95.9782, VLoss: 0.00093657,\n",
      "Train: train_20190331 [0008] TF1: 96.6982, Tloss: 0.00152483, VF1: 96.3710, VLoss: 0.00089786,\n",
      "Train: train_20190331 [0009] TF1: 97.0481, Tloss: 0.00146017, VF1: 92.8205, VLoss: 0.00174587,\n",
      "Train: train_20190331 [0010] TF1: 96.9889, Tloss: 0.00144522, VF1: 95.6638, VLoss: 0.00094778,\n",
      "Train: train_20190331 [0011] TF1: 97.5303, Tloss: 0.00130510, VF1: 94.1482, VLoss: 0.00136353,\n",
      "Train: train_20190331 [0012] TF1: 96.9466, Tloss: 0.00143793, VF1: 95.2761, VLoss: 0.00108945,\n",
      "Train: train_20190331 [0013] TF1: 97.1656, Tloss: 0.00134168, VF1: 96.1126, VLoss: 0.00105172,\n",
      "Train: train_20190331 [0014] TF1: 97.4784, Tloss: 0.00128822, VF1: 95.8362, VLoss: 0.00110495,\n",
      "Train: train_20190331 [0015] TF1: 97.1997, Tloss: 0.00130264, VF1: 96.3365, VLoss: 0.00095846,\n",
      "Train: train_20190331 [0016] TF1: 97.6159, Tloss: 0.00121472, VF1: 96.2008, VLoss: 0.00102600,\n",
      "Train: train_20190331 [0017] TF1: 97.2015, Tloss: 0.00137060, VF1: 96.3563, VLoss: 0.00085230,\n",
      "Train: train_20190331 [0018] TF1: 97.5674, Tloss: 0.00115457, VF1: 94.9968, VLoss: 0.00136672,\n",
      "Train: train_20190331 [0019] TF1: 97.3501, Tloss: 0.00120058, VF1: 95.7916, VLoss: 0.00103188,\n",
      "Train: train_20190331 [0020] TF1: 97.7307, Tloss: 0.00119129, VF1: 96.6216, VLoss: 0.00088310,\n",
      "Train: train_20190331 [0021] TF1: 97.7329, Tloss: 0.00111862, VF1: 95.7880, VLoss: 0.00099103,\n",
      "Train: train_20190331 [0022] TF1: 97.3994, Tloss: 0.00118709, VF1: 95.5923, VLoss: 0.00120221,\n",
      "Train: train_20190331 [0023] TF1: 97.6613, Tloss: 0.00109603, VF1: 95.2005, VLoss: 0.00122921,\n",
      "Train: train_20190331 [0024] TF1: 97.3484, Tloss: 0.00124608, VF1: 96.0482, VLoss: 0.00098391,\n",
      "Train: train_20190331 [0025] TF1: 97.7991, Tloss: 0.00104392, VF1: 86.3503, VLoss: 0.00311972,\n",
      "Train: train_20190331 [0026] TF1: 97.5472, Tloss: 0.00122365, VF1: 95.7766, VLoss: 0.00108870,\n",
      "Train: train_20190331 [0027] TF1: 97.7307, Tloss: 0.00105749, VF1: 95.7633, VLoss: 0.00100779,\n",
      "Train: train_20190331 [0028] TF1: 98.0801, Tloss: 0.00105951, VF1: 96.0162, VLoss: 0.00094021,\n",
      "Train: train_20190331 [0029] TF1: 97.7314, Tloss: 0.00103575, VF1: 96.4358, VLoss: 0.00095808,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_017.pth Fragment=00 score=0.8706966657630795\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_020.pth Fragment=00 score=0.8619623278043296\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_008.pth Fragment=00 score=0.8760107816711591\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_007.pth Fragment=00 score=0.8547453703703705\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_028.pth Fragment=00 score=0.8603628367234745\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_010.pth Fragment=00 score=0.8511198945981554\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_017.pth Fragment=00 score=0.9631604459524963\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_020.pth Fragment=00 score=0.9587704318126372\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_008.pth Fragment=00 score=0.9649355344017351\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_007.pth Fragment=00 score=0.9490814942670447\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_028.pth Fragment=00 score=0.9638700290979632\n",
      "Evaluate: ModelPath=./traces/ws063/model_train_20190331_010.pth Fragment=00 score=0.9645048730598003\n",
      "Extend: f1score=89.1406, vcoverage=95.7012, vf1score=95.5099, vexentdSize=34039, ecoverage=4.2988, ef1score=76.8608, erestSize=1529\n",
      "Extend: f1score=96.7906, vcoverage=93.0262, vf1score=99.4955, vexentdSize=11592, ecoverage=6.9738, ef1score=79.5718, erestSize=869\n",
      "########\n",
      "######## 20190430\n",
      "40246\n",
      "0    36228\n",
      "1     4018\n",
      "Name: label, dtype: int64\n",
      "10062\n",
      "0    9042\n",
      "1    1020\n",
      "Name: label, dtype: int64\n",
      "35568\n",
      "0    33694\n",
      "1     1874\n",
      "Name: label, dtype: int64\n",
      "13315\n",
      "0    11267\n",
      "1     2048\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20190430 [0000] TF1: 91.3336, Tloss: 0.00219175, VF1: 95.8959, VLoss: 0.00065446,\n",
      "Train: train_20190430 [0001] TF1: 94.5825, Tloss: 0.00130306, VF1: 95.7490, VLoss: 0.00052142,\n",
      "Train: train_20190430 [0002] TF1: 94.9701, Tloss: 0.00119606, VF1: 94.4931, VLoss: 0.00053971,\n",
      "Train: train_20190430 [0003] TF1: 95.3972, Tloss: 0.00113558, VF1: 92.9044, VLoss: 0.00127931,\n",
      "Train: train_20190430 [0004] TF1: 95.2235, Tloss: 0.00112314, VF1: 91.1941, VLoss: 0.00122142,\n",
      "Train: train_20190430 [0005] TF1: 95.1095, Tloss: 0.00112708, VF1: 95.5994, VLoss: 0.00050546,\n",
      "Train: train_20190430 [0006] TF1: 95.3642, Tloss: 0.00109055, VF1: 96.8734, VLoss: 0.00039777,\n",
      "Train: train_20190430 [0007] TF1: 95.5649, Tloss: 0.00107353, VF1: 94.8179, VLoss: 0.00059184,\n",
      "Train: train_20190430 [0008] TF1: 95.5383, Tloss: 0.00108049, VF1: 95.6256, VLoss: 0.00051934,\n",
      "Train: train_20190430 [0009] TF1: 95.5471, Tloss: 0.00102000, VF1: 94.2268, VLoss: 0.00058742,\n",
      "Train: train_20190430 [0010] TF1: 95.7052, Tloss: 0.00098200, VF1: 96.0040, VLoss: 0.00046057,\n",
      "Train: train_20190430 [0011] TF1: 95.3909, Tloss: 0.00101851, VF1: 89.7019, VLoss: 0.00106614,\n",
      "Train: train_20190430 [0012] TF1: 95.6898, Tloss: 0.00097894, VF1: 96.7710, VLoss: 0.00042827,\n",
      "Train: train_20190430 [0013] TF1: 95.9980, Tloss: 0.00095561, VF1: 95.8293, VLoss: 0.00047904,\n",
      "Train: train_20190430 [0014] TF1: 95.8890, Tloss: 0.00096818, VF1: 96.0435, VLoss: 0.00056607,\n",
      "Train: train_20190430 [0015] TF1: 95.8354, Tloss: 0.00097081, VF1: 96.7229, VLoss: 0.00043646,\n",
      "Train: train_20190430 [0016] TF1: 95.8005, Tloss: 0.00096140, VF1: 96.6060, VLoss: 0.00043744,\n",
      "Train: train_20190430 [0017] TF1: 95.9645, Tloss: 0.00090688, VF1: 93.8990, VLoss: 0.00066358,\n",
      "Train: train_20190430 [0018] TF1: 96.3424, Tloss: 0.00088891, VF1: 96.6169, VLoss: 0.00043751,\n",
      "Train: train_20190430 [0019] TF1: 96.0518, Tloss: 0.00093110, VF1: 96.2854, VLoss: 0.00052794,\n",
      "Train: train_20190430 [0020] TF1: 96.0924, Tloss: 0.00088987, VF1: 96.6617, VLoss: 0.00040277,\n",
      "Train: train_20190430 [0021] TF1: 96.3424, Tloss: 0.00085699, VF1: 96.1170, VLoss: 0.00045118,\n",
      "Train: train_20190430 [0022] TF1: 96.1948, Tloss: 0.00090175, VF1: 95.8718, VLoss: 0.00049869,\n",
      "Train: train_20190430 [0023] TF1: 96.3076, Tloss: 0.00086312, VF1: 95.9070, VLoss: 0.00044656,\n",
      "Train: train_20190430 [0024] TF1: 96.1797, Tloss: 0.00083248, VF1: 96.6337, VLoss: 0.00042451,\n",
      "Train: train_20190430 [0025] TF1: 96.5805, Tloss: 0.00078398, VF1: 96.2598, VLoss: 0.00047360,\n",
      "Train: train_20190430 [0026] TF1: 96.4748, Tloss: 0.00085649, VF1: 96.5213, VLoss: 0.00042834,\n",
      "Train: train_20190430 [0027] TF1: 96.4213, Tloss: 0.00082923, VF1: 97.1684, VLoss: 0.00041538,\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(timeTags)):\n",
    "    currentTag = timeTags[idx].isoformat().split('T')[0].replace('-', '')\n",
    "    \n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #\n",
    "    otest_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[idx-1], \n",
    "                              'label'   : malware_overtime['label'].apply(list).iloc[idx-1] })\n",
    "    #\n",
    "    ntest_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[idx], \n",
    "                              'label'   : malware_overtime['label'].apply(list).iloc[idx] })\n",
    "    \n",
    "    dataset_df = pd.concat(dataset_lst)\n",
    "    trainLoader, validLoader, otestLoader, ntestLoader = getDataloaders(dataset_df, otest_df, ntest_df, trainPercentage=0.8)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "    \n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, ntestLoader, device)\n",
    "    exresult_df   = evaluate(ws, selectedModelPaths, otestLoader, device)\n",
    "    \n",
    "    extend_df, _  = extendDataset(ws, evalresult_df, probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "    _, rest_df    = extendDataset(ws, exresult_df,   probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "\n",
    "    #\n",
    "    dataset_lst.append(extend_df)\n",
    "\n",
    "    #\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df, exresult_df, dataset_lst, rest_df)], \n",
    "                                     columns=['TimeTag', 'models', 'evalResuls', \n",
    "                                              'extendResults', 'datasetList', \n",
    "                                              'restDataset'])\n",
    "    \n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
