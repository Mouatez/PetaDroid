{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def getDataloaders(dataset_df, test_df, batchSize=32, numWorkers=16, trainPercentage=0.7, validPercentage=0.8):\n",
    "    rand_idx = np.random.permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "    #test_df  = dataset_df.iloc[rand_idx[int(validPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(test_df))\n",
    "    print(test_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    testDataset = SampleDataset(test_df.filePath.values, test_df.label.values)\n",
    "    testLoader  = DataLoader(testDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, testLoader\n",
    "\n",
    "def evalDataset(ws, result_df, probaUpperBorn = 0.9,  probaLowerBorn = 0.1):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ws               = 'detectionWS04'\n",
    "epochNum         = 50\n",
    "device           = torch.device('cuda:0')\n",
    "ensembleSize     = 10\n",
    "\n",
    "trainPercentageParam = 0.8\n",
    "validPercentageParam = 0.2\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedf = pd.read_parquet('dataset/timedf.parquet')\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 2013\n",
      "8000\n",
      "1    4015\n",
      "0    3985\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "0    1015\n",
      "1     985\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2013 [0000] TF1: 94.6596, Tloss: 0.00453242, VF1: 97.3940, VLoss: 0.00187592,\n",
      "Train: train_2013 [0001] TF1: 97.2973, Tloss: 0.00246892, VF1: 95.8267, VLoss: 0.00184903,\n",
      "Train: train_2013 [0002] TF1: 97.8228, Tloss: 0.00191648, VF1: 89.6976, VLoss: 0.00404076,\n",
      "Train: train_2013 [0003] TF1: 98.1401, Tloss: 0.00171805, VF1: 84.7953, VLoss: 0.00515232,\n",
      "Train: train_2013 [0004] TF1: 98.6486, Tloss: 0.00133888, VF1: 96.3741, VLoss: 0.00154710,\n",
      "Train: train_2013 [0005] TF1: 98.4262, Tloss: 0.00129920, VF1: 97.4104, VLoss: 0.00119129,\n",
      "Train: train_2013 [0006] TF1: 98.9650, Tloss: 0.00102058, VF1: 98.0471, VLoss: 0.00094477,\n",
      "Train: train_2013 [0007] TF1: 98.8526, Tloss: 0.00098519, VF1: 96.5951, VLoss: 0.00156396,\n",
      "Train: train_2013 [0008] TF1: 98.8271, Tloss: 0.00116078, VF1: 98.5405, VLoss: 0.00068746,\n",
      "Train: train_2013 [0009] TF1: 98.6643, Tloss: 0.00118249, VF1: 98.2932, VLoss: 0.00088113,\n",
      "Train: train_2013 [0010] TF1: 99.1898, Tloss: 0.00081106, VF1: 98.5758, VLoss: 0.00068680,\n",
      "Train: train_2013 [0011] TF1: 99.0647, Tloss: 0.00083115, VF1: 98.2060, VLoss: 0.00098266,\n",
      "Train: train_2013 [0012] TF1: 99.0143, Tloss: 0.00088112, VF1: 97.9887, VLoss: 0.00088880,\n",
      "Train: train_2013 [0013] TF1: 99.1146, Tloss: 0.00082395, VF1: 97.7320, VLoss: 0.00091473,\n",
      "Train: train_2013 [0014] TF1: 99.2777, Tloss: 0.00066864, VF1: 97.8000, VLoss: 0.00096903,\n",
      "Train: train_2013 [0015] TF1: 99.3141, Tloss: 0.00065985, VF1: 96.5517, VLoss: 0.00159408,\n",
      "Train: train_2013 [0016] TF1: 99.1150, Tloss: 0.00078110, VF1: 98.4600, VLoss: 0.00081114,\n",
      "Train: train_2013 [0017] TF1: 98.9524, Tloss: 0.00100964, VF1: 94.5513, VLoss: 0.00222663,\n",
      "Train: train_2013 [0018] TF1: 99.2888, Tloss: 0.00067739, VF1: 98.8753, VLoss: 0.00071415,\n",
      "Train: train_2013 [0019] TF1: 99.2901, Tloss: 0.00067372, VF1: 97.4129, VLoss: 0.00110904,\n",
      "Train: train_2013 [0020] TF1: 99.4644, Tloss: 0.00052927, VF1: 96.9817, VLoss: 0.00131659,\n",
      "Train: train_2013 [0021] TF1: 99.2016, Tloss: 0.00064793, VF1: 98.3137, VLoss: 0.00091359,\n",
      "Train: train_2013 [0022] TF1: 99.2401, Tloss: 0.00080204, VF1: 98.2635, VLoss: 0.00144316,\n",
      "Train: train_2013 [0023] TF1: 99.4894, Tloss: 0.00049238, VF1: 98.1055, VLoss: 0.00083073,\n",
      "Train: train_2013 [0024] TF1: 99.2269, Tloss: 0.00066008, VF1: 98.2170, VLoss: 0.00083404,\n",
      "Train: train_2013 [0025] TF1: 99.2770, Tloss: 0.00064000, VF1: 97.6602, VLoss: 0.00111190,\n",
      "Train: train_2013 [0026] TF1: 99.4022, Tloss: 0.00054446, VF1: 97.4066, VLoss: 0.00152704,\n",
      "Train: train_2013 [0027] TF1: 99.4143, Tloss: 0.00062333, VF1: 95.9334, VLoss: 0.00200101,\n",
      "Train: train_2013 [0028] TF1: 99.2034, Tloss: 0.00065181, VF1: 96.4567, VLoss: 0.00147097,\n",
      "Train: train_2013 [0029] TF1: 99.4400, Tloss: 0.00048463, VF1: 90.9192, VLoss: 0.00571878,\n",
      "Train: train_2013 [0030] TF1: 99.3270, Tloss: 0.00062269, VF1: 97.7915, VLoss: 0.00111500,\n",
      "Train: train_2013 [0031] TF1: 99.4269, Tloss: 0.00051117, VF1: 97.7642, VLoss: 0.00085770,\n",
      "Train: train_2013 [0032] TF1: 99.4518, Tloss: 0.00046887, VF1: 98.6246, VLoss: 0.00071105,\n",
      "Train: train_2013 [0033] TF1: 99.7258, Tloss: 0.00036865, VF1: 98.4725, VLoss: 0.00076704,\n",
      "Train: train_2013 [0034] TF1: 99.4896, Tloss: 0.00047293, VF1: 95.8498, VLoss: 0.00212452,\n",
      "Train: train_2013 [0035] TF1: 99.3019, Tloss: 0.00064684, VF1: 97.9859, VLoss: 0.00119142,\n",
      "Train: train_2013 [0036] TF1: 99.5012, Tloss: 0.00054495, VF1: 98.5302, VLoss: 0.00079525,\n",
      "Train: train_2013 [0037] TF1: 99.5515, Tloss: 0.00045627, VF1: 96.0464, VLoss: 0.00249811,\n",
      "Train: train_2013 [0038] TF1: 99.5143, Tloss: 0.00047730, VF1: 97.9859, VLoss: 0.00102243,\n",
      "Train: train_2013 [0039] TF1: 99.5268, Tloss: 0.00045781, VF1: 97.9449, VLoss: 0.00122989,\n",
      "Epoch    39: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2013 [0040] TF1: 99.5764, Tloss: 0.00038970, VF1: 96.8348, VLoss: 0.00203249,\n",
      "Train: train_2013 [0041] TF1: 99.5393, Tloss: 0.00048509, VF1: 98.3051, VLoss: 0.00097161,\n",
      "Train: train_2013 [0042] TF1: 99.6511, Tloss: 0.00036515, VF1: 98.0313, VLoss: 0.00113133,\n",
      "Train: train_2013 [0043] TF1: 99.5513, Tloss: 0.00036558, VF1: 98.3505, VLoss: 0.00122818,\n",
      "Train: train_2013 [0044] TF1: 99.5643, Tloss: 0.00042440, VF1: 98.3822, VLoss: 0.00077251,\n",
      "Train: train_2013 [0045] TF1: 99.8008, Tloss: 0.00020009, VF1: 97.5635, VLoss: 0.00146133,\n",
      "Train: train_2013 [0046] TF1: 99.6264, Tloss: 0.00033370, VF1: 96.2185, VLoss: 0.00218193,\n",
      "Train: train_2013 [0047] TF1: 99.6260, Tloss: 0.00032034, VF1: 97.8877, VLoss: 0.00111993,\n",
      "Train: train_2013 [0048] TF1: 99.5392, Tloss: 0.00043803, VF1: 93.4606, VLoss: 0.00398047,\n",
      "Train: train_2013 [0049] TF1: 99.6138, Tloss: 0.00040364, VF1: 98.6667, VLoss: 0.00097728,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_010.pth Fragment=00 score=0.7540345940577672\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_008.pth Fragment=00 score=0.7853432655526476\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_032.pth Fragment=00 score=0.7301339191415657\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_018.pth Fragment=00 score=0.7099502060439561\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_033.pth Fragment=00 score=0.7642426479635782\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_044.pth Fragment=00 score=0.774998986664505\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_036.pth Fragment=00 score=0.7588104822147375\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_016.pth Fragment=00 score=0.7164083514794415\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_023.pth Fragment=00 score=0.7309923020141306\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2013_024.pth Fragment=00 score=0.744587301256538\n",
      "Extend: f1score=74.8162, vcoverage=90.5350, vf1score=76.8907, vexentdSize=54321, ecoverage=9.4650, ef1score=64.6712, erestSize=5679\n",
      "########\n",
      "######## 2014\n",
      "8000\n",
      "0    4025\n",
      "1    3975\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "1    1025\n",
      "0     975\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2014 [0000] TF1: 95.1705, Tloss: 0.00427603, VF1: 23.1234, VLoss: 0.02153133,\n",
      "Train: train_2014 [0001] TF1: 97.6362, Tloss: 0.00221223, VF1: 81.1594, VLoss: 0.00725623,\n",
      "Train: train_2014 [0002] TF1: 97.9432, Tloss: 0.00190375, VF1: 97.1569, VLoss: 0.00126757,\n",
      "Train: train_2014 [0003] TF1: 98.2695, Tloss: 0.00155857, VF1: 96.7033, VLoss: 0.00133600,\n",
      "Train: train_2014 [0004] TF1: 98.4724, Tloss: 0.00140473, VF1: 95.3184, VLoss: 0.00239162,\n",
      "Train: train_2014 [0005] TF1: 98.4984, Tloss: 0.00140474, VF1: 97.0743, VLoss: 0.00163200,\n",
      "Train: train_2014 [0006] TF1: 98.8022, Tloss: 0.00109301, VF1: 97.8828, VLoss: 0.00099478,\n",
      "Train: train_2014 [0007] TF1: 99.0406, Tloss: 0.00095291, VF1: 97.8661, VLoss: 0.00100518,\n",
      "Train: train_2014 [0008] TF1: 98.7766, Tloss: 0.00119902, VF1: 97.3494, VLoss: 0.00132558,\n",
      "Train: train_2014 [0009] TF1: 98.8398, Tloss: 0.00108158, VF1: 97.1513, VLoss: 0.00128079,\n",
      "Train: train_2014 [0010] TF1: 98.6377, Tloss: 0.00103460, VF1: 95.1937, VLoss: 0.00251632,\n",
      "Train: train_2014 [0011] TF1: 99.1808, Tloss: 0.00081621, VF1: 97.6859, VLoss: 0.00120526,\n",
      "Train: train_2014 [0012] TF1: 99.1429, Tloss: 0.00095499, VF1: 93.7874, VLoss: 0.00339857,\n",
      "Train: train_2014 [0013] TF1: 99.0042, Tloss: 0.00094761, VF1: 98.1022, VLoss: 0.00111134,\n",
      "Train: train_2014 [0014] TF1: 99.0930, Tloss: 0.00085753, VF1: 94.0268, VLoss: 0.00287010,\n",
      "Train: train_2014 [0015] TF1: 99.2940, Tloss: 0.00074989, VF1: 96.3103, VLoss: 0.00195697,\n",
      "Train: train_2014 [0016] TF1: 99.1940, Tloss: 0.00077902, VF1: 97.7218, VLoss: 0.00105721,\n",
      "Train: train_2014 [0017] TF1: 98.9792, Tloss: 0.00089252, VF1: 93.3517, VLoss: 0.00303082,\n",
      "Train: train_2014 [0018] TF1: 99.2568, Tloss: 0.00071940, VF1: 98.0873, VLoss: 0.00102063,\n",
      "Train: train_2014 [0019] TF1: 99.3579, Tloss: 0.00058781, VF1: 98.1818, VLoss: 0.00099692,\n",
      "Train: train_2014 [0020] TF1: 99.3574, Tloss: 0.00072589, VF1: 97.9157, VLoss: 0.00110905,\n",
      "Train: train_2014 [0021] TF1: 99.2574, Tloss: 0.00072815, VF1: 97.3134, VLoss: 0.00162149,\n",
      "Train: train_2014 [0022] TF1: 99.1815, Tloss: 0.00066780, VF1: 89.6280, VLoss: 0.00634092,\n",
      "Train: train_2014 [0023] TF1: 99.1686, Tloss: 0.00073578, VF1: 97.8764, VLoss: 0.00121314,\n",
      "Train: train_2014 [0024] TF1: 99.1802, Tloss: 0.00071295, VF1: 97.5826, VLoss: 0.00130388,\n",
      "Train: train_2014 [0025] TF1: 99.4837, Tloss: 0.00052109, VF1: 98.4885, VLoss: 0.00087088,\n",
      "Train: train_2014 [0026] TF1: 99.2570, Tloss: 0.00081128, VF1: 97.9512, VLoss: 0.00106563,\n",
      "Train: train_2014 [0027] TF1: 99.2693, Tloss: 0.00067422, VF1: 96.7320, VLoss: 0.00178726,\n",
      "Train: train_2014 [0028] TF1: 99.4456, Tloss: 0.00052379, VF1: 95.3157, VLoss: 0.00301578,\n",
      "Train: train_2014 [0029] TF1: 99.3448, Tloss: 0.00066459, VF1: 97.9671, VLoss: 0.00105069,\n",
      "Train: train_2014 [0030] TF1: 99.2318, Tloss: 0.00068739, VF1: 93.2362, VLoss: 0.00344896,\n",
      "Train: train_2014 [0031] TF1: 99.4084, Tloss: 0.00058969, VF1: 96.9871, VLoss: 0.00152510,\n",
      "Epoch    31: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2014 [0032] TF1: 99.4581, Tloss: 0.00053779, VF1: 97.8131, VLoss: 0.00140900,\n",
      "Train: train_2014 [0033] TF1: 99.4710, Tloss: 0.00048570, VF1: 96.9957, VLoss: 0.00181803,\n",
      "Train: train_2014 [0034] TF1: 99.4714, Tloss: 0.00048265, VF1: 96.9362, VLoss: 0.00248189,\n",
      "Train: train_2014 [0035] TF1: 99.4965, Tloss: 0.00043364, VF1: 97.5303, VLoss: 0.00122791,\n",
      "Train: train_2014 [0036] TF1: 99.6097, Tloss: 0.00033739, VF1: 97.1684, VLoss: 0.00167868,\n",
      "Train: train_2014 [0037] TF1: 99.6095, Tloss: 0.00043106, VF1: 97.8325, VLoss: 0.00143063,\n",
      "Train: train_2014 [0038] TF1: 99.4584, Tloss: 0.00047814, VF1: 97.1058, VLoss: 0.00189325,\n",
      "Train: train_2014 [0039] TF1: 99.6094, Tloss: 0.00040095, VF1: 71.7500, VLoss: 0.01478280,\n",
      "Train: train_2014 [0040] TF1: 99.4965, Tloss: 0.00049862, VF1: 97.8208, VLoss: 0.00137676,\n",
      "Train: train_2014 [0041] TF1: 99.5217, Tloss: 0.00046000, VF1: 97.1712, VLoss: 0.00165577,\n",
      "Train: train_2014 [0042] TF1: 99.5597, Tloss: 0.00039159, VF1: 96.3490, VLoss: 0.00219886,\n",
      "Epoch    42: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2014 [0043] TF1: 99.5718, Tloss: 0.00033240, VF1: 98.1942, VLoss: 0.00096224,\n",
      "Train: train_2014 [0044] TF1: 99.6098, Tloss: 0.00034389, VF1: 98.1445, VLoss: 0.00121589,\n",
      "Train: train_2014 [0045] TF1: 99.5094, Tloss: 0.00039048, VF1: 98.0257, VLoss: 0.00127161,\n",
      "Train: train_2014 [0046] TF1: 99.5845, Tloss: 0.00040778, VF1: 96.7064, VLoss: 0.00208709,\n",
      "Train: train_2014 [0047] TF1: 99.6102, Tloss: 0.00034897, VF1: 97.7623, VLoss: 0.00161599,\n",
      "Train: train_2014 [0048] TF1: 99.7607, Tloss: 0.00030677, VF1: 98.3784, VLoss: 0.00122372,\n",
      "Train: train_2014 [0049] TF1: 99.6602, Tloss: 0.00034948, VF1: 97.6744, VLoss: 0.00132535,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_025.pth Fragment=00 score=0.8220179936151688\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_043.pth Fragment=00 score=0.8188047498540004\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_006.pth Fragment=00 score=0.8023366336633664\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_019.pth Fragment=00 score=0.8065373633947032\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_007.pth Fragment=00 score=0.8312708325351519\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_018.pth Fragment=00 score=0.8070796460176992\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_029.pth Fragment=00 score=0.8492544312107287\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_016.pth Fragment=00 score=0.8345378615968422\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_026.pth Fragment=00 score=0.8552423305947087\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2014_020.pth Fragment=00 score=0.7962532960761934\n",
      "Extend: f1score=82.0356, vcoverage=91.3967, vf1score=85.2881, vexentdSize=54838, ecoverage=8.6033, ef1score=60.4955, erestSize=5162\n",
      "########\n",
      "######## 2015\n",
      "8000\n",
      "1    4033\n",
      "0    3967\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "0    1033\n",
      "1     967\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2015 [0000] TF1: 94.1382, Tloss: 0.00507300, VF1: 86.3799, VLoss: 0.00531907,\n",
      "Train: train_2015 [0001] TF1: 96.4281, Tloss: 0.00303865, VF1: 88.2488, VLoss: 0.00409117,\n",
      "Train: train_2015 [0002] TF1: 97.1051, Tloss: 0.00260974, VF1: 94.7159, VLoss: 0.00216169,\n",
      "Train: train_2015 [0003] TF1: 97.4888, Tloss: 0.00218483, VF1: 85.6128, VLoss: 0.00595769,\n",
      "Train: train_2015 [0004] TF1: 97.6431, Tloss: 0.00208852, VF1: 95.9021, VLoss: 0.00173266,\n",
      "Train: train_2015 [0005] TF1: 98.0353, Tloss: 0.00167515, VF1: 96.0428, VLoss: 0.00202640,\n",
      "Train: train_2015 [0006] TF1: 98.1433, Tloss: 0.00167391, VF1: 97.6042, VLoss: 0.00190984,\n",
      "Train: train_2015 [0007] TF1: 98.1958, Tloss: 0.00161833, VF1: 94.1929, VLoss: 0.00270495,\n",
      "Train: train_2015 [0008] TF1: 98.5193, Tloss: 0.00139385, VF1: 88.7486, VLoss: 0.00461598,\n",
      "Train: train_2015 [0009] TF1: 98.0724, Tloss: 0.00164889, VF1: 93.5217, VLoss: 0.00301679,\n",
      "Train: train_2015 [0010] TF1: 98.5957, Tloss: 0.00129835, VF1: 97.8033, VLoss: 0.00112433,\n",
      "Train: train_2015 [0011] TF1: 98.7078, Tloss: 0.00110090, VF1: 96.3831, VLoss: 0.00158500,\n",
      "Train: train_2015 [0012] TF1: 98.6581, Tloss: 0.00133815, VF1: 97.1963, VLoss: 0.00117892,\n",
      "Train: train_2015 [0013] TF1: 98.7706, Tloss: 0.00107899, VF1: 96.2733, VLoss: 0.00169137,\n",
      "Train: train_2015 [0014] TF1: 98.6571, Tloss: 0.00126414, VF1: 95.8186, VLoss: 0.00158125,\n",
      "Train: train_2015 [0015] TF1: 98.8691, Tloss: 0.00100657, VF1: 96.2334, VLoss: 0.00162687,\n",
      "Train: train_2015 [0016] TF1: 98.9199, Tloss: 0.00104191, VF1: 95.6740, VLoss: 0.00176605,\n",
      "Train: train_2015 [0017] TF1: 98.8945, Tloss: 0.00113174, VF1: 95.5128, VLoss: 0.00207451,\n",
      "Train: train_2015 [0018] TF1: 98.6945, Tloss: 0.00112258, VF1: 89.2571, VLoss: 0.00506996,\n",
      "Train: train_2015 [0019] TF1: 98.5957, Tloss: 0.00117571, VF1: 97.1635, VLoss: 0.00132430,\n",
      "Train: train_2015 [0020] TF1: 98.9199, Tloss: 0.00099080, VF1: 97.1780, VLoss: 0.00117811,\n",
      "Train: train_2015 [0021] TF1: 98.9076, Tloss: 0.00098910, VF1: 95.1417, VLoss: 0.00233792,\n",
      "Train: train_2015 [0022] TF1: 99.0438, Tloss: 0.00098840, VF1: 96.9072, VLoss: 0.00132523,\n",
      "Train: train_2015 [0023] TF1: 99.0942, Tloss: 0.00085606, VF1: 96.8008, VLoss: 0.00151690,\n",
      "Train: train_2015 [0024] TF1: 99.0557, Tloss: 0.00092561, VF1: 97.1757, VLoss: 0.00137132,\n",
      "Train: train_2015 [0025] TF1: 99.0320, Tloss: 0.00092779, VF1: 96.9345, VLoss: 0.00172101,\n",
      "Train: train_2015 [0026] TF1: 99.1315, Tloss: 0.00088742, VF1: 97.6240, VLoss: 0.00150137,\n",
      "Train: train_2015 [0027] TF1: 99.2302, Tloss: 0.00081175, VF1: 96.6258, VLoss: 0.00190953,\n",
      "Train: train_2015 [0028] TF1: 98.9330, Tloss: 0.00097423, VF1: 97.1164, VLoss: 0.00140885,\n",
      "Train: train_2015 [0029] TF1: 98.9939, Tloss: 0.00086357, VF1: 96.7196, VLoss: 0.00169952,\n",
      "Train: train_2015 [0030] TF1: 99.2056, Tloss: 0.00077214, VF1: 96.9759, VLoss: 0.00150482,\n",
      "Train: train_2015 [0031] TF1: 99.1315, Tloss: 0.00074828, VF1: 97.2610, VLoss: 0.00147045,\n",
      "Train: train_2015 [0032] TF1: 98.9937, Tloss: 0.00081418, VF1: 96.8321, VLoss: 0.00206436,\n",
      "Train: train_2015 [0033] TF1: 99.0573, Tloss: 0.00088786, VF1: 93.4937, VLoss: 0.00283795,\n",
      "Train: train_2015 [0034] TF1: 99.1684, Tloss: 0.00075811, VF1: 73.2929, VLoss: 0.02190163,\n",
      "Train: train_2015 [0035] TF1: 99.1315, Tloss: 0.00074870, VF1: 95.6740, VLoss: 0.00189157,\n",
      "Train: train_2015 [0036] TF1: 99.1309, Tloss: 0.00078090, VF1: 96.5799, VLoss: 0.00161894,\n",
      "Train: train_2015 [0037] TF1: 99.1438, Tloss: 0.00082755, VF1: 97.0976, VLoss: 0.00150068,\n",
      "Epoch    37: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2015 [0038] TF1: 99.2925, Tloss: 0.00061554, VF1: 97.5660, VLoss: 0.00124888,\n",
      "Train: train_2015 [0039] TF1: 99.4416, Tloss: 0.00056314, VF1: 97.5385, VLoss: 0.00124081,\n",
      "Train: train_2015 [0040] TF1: 99.4790, Tloss: 0.00050673, VF1: 97.5584, VLoss: 0.00147502,\n",
      "Train: train_2015 [0041] TF1: 99.4538, Tloss: 0.00059303, VF1: 97.5302, VLoss: 0.00164097,\n",
      "Train: train_2015 [0042] TF1: 99.3797, Tloss: 0.00053755, VF1: 97.4305, VLoss: 0.00169705,\n",
      "Train: train_2015 [0043] TF1: 99.3054, Tloss: 0.00069380, VF1: 97.4790, VLoss: 0.00185155,\n",
      "Train: train_2015 [0044] TF1: 99.3918, Tloss: 0.00060941, VF1: 96.5164, VLoss: 0.00186876,\n",
      "Train: train_2015 [0045] TF1: 99.1932, Tloss: 0.00077633, VF1: 96.8996, VLoss: 0.00193149,\n",
      "Train: train_2015 [0046] TF1: 99.4911, Tloss: 0.00055461, VF1: 96.8464, VLoss: 0.00152221,\n",
      "Epoch    46: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2015 [0047] TF1: 99.4422, Tloss: 0.00058528, VF1: 96.4229, VLoss: 0.00221821,\n",
      "Train: train_2015 [0048] TF1: 99.5037, Tloss: 0.00048760, VF1: 96.1732, VLoss: 0.00200278,\n",
      "Train: train_2015 [0049] TF1: 99.4670, Tloss: 0.00053672, VF1: 97.4936, VLoss: 0.00139248,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_010.pth Fragment=00 score=0.877329163984181\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_020.pth Fragment=00 score=0.8983727574397105\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_012.pth Fragment=00 score=0.8852702506883061\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_039.pth Fragment=00 score=0.8968549445094494\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_038.pth Fragment=00 score=0.8940593353301953\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_019.pth Fragment=00 score=0.8998117697197856\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_022.pth Fragment=00 score=0.8820919406459121\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_024.pth Fragment=00 score=0.8701652846569092\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_049.pth Fragment=00 score=0.9008704232300807\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2015_028.pth Fragment=00 score=0.894122289383531\n",
      "Extend: f1score=89.7253, vcoverage=91.3933, vf1score=92.7384, vexentdSize=54836, ecoverage=8.6067, ef1score=66.9845, erestSize=5164\n",
      "########\n",
      "######## 2016\n",
      "8000\n",
      "0    4028\n",
      "1    3972\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "1    1028\n",
      "0     972\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2016 [0000] TF1: 91.8347, Tloss: 0.00643894, VF1: 94.6951, VLoss: 0.00249352,\n",
      "Train: train_2016 [0001] TF1: 95.1547, Tloss: 0.00416629, VF1: 94.6996, VLoss: 0.00230776,\n",
      "Train: train_2016 [0002] TF1: 95.8640, Tloss: 0.00360853, VF1: 90.1109, VLoss: 0.00442157,\n",
      "Train: train_2016 [0003] TF1: 96.1201, Tloss: 0.00332856, VF1: 96.1500, VLoss: 0.00199027,\n",
      "Train: train_2016 [0004] TF1: 96.3404, Tloss: 0.00308094, VF1: 83.3994, VLoss: 0.00701549,\n",
      "Train: train_2016 [0005] TF1: 97.0096, Tloss: 0.00267870, VF1: 95.9960, VLoss: 0.00165910,\n",
      "Train: train_2016 [0006] TF1: 96.7186, Tloss: 0.00272410, VF1: 91.5663, VLoss: 0.00371319,\n",
      "Train: train_2016 [0007] TF1: 97.2391, Tloss: 0.00225550, VF1: 95.6522, VLoss: 0.00220235,\n",
      "Train: train_2016 [0008] TF1: 97.4034, Tloss: 0.00226959, VF1: 95.1466, VLoss: 0.00232772,\n",
      "Train: train_2016 [0009] TF1: 97.4248, Tloss: 0.00229214, VF1: 96.4286, VLoss: 0.00184956,\n",
      "Train: train_2016 [0010] TF1: 97.4060, Tloss: 0.00220334, VF1: 76.4398, VLoss: 0.01521961,\n",
      "Train: train_2016 [0011] TF1: 97.7603, Tloss: 0.00207772, VF1: 96.0964, VLoss: 0.00180362,\n",
      "Train: train_2016 [0012] TF1: 97.7057, Tloss: 0.00206892, VF1: 96.6875, VLoss: 0.00169249,\n",
      "Train: train_2016 [0013] TF1: 97.7868, Tloss: 0.00200361, VF1: 78.9412, VLoss: 0.00804979,\n",
      "Train: train_2016 [0014] TF1: 97.7581, Tloss: 0.00193142, VF1: 96.2890, VLoss: 0.00156928,\n",
      "Train: train_2016 [0015] TF1: 97.8745, Tloss: 0.00183752, VF1: 96.6475, VLoss: 0.00176950,\n",
      "Train: train_2016 [0016] TF1: 97.8879, Tloss: 0.00185428, VF1: 95.5450, VLoss: 0.00228921,\n",
      "Train: train_2016 [0017] TF1: 97.9117, Tloss: 0.00188214, VF1: 95.4910, VLoss: 0.00204109,\n",
      "Train: train_2016 [0018] TF1: 97.7964, Tloss: 0.00188906, VF1: 96.5022, VLoss: 0.00201689,\n",
      "Train: train_2016 [0019] TF1: 98.2296, Tloss: 0.00159460, VF1: 95.1128, VLoss: 0.00243253,\n",
      "Train: train_2016 [0020] TF1: 97.9767, Tloss: 0.00176016, VF1: 97.1262, VLoss: 0.00171592,\n",
      "Train: train_2016 [0021] TF1: 97.8368, Tloss: 0.00195386, VF1: 97.2786, VLoss: 0.00149002,\n",
      "Train: train_2016 [0022] TF1: 98.1809, Tloss: 0.00167524, VF1: 95.7639, VLoss: 0.00207319,\n",
      "Train: train_2016 [0023] TF1: 98.2163, Tloss: 0.00167379, VF1: 90.7937, VLoss: 0.00443500,\n",
      "Train: train_2016 [0024] TF1: 98.2012, Tloss: 0.00164756, VF1: 94.6629, VLoss: 0.00271587,\n",
      "Train: train_2016 [0025] TF1: 98.3814, Tloss: 0.00156135, VF1: 88.1223, VLoss: 0.00794253,\n",
      "Train: train_2016 [0026] TF1: 98.0179, Tloss: 0.00171624, VF1: 94.8223, VLoss: 0.00254850,\n",
      "Train: train_2016 [0027] TF1: 98.2301, Tloss: 0.00157414, VF1: 95.7958, VLoss: 0.00180115,\n",
      "Train: train_2016 [0028] TF1: 98.1772, Tloss: 0.00151841, VF1: 95.1368, VLoss: 0.00243969,\n",
      "Train: train_2016 [0029] TF1: 98.4954, Tloss: 0.00141846, VF1: 96.3397, VLoss: 0.00214936,\n",
      "Train: train_2016 [0030] TF1: 98.2927, Tloss: 0.00154116, VF1: 84.7230, VLoss: 0.00710455,\n",
      "Train: train_2016 [0031] TF1: 98.5229, Tloss: 0.00135794, VF1: 93.4862, VLoss: 0.00394057,\n",
      "Train: train_2016 [0032] TF1: 98.5983, Tloss: 0.00129234, VF1: 96.3627, VLoss: 0.00219888,\n",
      "Train: train_2016 [0033] TF1: 98.2296, Tloss: 0.00158090, VF1: 95.1977, VLoss: 0.00266509,\n",
      "Train: train_2016 [0034] TF1: 98.3582, Tloss: 0.00148806, VF1: 96.4624, VLoss: 0.00206830,\n",
      "Train: train_2016 [0035] TF1: 98.5199, Tloss: 0.00127842, VF1: 88.2257, VLoss: 0.00549296,\n",
      "Train: train_2016 [0036] TF1: 98.5727, Tloss: 0.00129588, VF1: 97.2182, VLoss: 0.00160823,\n",
      "Train: train_2016 [0037] TF1: 98.3205, Tloss: 0.00157135, VF1: 96.6118, VLoss: 0.00160718,\n",
      "Train: train_2016 [0038] TF1: 98.6236, Tloss: 0.00124642, VF1: 96.4661, VLoss: 0.00204321,\n",
      "Train: train_2016 [0039] TF1: 98.6754, Tloss: 0.00122660, VF1: 96.7774, VLoss: 0.00190689,\n",
      "Train: train_2016 [0040] TF1: 98.6620, Tloss: 0.00125505, VF1: 97.0958, VLoss: 0.00176227,\n",
      "Train: train_2016 [0041] TF1: 98.6353, Tloss: 0.00122519, VF1: 96.8750, VLoss: 0.00161284,\n",
      "Train: train_2016 [0042] TF1: 98.5474, Tloss: 0.00133542, VF1: 97.3366, VLoss: 0.00158663,\n",
      "Train: train_2016 [0043] TF1: 98.4977, Tloss: 0.00135021, VF1: 83.3741, VLoss: 0.01425729,\n",
      "Train: train_2016 [0044] TF1: 98.8625, Tloss: 0.00116413, VF1: 96.4491, VLoss: 0.00194209,\n",
      "Train: train_2016 [0045] TF1: 98.7386, Tloss: 0.00120248, VF1: 96.4834, VLoss: 0.00186191,\n",
      "Train: train_2016 [0046] TF1: 98.6495, Tloss: 0.00131817, VF1: 96.6650, VLoss: 0.00182069,\n",
      "Train: train_2016 [0047] TF1: 98.6987, Tloss: 0.00120008, VF1: 96.3105, VLoss: 0.00191742,\n",
      "Train: train_2016 [0048] TF1: 98.6613, Tloss: 0.00120591, VF1: 97.4058, VLoss: 0.00146330,\n",
      "Train: train_2016 [0049] TF1: 98.3855, Tloss: 0.00132490, VF1: 95.9480, VLoss: 0.00234314,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_048.pth Fragment=00 score=0.9123291930830756\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_021.pth Fragment=00 score=0.9014835747085836\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_014.pth Fragment=00 score=0.9103112502173536\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_042.pth Fragment=00 score=0.9014870520978664\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_037.pth Fragment=00 score=0.9121288170992573\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_036.pth Fragment=00 score=0.9110110803324099\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_041.pth Fragment=00 score=0.9141732419413318\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_005.pth Fragment=00 score=0.9079257309131218\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_012.pth Fragment=00 score=0.9101365440236766\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2016_020.pth Fragment=00 score=0.9092620804162885\n",
      "Extend: f1score=92.0003, vcoverage=90.1933, vf1score=95.1501, vexentdSize=54116, ecoverage=9.8067, ef1score=64.7985, erestSize=5884\n",
      "########\n",
      "######## 2017\n",
      "8000\n",
      "0    4014\n",
      "1    3986\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "1    1014\n",
      "0     986\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2017 [0000] TF1: 93.6181, Tloss: 0.00532760, VF1: 95.3500, VLoss: 0.00199663,\n",
      "Train: train_2017 [0001] TF1: 96.4020, Tloss: 0.00325092, VF1: 83.8308, VLoss: 0.00672877,\n",
      "Train: train_2017 [0002] TF1: 96.6545, Tloss: 0.00308467, VF1: 96.9906, VLoss: 0.00145914,\n",
      "Train: train_2017 [0003] TF1: 97.2843, Tloss: 0.00259110, VF1: 72.2737, VLoss: 0.01613063,\n",
      "Train: train_2017 [0004] TF1: 97.5369, Tloss: 0.00230723, VF1: 95.7143, VLoss: 0.00173103,\n",
      "Train: train_2017 [0005] TF1: 97.4216, Tloss: 0.00241327, VF1: 93.9252, VLoss: 0.00294057,\n",
      "Train: train_2017 [0006] TF1: 97.4934, Tloss: 0.00227619, VF1: 97.1316, VLoss: 0.00121887,\n",
      "Train: train_2017 [0007] TF1: 97.8552, Tloss: 0.00204330, VF1: 96.4057, VLoss: 0.00154292,\n",
      "Train: train_2017 [0008] TF1: 97.6568, Tloss: 0.00209467, VF1: 96.6667, VLoss: 0.00137929,\n",
      "Train: train_2017 [0009] TF1: 97.9448, Tloss: 0.00197191, VF1: 96.9874, VLoss: 0.00143756,\n",
      "Train: train_2017 [0010] TF1: 97.9185, Tloss: 0.00197181, VF1: 92.9963, VLoss: 0.00315480,\n",
      "Train: train_2017 [0011] TF1: 97.9937, Tloss: 0.00200408, VF1: 91.9781, VLoss: 0.00339361,\n",
      "Train: train_2017 [0012] TF1: 98.1488, Tloss: 0.00176506, VF1: 97.3801, VLoss: 0.00110290,\n",
      "Train: train_2017 [0013] TF1: 98.1589, Tloss: 0.00174348, VF1: 96.1975, VLoss: 0.00168000,\n",
      "Train: train_2017 [0014] TF1: 98.1484, Tloss: 0.00182156, VF1: 96.8473, VLoss: 0.00129182,\n",
      "Train: train_2017 [0015] TF1: 98.1061, Tloss: 0.00166138, VF1: 97.3407, VLoss: 0.00101535,\n",
      "Train: train_2017 [0016] TF1: 98.2350, Tloss: 0.00168183, VF1: 97.2576, VLoss: 0.00122879,\n",
      "Train: train_2017 [0017] TF1: 98.2872, Tloss: 0.00170023, VF1: 94.8943, VLoss: 0.00204927,\n",
      "Train: train_2017 [0018] TF1: 98.3499, Tloss: 0.00163845, VF1: 95.4826, VLoss: 0.00217627,\n",
      "Train: train_2017 [0019] TF1: 98.1493, Tloss: 0.00166492, VF1: 97.7756, VLoss: 0.00104872,\n",
      "Train: train_2017 [0020] TF1: 98.3891, Tloss: 0.00158194, VF1: 93.9804, VLoss: 0.00276485,\n",
      "Train: train_2017 [0021] TF1: 98.1882, Tloss: 0.00175191, VF1: 96.9082, VLoss: 0.00171097,\n",
      "Train: train_2017 [0022] TF1: 98.6157, Tloss: 0.00139984, VF1: 96.6633, VLoss: 0.00139321,\n",
      "Train: train_2017 [0023] TF1: 98.4255, Tloss: 0.00148912, VF1: 97.1571, VLoss: 0.00115486,\n",
      "Train: train_2017 [0024] TF1: 98.3247, Tloss: 0.00159719, VF1: 96.9462, VLoss: 0.00146768,\n",
      "Train: train_2017 [0025] TF1: 98.5022, Tloss: 0.00143840, VF1: 97.0902, VLoss: 0.00126278,\n",
      "Train: train_2017 [0026] TF1: 98.2889, Tloss: 0.00156929, VF1: 95.1659, VLoss: 0.00241791,\n",
      "Train: train_2017 [0027] TF1: 98.5150, Tloss: 0.00138124, VF1: 95.1257, VLoss: 0.00186775,\n",
      "Train: train_2017 [0028] TF1: 98.4898, Tloss: 0.00146254, VF1: 95.9878, VLoss: 0.00143733,\n",
      "Train: train_2017 [0029] TF1: 98.3891, Tloss: 0.00149118, VF1: 95.9233, VLoss: 0.00197719,\n",
      "Train: train_2017 [0030] TF1: 98.5289, Tloss: 0.00140869, VF1: 95.3799, VLoss: 0.00197651,\n",
      "Train: train_2017 [0031] TF1: 98.6764, Tloss: 0.00119169, VF1: 97.4740, VLoss: 0.00116334,\n",
      "Train: train_2017 [0032] TF1: 98.3772, Tloss: 0.00149909, VF1: 85.2311, VLoss: 0.00673947,\n",
      "Train: train_2017 [0033] TF1: 98.3264, Tloss: 0.00148785, VF1: 96.6179, VLoss: 0.00167654,\n",
      "Train: train_2017 [0034] TF1: 98.4898, Tloss: 0.00133937, VF1: 97.1486, VLoss: 0.00128609,\n",
      "Train: train_2017 [0035] TF1: 98.6054, Tloss: 0.00126776, VF1: 97.3930, VLoss: 0.00130411,\n",
      "Train: train_2017 [0036] TF1: 98.6277, Tloss: 0.00124821, VF1: 97.5610, VLoss: 0.00112929,\n",
      "Train: train_2017 [0037] TF1: 98.6922, Tloss: 0.00132847, VF1: 96.2888, VLoss: 0.00163521,\n",
      "Epoch    37: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2017 [0038] TF1: 98.7288, Tloss: 0.00116736, VF1: 97.2946, VLoss: 0.00121027,\n",
      "Train: train_2017 [0039] TF1: 98.7934, Tloss: 0.00109802, VF1: 94.5489, VLoss: 0.00287293,\n",
      "Train: train_2017 [0040] TF1: 98.8812, Tloss: 0.00108315, VF1: 97.0884, VLoss: 0.00133052,\n",
      "Train: train_2017 [0041] TF1: 98.7803, Tloss: 0.00114639, VF1: 96.8421, VLoss: 0.00152214,\n",
      "Train: train_2017 [0042] TF1: 98.6928, Tloss: 0.00117255, VF1: 97.8575, VLoss: 0.00102222,\n",
      "Train: train_2017 [0043] TF1: 98.6912, Tloss: 0.00119695, VF1: 97.5969, VLoss: 0.00120572,\n",
      "Train: train_2017 [0044] TF1: 98.7794, Tloss: 0.00113658, VF1: 94.6035, VLoss: 0.00358618,\n",
      "Train: train_2017 [0045] TF1: 98.7667, Tloss: 0.00107755, VF1: 96.9815, VLoss: 0.00134079,\n",
      "Train: train_2017 [0046] TF1: 98.8451, Tloss: 0.00115304, VF1: 97.8596, VLoss: 0.00107061,\n",
      "Train: train_2017 [0047] TF1: 98.8820, Tloss: 0.00102633, VF1: 97.8261, VLoss: 0.00108856,\n",
      "Train: train_2017 [0048] TF1: 98.8055, Tloss: 0.00107760, VF1: 97.6447, VLoss: 0.00107162,\n",
      "Train: train_2017 [0049] TF1: 98.8431, Tloss: 0.00111508, VF1: 97.0287, VLoss: 0.00155825,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_015.pth Fragment=00 score=0.9070771957027357\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_042.pth Fragment=00 score=0.9012704001522577\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_019.pth Fragment=00 score=0.8894252202118338\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_046.pth Fragment=00 score=0.9030056395795949\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_048.pth Fragment=00 score=0.8869745213235518\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_047.pth Fragment=00 score=0.8974183088051884\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_012.pth Fragment=00 score=0.8890459581137351\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_036.pth Fragment=00 score=0.9046799285896454\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_023.pth Fragment=00 score=0.9019281497299335\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2017_031.pth Fragment=00 score=0.8886002752137544\n",
      "Extend: f1score=90.3745, vcoverage=89.0383, vf1score=94.2638, vexentdSize=53423, ecoverage=10.9617, ef1score=38.6547, erestSize=6577\n",
      "########\n",
      "######## 2018\n",
      "8000\n",
      "1    4034\n",
      "0    3966\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "0    1034\n",
      "1     966\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2018 [0000] TF1: 92.6340, Tloss: 0.00617925, VF1: 92.5962, VLoss: 0.00337267,\n",
      "Train: train_2018 [0001] TF1: 94.3565, Tloss: 0.00490412, VF1: 94.7959, VLoss: 0.00255577,\n",
      "Train: train_2018 [0002] TF1: 94.9662, Tloss: 0.00430299, VF1: 74.5736, VLoss: 0.01257304,\n",
      "Train: train_2018 [0003] TF1: 95.3877, Tloss: 0.00396736, VF1: 95.6613, VLoss: 0.00213937,\n",
      "Train: train_2018 [0004] TF1: 95.5783, Tloss: 0.00393759, VF1: 93.8653, VLoss: 0.00295054,\n",
      "Train: train_2018 [0005] TF1: 95.8829, Tloss: 0.00359303, VF1: 94.8943, VLoss: 0.00237619,\n",
      "Train: train_2018 [0006] TF1: 95.7087, Tloss: 0.00362700, VF1: 94.8223, VLoss: 0.00257494,\n",
      "Train: train_2018 [0007] TF1: 96.1668, Tloss: 0.00325856, VF1: 93.9262, VLoss: 0.00274870,\n",
      "Train: train_2018 [0008] TF1: 96.2954, Tloss: 0.00318998, VF1: 94.8101, VLoss: 0.00237018,\n",
      "Train: train_2018 [0009] TF1: 96.3807, Tloss: 0.00328033, VF1: 94.5008, VLoss: 0.00241512,\n",
      "Train: train_2018 [0010] TF1: 96.4397, Tloss: 0.00322487, VF1: 95.7120, VLoss: 0.00213263,\n",
      "Train: train_2018 [0011] TF1: 96.3241, Tloss: 0.00318609, VF1: 95.2331, VLoss: 0.00270016,\n",
      "Train: train_2018 [0012] TF1: 96.7459, Tloss: 0.00299581, VF1: 94.6467, VLoss: 0.00227605,\n",
      "Train: train_2018 [0013] TF1: 96.7258, Tloss: 0.00284754, VF1: 94.7537, VLoss: 0.00237376,\n",
      "Train: train_2018 [0014] TF1: 96.7613, Tloss: 0.00281910, VF1: 94.7742, VLoss: 0.00266120,\n",
      "Train: train_2018 [0015] TF1: 96.4774, Tloss: 0.00297682, VF1: 95.6923, VLoss: 0.00248001,\n",
      "Train: train_2018 [0016] TF1: 96.8848, Tloss: 0.00272741, VF1: 95.0254, VLoss: 0.00260313,\n",
      "Train: train_2018 [0017] TF1: 96.7209, Tloss: 0.00273576, VF1: 94.8759, VLoss: 0.00258206,\n",
      "Train: train_2018 [0018] TF1: 96.6413, Tloss: 0.00292370, VF1: 91.5980, VLoss: 0.00338765,\n",
      "Train: train_2018 [0019] TF1: 97.1614, Tloss: 0.00267887, VF1: 95.7717, VLoss: 0.00216844,\n",
      "Train: train_2018 [0020] TF1: 97.0985, Tloss: 0.00265835, VF1: 93.5372, VLoss: 0.00331416,\n",
      "Train: train_2018 [0021] TF1: 96.8579, Tloss: 0.00261019, VF1: 96.1816, VLoss: 0.00217988,\n",
      "Train: train_2018 [0022] TF1: 97.0706, Tloss: 0.00262219, VF1: 96.3345, VLoss: 0.00218295,\n",
      "Train: train_2018 [0023] TF1: 97.1322, Tloss: 0.00260993, VF1: 95.7761, VLoss: 0.00227509,\n",
      "Train: train_2018 [0024] TF1: 96.7871, Tloss: 0.00265170, VF1: 89.6669, VLoss: 0.00365261,\n",
      "Train: train_2018 [0025] TF1: 96.9977, Tloss: 0.00262468, VF1: 95.4109, VLoss: 0.00227979,\n",
      "Train: train_2018 [0026] TF1: 97.1722, Tloss: 0.00247189, VF1: 95.4476, VLoss: 0.00234585,\n",
      "Train: train_2018 [0027] TF1: 97.4455, Tloss: 0.00239156, VF1: 91.7921, VLoss: 0.00417951,\n",
      "Train: train_2018 [0028] TF1: 97.0849, Tloss: 0.00245120, VF1: 95.6701, VLoss: 0.00236333,\n",
      "Train: train_2018 [0029] TF1: 97.5128, Tloss: 0.00234546, VF1: 94.7896, VLoss: 0.00364536,\n",
      "Train: train_2018 [0030] TF1: 97.2271, Tloss: 0.00263599, VF1: 93.2542, VLoss: 0.00277000,\n",
      "Train: train_2018 [0031] TF1: 97.5147, Tloss: 0.00227488, VF1: 95.4593, VLoss: 0.00224357,\n",
      "Train: train_2018 [0032] TF1: 97.4237, Tloss: 0.00232175, VF1: 94.8718, VLoss: 0.00248675,\n",
      "Train: train_2018 [0033] TF1: 97.4282, Tloss: 0.00234283, VF1: 91.3735, VLoss: 0.00391937,\n",
      "Train: train_2018 [0034] TF1: 97.4603, Tloss: 0.00236193, VF1: 90.3587, VLoss: 0.00428196,\n",
      "Train: train_2018 [0035] TF1: 97.5926, Tloss: 0.00226502, VF1: 95.6933, VLoss: 0.00200876,\n",
      "Train: train_2018 [0036] TF1: 97.4891, Tloss: 0.00219423, VF1: 95.6296, VLoss: 0.00215907,\n",
      "Train: train_2018 [0037] TF1: 97.7667, Tloss: 0.00218840, VF1: 96.5300, VLoss: 0.00204619,\n",
      "Train: train_2018 [0038] TF1: 97.4878, Tloss: 0.00226427, VF1: 94.1240, VLoss: 0.00283857,\n",
      "Train: train_2018 [0039] TF1: 97.4878, Tloss: 0.00222841, VF1: 92.9392, VLoss: 0.00352504,\n",
      "Train: train_2018 [0040] TF1: 97.7344, Tloss: 0.00221547, VF1: 95.6154, VLoss: 0.00262586,\n",
      "Train: train_2018 [0041] TF1: 97.6360, Tloss: 0.00213625, VF1: 94.1602, VLoss: 0.00264320,\n",
      "Train: train_2018 [0042] TF1: 97.6779, Tloss: 0.00228171, VF1: 95.1363, VLoss: 0.00255199,\n",
      "Train: train_2018 [0043] TF1: 97.8622, Tloss: 0.00217007, VF1: 95.5890, VLoss: 0.00250130,\n",
      "Train: train_2018 [0044] TF1: 97.5786, Tloss: 0.00218004, VF1: 94.5363, VLoss: 0.00285384,\n",
      "Train: train_2018 [0045] TF1: 97.5671, Tloss: 0.00223446, VF1: 96.1418, VLoss: 0.00214195,\n",
      "Train: train_2018 [0046] TF1: 97.8144, Tloss: 0.00209735, VF1: 96.1777, VLoss: 0.00234800,\n",
      "Train: train_2018 [0047] TF1: 97.8787, Tloss: 0.00201849, VF1: 95.8652, VLoss: 0.00241834,\n",
      "Train: train_2018 [0048] TF1: 97.8049, Tloss: 0.00201452, VF1: 94.6121, VLoss: 0.00294140,\n",
      "Train: train_2018 [0049] TF1: 97.8255, Tloss: 0.00206760, VF1: 93.6937, VLoss: 0.00299735,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_035.pth Fragment=00 score=0.8944803702228079\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_037.pth Fragment=00 score=0.8910734634961637\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_010.pth Fragment=00 score=0.8938523324065493\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_003.pth Fragment=00 score=0.8788112173243329\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_045.pth Fragment=00 score=0.8821823100623732\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_036.pth Fragment=00 score=0.8856424992853598\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_019.pth Fragment=00 score=0.8776792812251571\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_021.pth Fragment=00 score=0.8623756889964712\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_022.pth Fragment=00 score=0.8554084205036075\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2018_031.pth Fragment=00 score=0.8891568471823678\n",
      "Extend: f1score=88.8156, vcoverage=83.9183, vf1score=93.9495, vexentdSize=50351, ecoverage=16.0817, ef1score=29.5596, erestSize=9649\n",
      "########\n",
      "######## 2019\n",
      "8000\n",
      "1    4049\n",
      "0    3951\n",
      "Name: label, dtype: int64\n",
      "2000\n",
      "0    1049\n",
      "1     951\n",
      "Name: label, dtype: int64\n",
      "60000\n",
      "1    30000\n",
      "0    30000\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_2019 [0000] TF1: 96.5500, Tloss: 0.00321004, VF1: 95.4361, VLoss: 0.00206611,\n",
      "Train: train_2019 [0001] TF1: 97.4422, Tloss: 0.00230217, VF1: 94.7888, VLoss: 0.00196276,\n",
      "Train: train_2019 [0002] TF1: 98.1347, Tloss: 0.00184003, VF1: 88.8370, VLoss: 0.00482824,\n",
      "Train: train_2019 [0003] TF1: 98.3164, Tloss: 0.00158231, VF1: 87.4479, VLoss: 0.00445075,\n",
      "Train: train_2019 [0004] TF1: 98.3084, Tloss: 0.00149019, VF1: 81.5609, VLoss: 0.01091674,\n",
      "Train: train_2019 [0005] TF1: 98.2942, Tloss: 0.00152052, VF1: 97.2683, VLoss: 0.00130534,\n",
      "Train: train_2019 [0006] TF1: 98.5522, Tloss: 0.00140500, VF1: 98.0618, VLoss: 0.00100029,\n",
      "Train: train_2019 [0007] TF1: 98.4764, Tloss: 0.00141917, VF1: 97.8970, VLoss: 0.00080783,\n",
      "Train: train_2019 [0008] TF1: 98.5532, Tloss: 0.00126043, VF1: 96.7045, VLoss: 0.00139969,\n",
      "Train: train_2019 [0009] TF1: 98.7740, Tloss: 0.00115148, VF1: 97.9145, VLoss: 0.00095365,\n",
      "Train: train_2019 [0010] TF1: 98.6267, Tloss: 0.00121196, VF1: 97.2566, VLoss: 0.00132538,\n",
      "Train: train_2019 [0011] TF1: 98.8241, Tloss: 0.00121035, VF1: 98.0434, VLoss: 0.00080121,\n",
      "Train: train_2019 [0012] TF1: 98.8005, Tloss: 0.00114492, VF1: 96.1871, VLoss: 0.00174675,\n",
      "Train: train_2019 [0013] TF1: 98.8867, Tloss: 0.00108965, VF1: 98.0021, VLoss: 0.00089849,\n",
      "Train: train_2019 [0014] TF1: 98.8745, Tloss: 0.00102099, VF1: 97.9701, VLoss: 0.00122874,\n",
      "Train: train_2019 [0015] TF1: 98.8497, Tloss: 0.00108604, VF1: 98.4713, VLoss: 0.00073394,\n",
      "Train: train_2019 [0016] TF1: 98.9000, Tloss: 0.00105219, VF1: 97.9873, VLoss: 0.00111343,\n",
      "Train: train_2019 [0017] TF1: 98.8619, Tloss: 0.00108550, VF1: 97.5427, VLoss: 0.00102887,\n",
      "Train: train_2019 [0018] TF1: 98.9245, Tloss: 0.00101078, VF1: 97.9036, VLoss: 0.00092374,\n",
      "Train: train_2019 [0019] TF1: 98.9497, Tloss: 0.00105196, VF1: 96.7532, VLoss: 0.00120790,\n",
      "Train: train_2019 [0020] TF1: 99.0231, Tloss: 0.00090329, VF1: 98.3015, VLoss: 0.00077623,\n",
      "Train: train_2019 [0021] TF1: 98.8870, Tloss: 0.00100766, VF1: 98.3211, VLoss: 0.00092874,\n",
      "Train: train_2019 [0022] TF1: 99.1457, Tloss: 0.00083779, VF1: 98.0351, VLoss: 0.00110054,\n",
      "Train: train_2019 [0023] TF1: 98.9609, Tloss: 0.00106760, VF1: 97.7522, VLoss: 0.00087065,\n",
      "Train: train_2019 [0024] TF1: 99.0349, Tloss: 0.00089228, VF1: 98.2456, VLoss: 0.00078698,\n",
      "Train: train_2019 [0025] TF1: 99.0224, Tloss: 0.00087745, VF1: 97.5000, VLoss: 0.00118258,\n",
      "Train: train_2019 [0026] TF1: 99.1713, Tloss: 0.00082756, VF1: 98.1092, VLoss: 0.00083145,\n",
      "Train: train_2019 [0027] TF1: 98.9614, Tloss: 0.00096477, VF1: 98.2086, VLoss: 0.00093553,\n",
      "Train: train_2019 [0028] TF1: 99.1341, Tloss: 0.00092508, VF1: 98.6793, VLoss: 0.00071850,\n",
      "Train: train_2019 [0029] TF1: 99.0839, Tloss: 0.00080380, VF1: 97.0543, VLoss: 0.00177212,\n",
      "Train: train_2019 [0030] TF1: 99.2204, Tloss: 0.00084618, VF1: 98.2567, VLoss: 0.00082379,\n",
      "Train: train_2019 [0031] TF1: 99.2331, Tloss: 0.00073095, VF1: 98.2979, VLoss: 0.00092003,\n",
      "Train: train_2019 [0032] TF1: 99.0351, Tloss: 0.00094174, VF1: 98.2218, VLoss: 0.00092609,\n",
      "Train: train_2019 [0033] TF1: 99.3446, Tloss: 0.00070406, VF1: 98.5185, VLoss: 0.00065329,\n",
      "Train: train_2019 [0034] TF1: 99.1464, Tloss: 0.00080040, VF1: 90.3901, VLoss: 0.00543207,\n",
      "Train: train_2019 [0035] TF1: 99.2825, Tloss: 0.00069126, VF1: 98.2530, VLoss: 0.00100593,\n",
      "Train: train_2019 [0036] TF1: 99.1954, Tloss: 0.00077517, VF1: 89.7824, VLoss: 0.00469539,\n",
      "Train: train_2019 [0037] TF1: 99.0106, Tloss: 0.00094266, VF1: 95.2669, VLoss: 0.00312913,\n",
      "Train: train_2019 [0038] TF1: 99.1107, Tloss: 0.00077766, VF1: 97.7564, VLoss: 0.00106756,\n",
      "Train: train_2019 [0039] TF1: 99.2208, Tloss: 0.00074919, VF1: 97.9484, VLoss: 0.00088803,\n",
      "Train: train_2019 [0040] TF1: 99.1840, Tloss: 0.00076939, VF1: 98.6779, VLoss: 0.00078476,\n",
      "Train: train_2019 [0041] TF1: 99.1588, Tloss: 0.00070211, VF1: 98.5201, VLoss: 0.00085949,\n",
      "Epoch    41: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_2019 [0042] TF1: 99.2463, Tloss: 0.00060391, VF1: 98.4745, VLoss: 0.00082108,\n",
      "Train: train_2019 [0043] TF1: 99.3073, Tloss: 0.00072391, VF1: 98.7342, VLoss: 0.00082091,\n",
      "Train: train_2019 [0044] TF1: 99.3944, Tloss: 0.00062892, VF1: 98.4664, VLoss: 0.00080189,\n",
      "Train: train_2019 [0045] TF1: 99.2083, Tloss: 0.00066410, VF1: 97.3725, VLoss: 0.00133998,\n",
      "Train: train_2019 [0046] TF1: 99.3327, Tloss: 0.00068504, VF1: 98.2713, VLoss: 0.00090919,\n",
      "Train: train_2019 [0047] TF1: 99.2701, Tloss: 0.00064249, VF1: 97.7755, VLoss: 0.00100111,\n",
      "Train: train_2019 [0048] TF1: 99.1834, Tloss: 0.00073964, VF1: 98.0892, VLoss: 0.00085651,\n",
      "Epoch    48: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_2019 [0049] TF1: 99.3944, Tloss: 0.00051927, VF1: 98.4680, VLoss: 0.00071419,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_033.pth Fragment=00 score=0.8795233825250393\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_049.pth Fragment=00 score=0.864033052407826\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_028.pth Fragment=00 score=0.8700361566274329\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_015.pth Fragment=00 score=0.8576445244489037\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_020.pth Fragment=00 score=0.8815219554004622\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_040.pth Fragment=00 score=0.8781450718982037\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_024.pth Fragment=00 score=0.8891448136331804\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_011.pth Fragment=00 score=0.8565652962335385\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_044.pth Fragment=00 score=0.8738467159952499\n",
      "Evaluate: ModelPath=./traces/detectionWS04/model_train_2019_007.pth Fragment=00 score=0.8486680686996144\n",
      "Extend: f1score=87.9082, vcoverage=83.6783, vf1score=93.2867, vexentdSize=50207, ecoverage=16.3217, ef1score=33.8580, erestSize=9793\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for yearP in years:\n",
    "    currentTag = str(yearP)\n",
    "\n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    #\n",
    "    dataset_df = timedf.loc[timedf.year == yearP]\n",
    "    test_df    = timedf.loc[timedf.year != yearP]\n",
    "\n",
    "    #\n",
    "    trainLoader, validLoader, testLoader = getDataloaders(dataset_df, test_df, trainPercentage=trainPercentageParam, \n",
    "                                                                               validPercentage=validPercentageParam)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "\n",
    "    #\n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, testLoader, device)\n",
    "\n",
    "    #\n",
    "    evalDataset(ws, evalresult_df, probaUpperBorn=0.8,  probaLowerBorn=0.2)\n",
    "\n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df)], columns=['TimeTag', 'models', 'evalResuls'])\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    #\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "androzooD_df = pd.read_parquet('dataset/androzooDone_meta.parquet')\n",
    "androzooD_df['scan_date'] = pd.to_datetime(androzooD_df.vt_scan_date)\n",
    "\n",
    "androzoo_df = androzooD_df.loc[(androzooD_df.vt_detection == 0) | (androzooD_df.vt_detection > 15)]\n",
    "print(len(androzoo_df))\n",
    "\n",
    "androzoo_df['year']  = androzoo_df.scan_date.dt.year\n",
    "androzoo_df['label'] = (androzoo_df.vt_detection != 0).astype(int)\n",
    "\n",
    "dfList = []\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "\n",
    "for yearP in years:\n",
    "    print(yearP)\n",
    "    df = pd.concat([androzoo_df.loc[(androzoo_df.year == yearP) & (androzoo_df.label == 0)].sample(5000, random_state=54), \n",
    "                    androzoo_df.loc[(androzoo_df.year == yearP) & (androzoo_df.label == 1)].sample(5000, random_state=54)])\n",
    "    dfList.append(df)\n",
    "\n",
    "timedf = pd.concat(dfList)\n",
    "\n",
    "timedf.year.value_counts()\n",
    "\n",
    "timedf['filePath'] = '/ws/mnt/local/data/output/datasets/zoo/' + timedf.sha256\n",
    "\n",
    "timedf.to_parquet('dataset/timedf.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
