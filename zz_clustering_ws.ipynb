{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/opt/conda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "#import umap\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=8000):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "                        nn.Linear(self.sequenceSize  , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.Tanh(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.Tanh(),\n",
    "                        \n",
    "                        nn.Linear(256, 128),\n",
    "                        nn.BatchNorm1d(128),\n",
    "                        nn.Tanh(),\n",
    "            \n",
    "                        nn.Linear(128, 64),\n",
    "                        nn.BatchNorm1d(64),\n",
    "                        nn.Tanh(),\n",
    "                    )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "                        nn.Linear(64, 128),\n",
    "                        nn.BatchNorm1d(128),\n",
    "                        nn.Tanh(),\n",
    "\n",
    "                        nn.Linear(128, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.Tanh(),\n",
    "            \n",
    "                        nn.Linear(256, 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.Tanh(),\n",
    "                        \n",
    "                        nn.Linear(512, self.sequenceSize),\n",
    "                        nn.BatchNorm1d(self.sequenceSize),\n",
    "                        nn.Tanh(),\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        shash = self.encoder(x)\n",
    "        #print(x.size())\n",
    "\n",
    "        y = self.decoder(shash)\n",
    "        #print(x.size())\n",
    "\n",
    "        return y, x\n",
    "    \n",
    "    def sHash(self, x):\n",
    "        \n",
    "        shash = self.encoder(x)\n",
    "        \n",
    "        return shash\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=8000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels       = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName  = featureName\n",
    "        self.fhVectorizer = HashingVectorizer(tokenizer=lambda x: x.split(), \n",
    "                             ngram_range=(4, 4), \n",
    "                             decode_error='replace',\n",
    "                             n_features=self.sequenceSize)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        \n",
    "        contentList = np.concatenate(df[self.featureName]).tolist()\n",
    "        content     = ' '.join([str(item) for item in contentList])\n",
    "\n",
    "        vec = self.fhVectorizer.transform([content]).toarray()\n",
    "\n",
    "        sample = torch.from_numpy(vec)\n",
    "        \n",
    "        return (sample.float(), self.labels[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss = 0.0  \n",
    "    label_lst    = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.squeeze().to(device)\n",
    "        labels = labels\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        output1, output2 = model(inputs)\n",
    "        loss = F.mse_loss(output1, output2)\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    loss      = running_loss / len(labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    label_lst     = list()\n",
    "    hash_lst     = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.squeeze().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model.sHash(inputs)\n",
    "\n",
    "            #\n",
    "            label_lst.append(labels.cpu().numpy())\n",
    "            hash_lst.append(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "    labels = np.concatenate(label_lst)\n",
    "    hashes = np.concatenate(hash_lst)   \n",
    "\n",
    "    hdb = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=50, alpha=1.0)\n",
    "    clustring_result = hdb.fit_predict(hashes)\n",
    "\n",
    "    #return labels, clustring_result\n",
    "    homogeneity_score = metrics.homogeneity_score(labels, clustring_result)\n",
    "    numClusters       = len(set(clustring_result))\n",
    "    coverage          = len(clustring_result)/len(labels)\n",
    "\n",
    "    c_labels = labels[clustring_result != -1]\n",
    "    c_result = clustring_result[clustring_result != -1]\n",
    "\n",
    "    c_homogeneity_score = metrics.homogeneity_score(c_labels, c_result)\n",
    "    c_numClusters = len(set(c_result))\n",
    "    c_coverage = len(c_result)/len(labels)\n",
    "\n",
    "    return homogeneity_score, numClusters, coverage, c_homogeneity_score, c_numClusters, c_coverage, len(c_result)\n",
    "\n",
    "def getValid(batchSize=128, numWorkers=32):\n",
    "    global malware_rootDir\n",
    "    global malwareMetaPath\n",
    "    \n",
    "    eval_df = pd.read_msgpack(malwareMetaPath)\n",
    "    eval_df             = eval_df.loc[eval_df.md5.isin([fileName.split('/')[-1] for fileName in glob.glob(malware_rootDir + '*')])]\n",
    "    eval_df['filePath'] = malware_rootDir + eval_df.md5\n",
    "    eval_df['label']  = pd.factorize(eval_df.family)[0]\n",
    "    validDataset = SampleDataset(eval_df.filePath.values, eval_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=32, shuffle=False, num_workers=32)\n",
    "\n",
    "\n",
    "    #eval_df = eval_df.sample(evalDatasetSize, random_state=54)\n",
    "    #eval_df['vname']  = eval_df.family + '-' + eval_df.variant\n",
    "    #eval_df['vlabel'] = pd.factorize(eval_df.vname)[0]\n",
    "\n",
    "    #vlabelMap = pd.Series(pd.factorize(eval_df.vname)[1])\n",
    "    #labelMap  = pd.Series(pd.factorize(eval_df.family)[1])\n",
    "    #print(len(eval_df))\n",
    "\n",
    "    \n",
    "    return validLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ws = 'clusteringWS01'\n",
    "\n",
    "#\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}/'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "model  = Net()\n",
    "device = torch.device('cuda:1')\n",
    "model  = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The read_msgpack is deprecated and will be removed in a future version.\n",
      "It is recommended to use pyarrow for on-the-wire transmission of pandas objects.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "AMD [0000] homogeneity_score:0.49103 numClusters:101 coverage:1.00000 c_homogeneity_score:0.96558 c_numClusters:100 c_coverage:0.49908 c_size:11609\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "malware_rootDir = '/ws/mnt/local/data/output/datasets/amd/'\n",
    "malwareMetaPath = '/ws/mnt/habouch/datasets/android_dataset/meta/amd_meta.msg'\n",
    "eval_df = pd.read_msgpack(malwareMetaPath)\n",
    "eval_df             = eval_df.loc[eval_df.md5.isin([fileName.split('/')[-1] for fileName in glob.glob(malware_rootDir + '*')])]\n",
    "eval_df['filePath'] = malware_rootDir + eval_df.md5\n",
    "eval_df['label']  = pd.factorize(eval_df.family)[0]\n",
    "validDataset = SampleDataset(eval_df.filePath.values, eval_df.label.values)\n",
    "validLoader  = DataLoader(validDataset, batch_size=32, shuffle=False, num_workers=32)\n",
    "\n",
    "print('----------')\n",
    "epoch    = 0\n",
    "\n",
    "measures = assess(model, validLoader, device)\n",
    "homogeneity_score, numClusters, coverage, c_homogeneity_score, c_numClusters, c_coverage, c_size = measures \n",
    "\n",
    "Results_df = pd.DataFrame([measures], columns=['homogeneity_score', 'numClusters', 'coverage', 'c_homogeneity_score', 'c_numClusters', 'c_coverage', 'c_size'])\n",
    "Results_df.to_csv(outputtracesPath + 'amdResults_df.csv')\n",
    "\n",
    "message = 'AMD [{:04d}] '.format(epoch)\n",
    "#message += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "message += f'homogeneity_score:{homogeneity_score:2.5f} '\n",
    "message += f'numClusters:{numClusters:03d} '\n",
    "message += f'coverage:{coverage:2.5f} '\n",
    "message += f'c_homogeneity_score:{c_homogeneity_score:2.5f} '\n",
    "message += f'c_numClusters:{c_numClusters:03d} '\n",
    "message += f'c_coverage:{c_coverage:2.5f} '\n",
    "message += f'c_size:{c_size:05d}'\n",
    "\n",
    "with open(outputlogFilePath, 'a') as writer:\n",
    "    writer.write(message + '\\n')\n",
    "\n",
    "print(message)\n",
    "\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The read_msgpack is deprecated and will be removed in a future version.\n",
      "It is recommended to use pyarrow for on-the-wire transmission of pandas objects.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DREBIN [0000] homogeneity_score:0.38790 numClusters:028 coverage:1.00000 c_homogeneity_score:0.92282 c_numClusters:027 c_coverage:0.49310 c_size:02645\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "malware_rootDir = '/ws/mnt/local/data/output/datasets/drebin/'\n",
    "malwareMetaPath = '/ws/mnt/habouch/datasets/android_dataset/meta/drebin_meta.msg'\n",
    "eval_df = pd.read_msgpack(malwareMetaPath)\n",
    "\n",
    "eval_df             = eval_df.loc[eval_df.sha256.isin([fileName.split('/')[-1] for fileName in glob.glob(malware_rootDir + '*')])]\n",
    "eval_df['filePath'] = malware_rootDir + eval_df.sha256\n",
    "eval_df['label']  = pd.factorize(eval_df.drebin)[0]\n",
    "validDataset = SampleDataset(eval_df.filePath.values, eval_df.label.values)\n",
    "validLoader  = DataLoader(validDataset, batch_size=32, shuffle=False, num_workers=32)\n",
    "\n",
    "print('----------')\n",
    "epoch    = 0\n",
    "\n",
    "measures = assess(model, validLoader, device)\n",
    "homogeneity_score, numClusters, coverage, c_homogeneity_score, c_numClusters, c_coverage, c_size = measures \n",
    "\n",
    "Results_df = pd.DataFrame([measures], columns=['homogeneity_score', 'numClusters', 'coverage', 'c_homogeneity_score', 'c_numClusters', 'c_coverage', 'c_size'])\n",
    "Results_df.to_csv(outputtracesPath + 'drebinResults_df.csv')\n",
    "\n",
    "message = 'DREBIN [{:04d}] '.format(epoch)\n",
    "#message += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "message += f'homogeneity_score:{homogeneity_score:2.5f} '\n",
    "message += f'numClusters:{numClusters:03d} '\n",
    "message += f'coverage:{coverage:2.5f} '\n",
    "message += f'c_homogeneity_score:{c_homogeneity_score:2.5f} '\n",
    "message += f'c_numClusters:{c_numClusters:03d} '\n",
    "message += f'c_coverage:{c_coverage:2.5f} '\n",
    "message += f'c_size:{c_size:05d}'\n",
    "\n",
    "with open(outputlogFilePath, 'a') as writer:\n",
    "    writer.write(message + '\\n')\n",
    "\n",
    "print(message)\n",
    "\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The read_msgpack is deprecated and will be removed in a future version.\n",
      "It is recommended to use pyarrow for on-the-wire transmission of pandas objects.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "MALDOZER [0000] homogeneity_score:0.50003 numClusters:098 coverage:1.00000 c_homogeneity_score:0.91274 c_numClusters:097 c_coverage:0.55657 c_size:11048\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "malware_rootDir = '/ws/mnt/local/data/output/datasets/maldozer/'\n",
    "malwareMetaPath = '/ws/mnt/habouch/datasets/android_dataset/meta/maldozer_meta.msg'\n",
    "eval_df = pd.read_msgpack(malwareMetaPath)\n",
    "\n",
    "eval_df             = eval_df.loc[eval_df.md5.isin([fileName.split('/')[-1] for fileName in glob.glob(malware_rootDir + '*')])]\n",
    "eval_df['filePath'] = malware_rootDir + eval_df.md5\n",
    "eval_df['label']  = pd.factorize(eval_df.label)[0]\n",
    "validDataset = SampleDataset(eval_df.filePath.values, eval_df.label.values)\n",
    "validLoader  = DataLoader(validDataset, batch_size=32, shuffle=False, num_workers=32)\n",
    "\n",
    "print('----------')\n",
    "epoch    = 0\n",
    "\n",
    "measures = assess(model, validLoader, device)\n",
    "homogeneity_score, numClusters, coverage, c_homogeneity_score, c_numClusters, c_coverage, c_size = measures \n",
    "\n",
    "Results_df = pd.DataFrame([measures], columns=['homogeneity_score', 'numClusters', 'coverage', 'c_homogeneity_score', 'c_numClusters', 'c_coverage', 'c_size'])\n",
    "Results_df.to_csv(outputtracesPath + 'maldozerResults_df.csv')\n",
    "\n",
    "\n",
    "message = 'MALDOZER [{:04d}] '.format(epoch)\n",
    "#message += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "message += f'homogeneity_score:{homogeneity_score:2.5f} '\n",
    "message += f'numClusters:{numClusters:03d} '\n",
    "message += f'coverage:{coverage:2.5f} '\n",
    "message += f'c_homogeneity_score:{c_homogeneity_score:2.5f} '\n",
    "message += f'c_numClusters:{c_numClusters:03d} '\n",
    "message += f'c_coverage:{c_coverage:2.5f} '\n",
    "message += f'c_size:{c_size:05d}'\n",
    "\n",
    "with open(outputlogFilePath, 'a') as writer:\n",
    "    writer.write(message + '\\n')\n",
    "\n",
    "print(message)\n",
    "\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The read_msgpack is deprecated and will be removed in a future version.\n",
      "It is recommended to use pyarrow for on-the-wire transmission of pandas objects.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENOME [0000] homogeneity_score:0.30646 numClusters:007 coverage:1.00000 c_homogeneity_score:0.88998 c_numClusters:006 c_coverage:0.37381 c_size:00471\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "malware_rootDir = '/ws/mnt/local/data/output/datasets/genome/'\n",
    "malwareMetaPath = '/ws/mnt/habouch/datasets/android_dataset/meta/genome_meta.msg'\n",
    "eval_df = pd.read_msgpack(malwareMetaPath)\n",
    "\n",
    "eval_df             = eval_df.loc[eval_df.sha1.isin([fileName.split('/')[-1] for fileName in glob.glob(malware_rootDir + '*')])]\n",
    "eval_df['filePath'] = malware_rootDir + eval_df.sha1\n",
    "eval_df['label']    = pd.factorize(eval_df.genome)[0]\n",
    "validDataset = SampleDataset(eval_df.filePath.values, eval_df.label.values)\n",
    "validLoader  = DataLoader(validDataset, batch_size=32, shuffle=False, num_workers=32)\n",
    "\n",
    "print('----------')\n",
    "epoch    = 0\n",
    "\n",
    "measures = assess(model, validLoader, device)\n",
    "homogeneity_score, numClusters, coverage, c_homogeneity_score, c_numClusters, c_coverage, c_size = measures \n",
    "\n",
    "Results_df = pd.DataFrame([measures], columns=['homogeneity_score', 'numClusters', 'coverage', 'c_homogeneity_score', 'c_numClusters', 'c_coverage', 'c_size'])\n",
    "Results_df.to_csv(outputtracesPath + 'genomeResults_df.csv')\n",
    "\n",
    "message = 'GENOME [{:04d}] '.format(epoch)\n",
    "#message += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "message += f'homogeneity_score:{homogeneity_score:2.5f} '\n",
    "message += f'numClusters:{numClusters:03d} '\n",
    "message += f'coverage:{coverage:2.5f} '\n",
    "message += f'c_homogeneity_score:{c_homogeneity_score:2.5f} '\n",
    "message += f'c_numClusters:{c_numClusters:03d} '\n",
    "message += f'c_coverage:{c_coverage:2.5f} '\n",
    "message += f'c_size:{c_size:05d}'\n",
    "\n",
    "with open(outputlogFilePath, 'a') as writer:\n",
    "    writer.write(message + '\\n')\n",
    "\n",
    "print(message)\n",
    "\n",
    "print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
