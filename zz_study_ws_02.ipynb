{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sequenceSize=20000, embeddingDim=128, vocabularySize=2**16, filterWidth=5, filterNumber=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequenceSize   = sequenceSize\n",
    "        self.embeddingDim   = embeddingDim\n",
    "        self.vocabularySize = vocabularySize\n",
    "        self.filterWidth    = filterWidth\n",
    "        self.filterNumber   = filterNumber \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabularySize, self.embeddingDim)\n",
    "        self.conv = nn.Sequential(\n",
    "                            nn.Conv2d(1, self.filterNumber, (self.filterWidth, self.embeddingDim)),\n",
    "                            nn.BatchNorm2d(self.filterNumber),\n",
    "                            nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.filterNumber , 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "            \n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(256, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        x = x.max(dim=2)[0]\n",
    "        #print(x.size())\n",
    "\n",
    "        x = x.view(-1,  self.filterNumber)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, filePathList, labels, sequenceSize=20000, featureName='functionMethodCallsArgs'):\n",
    "        self.filePathList = filePathList\n",
    "        self.labels = labels\n",
    "        self.sequenceSize = sequenceSize\n",
    "        self.featureName = featureName\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filePathList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_parquet(self.filePathList[idx])\n",
    "        seed = int(round(time.time()%1, 6) * 1000000)\n",
    "        x = np.concatenate(df.iloc[np.random.RandomState(seed).permutation(len(df))][self.featureName].values)\n",
    "\n",
    "        if len(x) > self.sequenceSize:\n",
    "            x = x[:self.sequenceSize]\n",
    "        else:\n",
    "            x = np.concatenate((x, np.zeros([self.sequenceSize - len(x)])))\n",
    "            \n",
    "        sample = torch.from_numpy(x)\n",
    "        return (sample.long(), self.labels[idx], self.filePathList[idx])\n",
    "\n",
    "def train(model, optimizer, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels, _ in dataLoader:\n",
    "        \n",
    "        #\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().long()\n",
    "        loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "        #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        label_lst.append(labels.cpu().numpy())\n",
    "        predicted_lst.append(predicted.cpu().numpy())        \n",
    "        running_loss += loss.item() \n",
    "\n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss\n",
    "\n",
    "def assess(model, dataLoader, device):\n",
    "    running_loss  = 0.0  \n",
    "    label_lst     = list()\n",
    "    predicted_lst = list()\n",
    "    proba_lst     = list()\n",
    "    path_lst      = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels, paths in dataLoader:\n",
    "            #\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).squeeze().long()\n",
    "            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float())\n",
    "\n",
    "            #\n",
    "            if len(inputs) > 1:\n",
    "                label_lst.append(labels.cpu().numpy())\n",
    "                predicted_lst.append(predicted.cpu().numpy())\n",
    "                proba_lst.append(outputs.squeeze().cpu().numpy())\n",
    "                path_lst.append(paths)\n",
    "                running_loss += loss.item() \n",
    "    \n",
    "    labels    = np.concatenate(label_lst)\n",
    "    predicted = np.concatenate(predicted_lst)\n",
    "    proba     = np.concatenate(proba_lst)\n",
    "    paths     = np.concatenate(path_lst)\n",
    "    loss      = running_loss / len(predicted)\n",
    "    \n",
    "    return labels, predicted, loss, proba, paths\n",
    "\n",
    "def trainModel(ws, modelTag, epochNum, trainLoader, validLoader, device, lr=3e-4, weightDecay=9e-5):\n",
    "    #\n",
    "    model  = Net()\n",
    "    model  = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, factor=0.8)\n",
    "\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    outputtracesPath  = f'./traces/{ws}'\n",
    "    #shutil.rmtree(outputtracesPath)\n",
    "    #os.mkdir(outputtracesPath)\n",
    "\n",
    "    result_lst = list()\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    for epoch in range(epochNum):\n",
    "\n",
    "        tlabel, tpredicted, tloss = train(model, optimizer, trainLoader, device)\n",
    "        vlabel, vpredicted, vloss, vproba, vproba = assess(model, validLoader, device)\n",
    "\n",
    "        message  = f'Train: {modelTag} '\n",
    "        message += '[{:04d}] '.format(epoch)\n",
    "\n",
    "        tf1score  = f1_score(tlabel, tpredicted)\n",
    "        message  += 'TF1: {:2.4f}, '.format(tf1score*100)\n",
    "        message  += 'Tloss: {:2.8f}, '.format(tloss)\n",
    "\n",
    "        vf1score  = f1_score(vlabel, vpredicted)\n",
    "        message  += 'VF1: {:2.4f}, '.format(vf1score*100)\n",
    "        message  += 'VLoss: {:2.8f},'.format(vloss)  \n",
    "    \n",
    "        with open(outputlogFilePath, 'a') as writer:\n",
    "            writer.write(message + '\\n')\n",
    "        print(message)\n",
    "\n",
    "        modelOutputPath = f'{outputtracesPath}/model_{modelTag}_{epoch:03d}.pth'\n",
    "        torch.save(model.state_dict(), modelOutputPath)\n",
    "        result_lst.append((epoch, modelOutputPath, vlabel, vpredicted, vproba, vf1score, vloss, tf1score, tloss))\n",
    "\n",
    "        scheduler.step(tloss)\n",
    "\n",
    "    df = pd.DataFrame(result_lst, \n",
    "                      columns=['epoch', 'path', 'labels', 'predicted', 'proba', 'vf1score', 'vloss', 'tf1score', 'tloss'])\n",
    "    df.to_parquet(f'{outputtracesPath}/{modelTag}.parquet')\n",
    "\n",
    "    message = '----------'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate(ws, modelPathList, dataloader, device, numberFragments=1):\n",
    "    modelResultList = []\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    \n",
    "    for modelPath in modelPathList:\n",
    "        for fragment in range(numberFragments):\n",
    "            mdl = Net().to(device)\n",
    "            mdl.load_state_dict(torch.load(modelPath))\n",
    "            mdl.eval()\n",
    "            modelResult = assess(mdl, dataloader, device)\n",
    "            modelF1Score = f1_score(modelResult[0], modelResult[1])\n",
    "            modelResultList.append((modelPath, modelF1Score,) + modelResult)\n",
    "            message  = f'Evaluate: '\n",
    "            message += f'ModelPath={modelPath} Fragment={fragment:02d} '\n",
    "            message += f'score={modelF1Score}'\n",
    "            print(message)\n",
    "            with open(outputlogFilePath, 'a') as writer:\n",
    "                writer.write(message + '\\n')\n",
    "    return pd.DataFrame(modelResultList, columns=['name', 'f1score', 'Truth', 'Predicted', 'loss', 'Proba', 'Path'])\n",
    "\n",
    "def extendDataset(ws, result_df, probaUpperBorn = 0.8,  probaLowerBorn = 0.2):\n",
    "    outputlogFilePath = f'./traces/{ws}/logs'\n",
    "    results   = np.vstack(result_df.Proba.values)\n",
    "\n",
    "    truth       = result_df.Truth.iloc[0]\n",
    "    paths       = result_df.Path.iloc[0]\n",
    "    result_mean = results.mean(axis=0)\n",
    "    predicted   = (result_mean > 0.5).astype('int')\n",
    "    f1score     = f1_score(truth, predicted)\n",
    "\n",
    "    vtruth        = truth[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpaths        = paths[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vresult_prob  = result_mean[(result_mean >= probaUpperBorn) | (result_mean <= probaLowerBorn)]\n",
    "    vpredicted    = (vresult_prob > 0.5).astype('int')\n",
    "    vcoverage     = (len(vtruth)/len(truth))\n",
    "    vextendSize   = len(vtruth)\n",
    "    vf1score      = f1_score(vtruth, vpredicted)\n",
    "\n",
    "    etruth       = truth[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epaths       = paths[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    eresult_prob = result_mean[(result_mean < probaUpperBorn) & (result_mean > probaLowerBorn)]\n",
    "    epredicted    = (eresult_prob > 0.5).astype('int')\n",
    "    ecoverage     = (len(etruth)/len(truth))\n",
    "    erestSize     = len(etruth)\n",
    "    ef1score      = f1_score(etruth, epredicted)\n",
    "\n",
    "    message  = f'Extend: '\n",
    "    message += f'f1score={f1score*100:2.4f}, '\n",
    "    message += f'vcoverage={vcoverage*100:2.4f}, vf1score={vf1score*100:2.4f}, vexentdSize={vextendSize}, '\n",
    "    message += f'ecoverage={ecoverage*100:2.4f}, ef1score={ef1score*100:2.4f}, erestSize={erestSize}'\n",
    "\n",
    "    print(message)\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    extend_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })\n",
    "    \n",
    "    rest_df = dataset_df = pd.DataFrame( {'filePath': vpaths, \n",
    "                                            'label'   : vtruth })    \n",
    "    \n",
    "    return extend_df, rest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloaders(dataset_df, otest_df, ntest_df, batchSize=32, numWorkers=16, trainPercentage = 0.8):\n",
    "    rand_idx = np.random.RandomState(seed=54).permutation(len(dataset_df))\n",
    "    train_df = dataset_df.iloc[rand_idx[:int(trainPercentage * len(dataset_df))]]\n",
    "    valid_df = dataset_df.iloc[rand_idx[int(trainPercentage * len(dataset_df)):]]\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(train_df.label.value_counts())\n",
    "    print(len(valid_df))\n",
    "    print(valid_df.label.value_counts())\n",
    "    print(len(otest_df))\n",
    "    print(otest_df.label.value_counts())\n",
    "    print(len(ntest_df))\n",
    "    print(ntest_df.label.value_counts())\n",
    "    \n",
    "    trainDataset = SampleDataset(train_df.filePath.values, train_df.label.values)\n",
    "    trainLoader  = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "\n",
    "    validDataset = SampleDataset(valid_df.filePath.values, valid_df.label.values)\n",
    "    validLoader  = DataLoader(validDataset, batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "    otestDataset = SampleDataset(otest_df.filePath.values, otest_df.label.values)\n",
    "    otestLoader  = DataLoader(otestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    ntestDataset = SampleDataset(ntest_df.filePath.values, ntest_df.label.values)\n",
    "    ntestLoader  = DataLoader(ntestDataset,  batch_size=2*batchSize, shuffle=False, num_workers=numWorkers)\n",
    "    \n",
    "    return trainLoader, validLoader, otestLoader, ntestLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962411\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>sha1</th>\n",
       "      <th>md5</th>\n",
       "      <th>dex_date</th>\n",
       "      <th>apk_size</th>\n",
       "      <th>pkg_name</th>\n",
       "      <th>vercode</th>\n",
       "      <th>vt_detection</th>\n",
       "      <th>vt_scan_date</th>\n",
       "      <th>dex_size</th>\n",
       "      <th>markets</th>\n",
       "      <th>label</th>\n",
       "      <th>filePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vt_scan_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-31</th>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2948</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5532</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "      <td>5539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-31</th>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9560</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "      <td>9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-31</th>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22790</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22736</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "      <td>22793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>50077</td>\n",
       "      <td>50075</td>\n",
       "      <td>50075</td>\n",
       "      <td>50073</td>\n",
       "      <td>50077</td>\n",
       "      <td>50077</td>\n",
       "      <td>49967</td>\n",
       "      <td>50077</td>\n",
       "      <td>50077</td>\n",
       "      <td>50077</td>\n",
       "      <td>50077</td>\n",
       "      <td>50077</td>\n",
       "      <td>50077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>38791</td>\n",
       "      <td>38788</td>\n",
       "      <td>38788</td>\n",
       "      <td>38790</td>\n",
       "      <td>38791</td>\n",
       "      <td>38791</td>\n",
       "      <td>38676</td>\n",
       "      <td>38791</td>\n",
       "      <td>38791</td>\n",
       "      <td>38791</td>\n",
       "      <td>38791</td>\n",
       "      <td>38791</td>\n",
       "      <td>38791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-31</th>\n",
       "      <td>103358</td>\n",
       "      <td>103332</td>\n",
       "      <td>103332</td>\n",
       "      <td>103356</td>\n",
       "      <td>103358</td>\n",
       "      <td>103358</td>\n",
       "      <td>103332</td>\n",
       "      <td>103358</td>\n",
       "      <td>103358</td>\n",
       "      <td>103358</td>\n",
       "      <td>103358</td>\n",
       "      <td>103358</td>\n",
       "      <td>103358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>17485</td>\n",
       "      <td>17482</td>\n",
       "      <td>17482</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "      <td>17484</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "      <td>17485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "      <td>19570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "      <td>13946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "      <td>8261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "      <td>16025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "      <td>30921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "      <td>148997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "      <td>73816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "      <td>26954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "      <td>8652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "      <td>27814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "      <td>14749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "      <td>12864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "      <td>34566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "      <td>110732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "      <td>55940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "      <td>61344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "      <td>27916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sha256    sha1     md5  dex_date  apk_size  pkg_name  vercode  \\\n",
       "vt_scan_date                                                                  \n",
       "2013-01-31      2960    2960    2960      2960      2960      2960     2948   \n",
       "2013-04-30      5539    5539    5539      5539      5539      5539     5532   \n",
       "2013-07-31      9581    9581    9581      9581      9581      9581     9560   \n",
       "2013-10-31     22793   22793   22793     22790     22793     22793    22736   \n",
       "2014-01-31     50077   50075   50075     50073     50077     50077    49967   \n",
       "2014-04-30     38791   38788   38788     38790     38791     38791    38676   \n",
       "2014-07-31    103358  103332  103332    103356    103358    103358   103332   \n",
       "2014-10-31     17485   17482   17482     17485     17485     17485    17484   \n",
       "2015-01-31     19570   19570   19570     19570     19570     19570    19570   \n",
       "2015-04-30     13946   13946   13946     13946     13946     13946    13946   \n",
       "2015-07-31      8261    8261    8261      8261      8261      8261     8261   \n",
       "2015-10-31     16025   16025   16025     16025     16025     16025    16025   \n",
       "2016-01-31     30921   30921   30921     30921     30921     30921    30921   \n",
       "2016-04-30    148997  148997  148997    148997    148997    148997   148997   \n",
       "2016-07-31     73816   73816   73816     73816     73816     73816    73816   \n",
       "2016-10-31     26954   26954   26954     26954     26954     26954    26954   \n",
       "2017-01-31     14760   14760   14760     14760     14760     14760    14760   \n",
       "2017-04-30      8652    8652    8652      8652      8652      8652     8652   \n",
       "2017-07-31      4000    4000    4000      4000      4000      4000     4000   \n",
       "2017-10-31     27814   27814   27814     27814     27814     27814    27814   \n",
       "2018-01-31     14749   14749   14749     14749     14749     14749    14749   \n",
       "2018-04-30     12864   12864   12864     12864     12864     12864    12864   \n",
       "2018-07-31     34566   34566   34566     34566     34566     34566    34566   \n",
       "2018-10-31    110732  110732  110732    110732    110732    110732   110732   \n",
       "2019-01-31     55940   55940   55940     55940     55940     55940    55940   \n",
       "2019-04-30     61344   61344   61344     61344     61344     61344    61344   \n",
       "2019-07-31     27916   27916   27916     27916     27916     27916    27916   \n",
       "\n",
       "              vt_detection  vt_scan_date  dex_size  markets   label  filePath  \n",
       "vt_scan_date                                                                   \n",
       "2013-01-31            2960          2960      2960     2960    2960      2960  \n",
       "2013-04-30            5539          5539      5539     5539    5539      5539  \n",
       "2013-07-31            9581          9581      9581     9581    9581      9581  \n",
       "2013-10-31           22793         22793     22793    22793   22793     22793  \n",
       "2014-01-31           50077         50077     50077    50077   50077     50077  \n",
       "2014-04-30           38791         38791     38791    38791   38791     38791  \n",
       "2014-07-31          103358        103358    103358   103358  103358    103358  \n",
       "2014-10-31           17485         17485     17485    17485   17485     17485  \n",
       "2015-01-31           19570         19570     19570    19570   19570     19570  \n",
       "2015-04-30           13946         13946     13946    13946   13946     13946  \n",
       "2015-07-31            8261          8261      8261     8261    8261      8261  \n",
       "2015-10-31           16025         16025     16025    16025   16025     16025  \n",
       "2016-01-31           30921         30921     30921    30921   30921     30921  \n",
       "2016-04-30          148997        148997    148997   148997  148997    148997  \n",
       "2016-07-31           73816         73816     73816    73816   73816     73816  \n",
       "2016-10-31           26954         26954     26954    26954   26954     26954  \n",
       "2017-01-31           14760         14760     14760    14760   14760     14760  \n",
       "2017-04-30            8652          8652      8652     8652    8652      8652  \n",
       "2017-07-31            4000          4000      4000     4000    4000      4000  \n",
       "2017-10-31           27814         27814     27814    27814   27814     27814  \n",
       "2018-01-31           14749         14749     14749    14749   14749     14749  \n",
       "2018-04-30           12864         12864     12864    12864   12864     12864  \n",
       "2018-07-31           34566         34566     34566    34566   34566     34566  \n",
       "2018-10-31          110732        110732    110732   110732  110732    110732  \n",
       "2019-01-31           55940         55940     55940    55940   55940     55940  \n",
       "2019-04-30           61344         61344     61344    61344   61344     61344  \n",
       "2019-07-31           27916         27916     27916    27916   27916     27916  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdataset_df = pd.read_parquet('dataset/mdataset.parquet')\n",
    "print(len(mdataset_df))\n",
    "\n",
    "malware_overtime = mdataset_df.resample('3M', on='vt_scan_date', convention='start')\n",
    "\n",
    "malware_overtime.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ws               = 'ws061'\n",
    "epochNum         = 30\n",
    "dataset_rootDir  = '/ws/mnt/local/data/zoo/'\n",
    "device           = torch.device('cuda:6')\n",
    "ensembleSize     = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lst     = list()\n",
    "overtime_result = list()\n",
    "initial_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[0], \n",
    "                            'label'   : malware_overtime['label'].apply(list).iloc[0] })\n",
    "\n",
    "dataset_lst.append(initial_df)\n",
    "timeTags = list(malware_overtime.count().index)\n",
    "\n",
    "outputlogFilePath = f'./traces/{ws}/logs'\n",
    "outputtracesPath  = f'./traces/{ws}'\n",
    "os.mkdir(outputtracesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 20130430\n",
      "2368\n",
      "0    1753\n",
      "1     615\n",
      "Name: label, dtype: int64\n",
      "592\n",
      "0    431\n",
      "1    161\n",
      "Name: label, dtype: int64\n",
      "2960\n",
      "0    2184\n",
      "1     776\n",
      "Name: label, dtype: int64\n",
      "5539\n",
      "0    4214\n",
      "1    1325\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20130430 [0000] TF1: 89.3082, Tloss: 0.00565828, VF1: 93.6937, VLoss: 0.00233378,\n",
      "Train: train_20130430 [0001] TF1: 97.4486, Tloss: 0.00198371, VF1: 89.0141, VLoss: 0.00377790,\n",
      "Train: train_20130430 [0002] TF1: 97.4486, Tloss: 0.00147239, VF1: 97.1429, VLoss: 0.00077767,\n",
      "Train: train_20130430 [0003] TF1: 99.0212, Tloss: 0.00086266, VF1: 90.3409, VLoss: 0.00280800,\n",
      "Train: train_20130430 [0004] TF1: 99.5932, Tloss: 0.00049310, VF1: 97.4843, VLoss: 0.00076726,\n",
      "Train: train_20130430 [0005] TF1: 99.0244, Tloss: 0.00063403, VF1: 97.4522, VLoss: 0.00089481,\n",
      "Train: train_20130430 [0006] TF1: 98.9388, Tloss: 0.00061661, VF1: 98.1366, VLoss: 0.00090844,\n",
      "Train: train_20130430 [0007] TF1: 99.1064, Tloss: 0.00054472, VF1: 89.3471, VLoss: 0.00219432,\n",
      "Train: train_20130430 [0008] TF1: 98.7755, Tloss: 0.00070362, VF1: 95.7929, VLoss: 0.00117790,\n",
      "Train: train_20130430 [0009] TF1: 98.9422, Tloss: 0.00055979, VF1: 77.0992, VLoss: 0.00563950,\n",
      "Train: train_20130430 [0010] TF1: 99.1035, Tloss: 0.00051940, VF1: 89.7260, VLoss: 0.00314127,\n",
      "Epoch    10: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_20130430 [0011] TF1: 99.2677, Tloss: 0.00043589, VF1: 97.1429, VLoss: 0.00082538,\n",
      "Train: train_20130430 [0012] TF1: 99.5130, Tloss: 0.00029174, VF1: 98.1132, VLoss: 0.00073580,\n",
      "Train: train_20130430 [0013] TF1: 99.4304, Tloss: 0.00027472, VF1: 98.1250, VLoss: 0.00062738,\n",
      "Train: train_20130430 [0014] TF1: 99.8374, Tloss: 0.00014995, VF1: 94.0120, VLoss: 0.00147588,\n",
      "Train: train_20130430 [0015] TF1: 100.0000, Tloss: 0.00008657, VF1: 98.4326, VLoss: 0.00073001,\n",
      "Train: train_20130430 [0016] TF1: 99.5122, Tloss: 0.00022486, VF1: 96.1290, VLoss: 0.00143288,\n",
      "Train: train_20130430 [0017] TF1: 99.4314, Tloss: 0.00037372, VF1: 97.1429, VLoss: 0.00109718,\n",
      "Train: train_20130430 [0018] TF1: 99.7563, Tloss: 0.00021984, VF1: 95.4683, VLoss: 0.00131060,\n",
      "Train: train_20130430 [0019] TF1: 99.6748, Tloss: 0.00021820, VF1: 98.4326, VLoss: 0.00064657,\n",
      "Train: train_20130430 [0020] TF1: 99.8374, Tloss: 0.00010303, VF1: 98.1132, VLoss: 0.00087085,\n",
      "Train: train_20130430 [0021] TF1: 98.6145, Tloss: 0.00067893, VF1: 57.0922, VLoss: 0.01837145,\n",
      "Epoch    21: reducing learning rate of group 0 to 1.9200e-04.\n",
      "Train: train_20130430 [0022] TF1: 99.1035, Tloss: 0.00057035, VF1: 97.7918, VLoss: 0.00073529,\n",
      "Train: train_20130430 [0023] TF1: 99.4314, Tloss: 0.00027492, VF1: 98.7500, VLoss: 0.00068488,\n",
      "Train: train_20130430 [0024] TF1: 99.5932, Tloss: 0.00020161, VF1: 95.7831, VLoss: 0.00133975,\n",
      "Train: train_20130430 [0025] TF1: 99.8374, Tloss: 0.00013923, VF1: 98.4326, VLoss: 0.00075021,\n",
      "Train: train_20130430 [0026] TF1: 99.5932, Tloss: 0.00022083, VF1: 98.4326, VLoss: 0.00069368,\n",
      "Train: train_20130430 [0027] TF1: 99.8371, Tloss: 0.00021454, VF1: 97.2477, VLoss: 0.00106364,\n",
      "Epoch    27: reducing learning rate of group 0 to 1.5360e-04.\n",
      "Train: train_20130430 [0028] TF1: 100.0000, Tloss: 0.00006229, VF1: 98.4326, VLoss: 0.00077079,\n",
      "Train: train_20130430 [0029] TF1: 99.5938, Tloss: 0.00016341, VF1: 98.4326, VLoss: 0.00058602,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_029.pth Fragment=00 score=0.9517457826598666\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_013.pth Fragment=00 score=0.9616427741185587\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_019.pth Fragment=00 score=0.966857142857143\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_023.pth Fragment=00 score=0.9587227414330217\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_026.pth Fragment=00 score=0.9450809956538918\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_015.pth Fragment=00 score=0.9487785657998424\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_029.pth Fragment=00 score=0.9961240310077519\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_013.pth Fragment=00 score=0.9954867827208252\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_019.pth Fragment=00 score=0.9961340206185567\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_023.pth Fragment=00 score=0.9967721110393802\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_026.pth Fragment=00 score=0.9961240310077519\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130430_015.pth Fragment=00 score=0.9954809554551324\n",
      "Extend: f1score=96.0249, vcoverage=95.7393, vf1score=98.4396, vexentdSize=5303, ecoverage=4.2607, ef1score=79.2570, erestSize=236\n",
      "Extend: f1score=99.6772, vcoverage=99.5270, vf1score=99.8043, vexentdSize=2946, ecoverage=0.4730, ef1score=87.5000, erestSize=14\n",
      "########\n",
      "######## 20130731\n",
      "6610\n",
      "0    5071\n",
      "1    1539\n",
      "Name: label, dtype: int64\n",
      "1653\n",
      "0    1281\n",
      "1     372\n",
      "Name: label, dtype: int64\n",
      "5539\n",
      "0    4214\n",
      "1    1325\n",
      "Name: label, dtype: int64\n",
      "9581\n",
      "0    7120\n",
      "1    2461\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20130731 [0000] TF1: 92.5249, Tloss: 0.00380414, VF1: 66.0142, VLoss: 0.00746691,\n",
      "Train: train_20130731 [0001] TF1: 97.3702, Tloss: 0.00136794, VF1: 96.7235, VLoss: 0.00142258,\n",
      "Train: train_20130731 [0002] TF1: 98.1687, Tloss: 0.00106402, VF1: 98.9218, VLoss: 0.00026652,\n",
      "Train: train_20130731 [0003] TF1: 98.6971, Tloss: 0.00075034, VF1: 78.2334, VLoss: 0.00454668,\n",
      "Train: train_20130731 [0004] TF1: 98.6292, Tloss: 0.00071744, VF1: 98.7788, VLoss: 0.00030936,\n",
      "Train: train_20130731 [0005] TF1: 99.0215, Tloss: 0.00065012, VF1: 98.7821, VLoss: 0.00028280,\n",
      "Train: train_20130731 [0006] TF1: 98.4044, Tloss: 0.00076483, VF1: 97.9480, VLoss: 0.00049163,\n",
      "Train: train_20130731 [0007] TF1: 99.0538, Tloss: 0.00052170, VF1: 98.2265, VLoss: 0.00040304,\n",
      "Train: train_20130731 [0008] TF1: 98.9247, Tloss: 0.00055877, VF1: 99.3271, VLoss: 0.00020015,\n",
      "Train: train_20130731 [0009] TF1: 98.6310, Tloss: 0.00063974, VF1: 91.2669, VLoss: 0.00180125,\n",
      "Train: train_20130731 [0010] TF1: 99.1865, Tloss: 0.00042294, VF1: 98.3607, VLoss: 0.00035948,\n",
      "Train: train_20130731 [0011] TF1: 99.2511, Tloss: 0.00038528, VF1: 78.3158, VLoss: 0.00441560,\n",
      "Train: train_20130731 [0012] TF1: 99.0569, Tloss: 0.00042766, VF1: 85.8896, VLoss: 0.00278321,\n",
      "Train: train_20130731 [0013] TF1: 99.1542, Tloss: 0.00039426, VF1: 94.6429, VLoss: 0.00103030,\n",
      "Train: train_20130731 [0014] TF1: 99.1547, Tloss: 0.00046283, VF1: 99.0553, VLoss: 0.00020044,\n",
      "Train: train_20130731 [0015] TF1: 99.3485, Tloss: 0.00035138, VF1: 99.0553, VLoss: 0.00020985,\n",
      "Train: train_20130731 [0016] TF1: 99.3175, Tloss: 0.00027593, VF1: 76.6169, VLoss: 0.00550248,\n",
      "Train: train_20130731 [0017] TF1: 99.1531, Tloss: 0.00043614, VF1: 98.1030, VLoss: 0.00042831,\n",
      "Train: train_20130731 [0018] TF1: 98.8296, Tloss: 0.00049254, VF1: 98.5034, VLoss: 0.00036284,\n",
      "Train: train_20130731 [0019] TF1: 99.1859, Tloss: 0.00044330, VF1: 98.9130, VLoss: 0.00033561,\n",
      "Train: train_20130731 [0020] TF1: 99.3498, Tloss: 0.00032607, VF1: 98.3871, VLoss: 0.00043988,\n",
      "Train: train_20130731 [0021] TF1: 99.3175, Tloss: 0.00030014, VF1: 98.6702, VLoss: 0.00036556,\n",
      "Train: train_20130731 [0022] TF1: 99.4152, Tloss: 0.00028607, VF1: 96.8450, VLoss: 0.00097247,\n",
      "Epoch    22: reducing learning rate of group 0 to 2.4000e-04.\n",
      "Train: train_20130731 [0023] TF1: 99.2515, Tloss: 0.00029512, VF1: 98.5034, VLoss: 0.00050818,\n",
      "Train: train_20130731 [0024] TF1: 99.3813, Tloss: 0.00024042, VF1: 98.6631, VLoss: 0.00026010,\n",
      "Train: train_20130731 [0025] TF1: 99.4152, Tloss: 0.00027855, VF1: 97.8082, VLoss: 0.00065845,\n",
      "Train: train_20130731 [0026] TF1: 99.5772, Tloss: 0.00023019, VF1: 99.0654, VLoss: 0.00018555,\n",
      "Train: train_20130731 [0027] TF1: 99.5452, Tloss: 0.00022744, VF1: 88.1203, VLoss: 0.00302284,\n",
      "Train: train_20130731 [0028] TF1: 99.1547, Tloss: 0.00037410, VF1: 98.9247, VLoss: 0.00022200,\n",
      "Train: train_20130731 [0029] TF1: 98.7654, Tloss: 0.00048466, VF1: 99.1935, VLoss: 0.00017795,\n",
      "----------\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_029.pth Fragment=00 score=0.9365351629502573\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_026.pth Fragment=00 score=0.9469250210614996\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_008.pth Fragment=00 score=0.9219315039353331\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_014.pth Fragment=00 score=0.927996519469219\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_015.pth Fragment=00 score=0.9349137931034482\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_028.pth Fragment=00 score=0.9417040358744395\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_029.pth Fragment=00 score=0.9820542191676213\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_026.pth Fragment=00 score=0.9878971255673221\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_008.pth Fragment=00 score=0.9836065573770492\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_014.pth Fragment=00 score=0.9796545105566218\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_015.pth Fragment=00 score=0.982442748091603\n",
      "Evaluate: ModelPath=./traces/ws061/model_train_20130731_028.pth Fragment=00 score=0.9867273416761471\n",
      "Extend: f1score=94.5229, vcoverage=95.0318, vf1score=96.1229, vexentdSize=9105, ecoverage=4.9682, ef1score=83.0716, erestSize=476\n",
      "Extend: f1score=99.0129, vcoverage=97.7974, vf1score=99.7142, vexentdSize=5417, ecoverage=2.2026, ef1score=89.7297, erestSize=122\n",
      "########\n",
      "######## 20131031\n",
      "13894\n",
      "0    10658\n",
      "1     3236\n",
      "Name: label, dtype: int64\n",
      "3474\n",
      "0    2669\n",
      "1     805\n",
      "Name: label, dtype: int64\n",
      "9581\n",
      "0    7120\n",
      "1    2461\n",
      "Name: label, dtype: int64\n",
      "22793\n",
      "0    13256\n",
      "1     9537\n",
      "Name: label, dtype: int64\n",
      "----------\n",
      "Train: train_20131031 [0000] TF1: 94.5113, Tloss: 0.00290041, VF1: 93.0728, VLoss: 0.00175156,\n",
      "Train: train_20131031 [0001] TF1: 96.8035, Tloss: 0.00158293, VF1: 85.2211, VLoss: 0.00274759,\n",
      "Train: train_20131031 [0002] TF1: 97.0359, Tloss: 0.00137891, VF1: 96.4286, VLoss: 0.00070415,\n",
      "Train: train_20131031 [0003] TF1: 97.9484, Tloss: 0.00104375, VF1: 94.5411, VLoss: 0.00127454,\n",
      "Train: train_20131031 [0004] TF1: 97.6650, Tloss: 0.00104599, VF1: 90.5506, VLoss: 0.00225788,\n",
      "Train: train_20131031 [0005] TF1: 97.9199, Tloss: 0.00095411, VF1: 95.7255, VLoss: 0.00097680,\n",
      "Train: train_20131031 [0006] TF1: 98.1074, Tloss: 0.00089213, VF1: 96.0640, VLoss: 0.00088036,\n",
      "Train: train_20131031 [0007] TF1: 98.2630, Tloss: 0.00078707, VF1: 95.5959, VLoss: 0.00109764,\n",
      "Train: train_20131031 [0008] TF1: 98.3551, Tloss: 0.00081097, VF1: 97.9849, VLoss: 0.00051557,\n",
      "Train: train_20131031 [0009] TF1: 98.6187, Tloss: 0.00068968, VF1: 64.8867, VLoss: 0.01046041,\n",
      "Train: train_20131031 [0010] TF1: 98.0465, Tloss: 0.00088985, VF1: 90.9953, VLoss: 0.00246434,\n",
      "Train: train_20131031 [0011] TF1: 98.4973, Tloss: 0.00063113, VF1: 95.0090, VLoss: 0.00110438,\n",
      "Train: train_20131031 [0012] TF1: 98.2930, Tloss: 0.00077831, VF1: 96.7296, VLoss: 0.00080325,\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(timeTags)):\n",
    "    currentTag = timeTags[idx].isoformat().split('T')[0].replace('-', '')\n",
    "    \n",
    "    message  = '######## '\n",
    "    message += currentTag\n",
    "\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #\n",
    "    otest_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[idx-1], \n",
    "                              'label'   : malware_overtime['label'].apply(list).iloc[idx-1] })\n",
    "    \n",
    "    #\n",
    "    ntest_df = pd.DataFrame( {'filePath': malware_overtime['filePath'].apply(list).iloc[idx], \n",
    "                              'label'   : malware_overtime['label'].apply(list).iloc[idx] })\n",
    "    \n",
    "    dataset_df = pd.concat(dataset_lst)\n",
    "    trainLoader, validLoader, otestLoader, ntestLoader = getDataloaders(dataset_df, otest_df, ntest_df, trainPercentage=0.8)\n",
    "\n",
    "    #\n",
    "    models_df = trainModel(ws, f'train_{currentTag}', epochNum, trainLoader, validLoader, device)\n",
    "    models_df.sort_values(by=['vloss', 'tloss'], inplace=True)\n",
    "    selectedModelPaths = models_df.path.iloc[:ensembleSize].tolist()\n",
    "    \n",
    "    evalresult_df = evaluate(ws, selectedModelPaths, ntestLoader, device)\n",
    "    exresult_df   = evaluate(ws, selectedModelPaths, otestLoader, device)\n",
    "    \n",
    "    extend_df, _  = extendDataset(ws, evalresult_df, probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "    _, rest_df    = extendDataset(ws, exresult_df,   probaUpperBorn = 0.9, probaLowerBorn = 0.1)\n",
    "\n",
    "    #\n",
    "    dataset_lst.append(extend_df)\n",
    "\n",
    "    #\n",
    "    currentResults = pd.DataFrame([(currentTag, models_df, evalresult_df, exresult_df, dataset_lst, rest_df)], \n",
    "                                     columns=['TimeTag', 'models', 'evalResuls', \n",
    "                                              'extendResults', 'datasetList', \n",
    "                                              'restDataset'])\n",
    "    \n",
    "    #\n",
    "    outputPath = f'traces/{ws}/{currentTag}.pickle'\n",
    "    currentResults.to_pickle(outputPath)\n",
    "\n",
    "    message = '########'\n",
    "    with open(outputlogFilePath, 'a') as writer:\n",
    "        writer.write(message + '\\n')\n",
    "    print(message)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysciws",
   "language": "python",
   "name": "sciws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
